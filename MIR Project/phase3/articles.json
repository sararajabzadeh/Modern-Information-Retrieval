[{"id": 2981549002, "title": "Tensor Programs I: Wide Feedforward or Recurrent Neural Networks of Any Architecture are Gaussian Processes.", "abstract": "Wide neural networks with random weights and biases are Gaussian processes, as originally observed by Neal (1995) and more recently by Lee et al. (2018) and Matthews et al. (2018) for deep fully-connected networks, as well as by Novak et al. (2019) and Garriga-Alonso et al. (2019) for deep convolutional networks. We show that this Neural Network-Gaussian Process correspondence surprisingly extends to all modern feedforward or recurrent neural networks composed of multilayer perceptron, RNNs (e.g. LSTMs, GRUs), (nD or graph) convolution, pooling, skip connection, attention, batch normalization, and/or layer normalization. More generally, we introduce a language for expressing neural network computations, and our result encompasses all such expressible neural networks. This work serves as a tutorial on the *tensor programs* technique formulated in Yang (2019) and elucidates the Gaussian Process results obtained there. We provide open-source implementations of the Gaussian Process kernels of simple RNN, GRU, transformer, and batchnorm+ReLU network at this http URL.", "date": "2019", "authors": ["Greg Yang"], "references": [2194775991, 2963403868, 1836465849, 2964308564, 1677182931, 2963446712, 2157331557, 2310919327, 2064675550, 1533861849]}, {"id": 3105081694, "title": "COVID-Net: a tailored deep convolutional neural network design for detection of COVID-19 cases from chest X-ray images.", "abstract": "The Coronavirus Disease 2019 (COVID-19) pandemic continues to have a devastating effect on the health and well-being of the global population. A critical step in the fight against COVID-19 is effective screening of infected patients, with one of the key screening approaches being radiology examination using chest radiography. It was found in early studies that patients present abnormalities in chest radiography images that are characteristic of those infected with COVID-19. Motivated by this and inspired by the open source efforts of the research community, in this study we introduce COVID-Net, a deep convolutional neural network design tailored for the detection of COVID-19 cases from chest X-ray (CXR) images that is open source and available to the general public. To the best of the authors' knowledge, COVID-Net is one of the first open source network designs for COVID-19 detection from CXR images at the time of initial release. We also introduce COVIDx, an open access benchmark dataset that we generated comprising of 13,975 CXR images across 13,870 patient patient cases, with the largest number of publicly available COVID-19 positive cases to the best of the authors' knowledge. Furthermore, we investigate how COVID-Net makes predictions using an explainability method in an attempt to not only gain deeper insights into critical factors associated with COVID cases, which can aid clinicians in improved screening, but also audit COVID-Net in a responsible and transparent manner to validate that it is making decisions based on relevant information from the CXR images. By no means a production-ready solution, the hope is that the open access COVID-Net, along with the description on constructing the open source COVIDx dataset, will be leveraged and build upon by both researchers and citizen data scientists alike to accelerate the development of highly accurate yet practical deep learning solutions for detecting COVID-19 cases and accelerate treatment of those who need it the most.", "date": "2020", "authors": ["Linda Wang , Zhong Qiu Lin , Alexander Wong"], "references": [2194775991, 3001118548, 2962835968, 3008827533, 2919115771, 2108598243, 2963446712, 3007497549, 3010604545, 3008985036]}, {"id": 2950893734, "title": "Self-Attention Generative Adversarial Networks", "abstract": "In this paper, we propose the Self-Attention Generative Adversarial Network (SAGAN) which allows attention-driven, long-range dependency modeling for image generation tasks. Traditional convolutional GANs generate high-resolution details as a function of only spatially local points in lower-resolution feature maps. In SAGAN, details can be generated using cues from all feature locations. Moreover, the discriminator can check that highly detailed features in distant portions of the image are consistent with each other. Furthermore, recent work has shown that generator conditioning affects GAN performance. Leveraging this insight, we apply spectral normalization to the GAN generator and find that this improves training dynamics. The proposed SAGAN achieves the state-of-the-art results, boosting the best published Inception score from 36.8 to 52.52 and reducing Frechet Inception distance from 27.62 to 18.65 on the challenging ImageNet dataset. Visualization of the attention layers shows that the generator leverages neighborhoods that correspond to object shapes rather than local regions of fixed shape.", "date": "2018", "authors": ["Han Zhang 1, Ian Goodfellow 1, Dimitris Metaxas 2, Augustus Odena 1"], "references": [2964121744, 2963403868, 2117539524, 2099471712, 2964308564, 2963073614, 2962793481, 2963684088, 2963373786, 2963470893]}, {"id": 2194775991, "title": "Deep Residual Learning for Image Recognition", "abstract": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers\u20148\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.", "date": "2016", "authors": ["Kaiming He , Xiangyu Zhang , Shaoqing Ren , Jian Sun"], "references": [2618530766, 2962835968, 2097117768, 639708223, 1836465849, 2102605133, 2117539524, 1903029394, 2155893237, 1536680647]}, {"id": 2963403868, "title": "Attention is All You Need", "abstract": "The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms. We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.", "date": "2017", "authors": ["Ashish Vaswani 1, Noam Shazeer 1, Niki Parmar 2, Jakob Uszkoreit 1, Llion Jones 1, Aidan N. Gomez 1, Lukasz Kaiser 1, Illia Polosukhin 1"], "references": [2963341956, 2965373594, 2970597249, 2963091558, 2923014074, 2996428491, 2911489562, 2964110616]}, {"id": 1836465849, "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift", "abstract": "Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters.", "date": "2015", "authors": ["Sergey Ioffe , Christian Szegedy"], "references": [2097117768, 2117539524, 2095705004, 1677182931, 2146502635, 2310919327, 1665214252, 2168231600, 1533861849, 104184427]}, {"id": 2964308564, "title": "Neural Machine Translation by Jointly Learning to Align and Translate", "abstract": "Abstract: Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.", "date": "2014", "authors": ["Dzmitry Bahdanau 1, Kyunghyun Cho 2, Yoshua Bengio 2"], "references": [2157331557, 2064675550, 2294059674, 2132339004, 6908809, 1753482797, 1810943226, 2964199361, 2153653739, 1815076433]}, {"id": 1677182931, "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification", "abstract": "Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on the learnable activation and advanced initialization, we achieve 4.94% top-5 test error on the ImageNet 2012 classification dataset. This is a 26% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66% [33]). To our knowledge, our result is the first to surpass the reported human-level performance (5.1%, [26]) on this dataset.", "date": "2015", "authors": ["Kaiming He 1, Xiangyu Zhang 2, Shaoqing Ren 1, Jian Sun 1"], "references": [2618530766, 2962835968, 2097117768, 1836465849, 2102605133, 2117539524, 2095705004, 2108598243, 2155893237, 1536680647]}, {"id": 2963446712, "title": "Densely Connected Convolutional Networks", "abstract": "Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections&#x2014;one between each layer and its subsequent layer&#x2014;our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less memory and computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet.", "date": "2017", "authors": ["Gao Huang 1, Zhuang Liu 2, Laurens van der Maaten 3, Kilian Q. Weinberger 1"], "references": [2194775991, 2618530766, 2097117768, 1836465849, 2117539524, 1903029394, 2095705004, 2108598243, 1677182931, 2183341477]}, {"id": 2157331557, "title": "Learning Phrase Representations using RNN Encoder--Decoder for Statistical Machine Translation", "abstract": "In this paper, we propose a novel neural network model called RNN Encoder\u2010 Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder\u2010Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.", "date": "2014", "authors": ["Kyunghyun Cho 1, Bart van Merrienboer 1, Caglar Gulcehre 1, Dzmitry Bahdanau 2, Fethi Bougares 2, Holger Schwenk 2, Yoshua Bengio 3, 4, 5"], "references": [2618530766, 2153579005, 2064675550, 2294059674, 2147768505, 2132339004, 6908809, 1753482797, 2156387975, 2963504252]}, {"id": 2310919327, "title": "Gradient-based learning applied to document recognition", "abstract": "", "date": "2000", "authors": ["Yann Lecun 1, Leon Bottou 2, 3, Yoshua Bengio 3, 4, 5, Patrick Haffner 3, 6"], "references": [2147880316, 2156163116, 2963399829, 2964311892, 2157364932, 1510526001, 1944615693, 2158778629]}, {"id": 2064675550, "title": "Long short-term memory", "abstract": "Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.", "date": "1997", "authors": ["Sepp Hochreiter 1, J\u00fcrgen Schmidhuber 2"], "references": [2107878631, 2128499899, 2007431958, 2123716044, 2143503258, 2154890045, 194249466, 2048060899, 2103452139, 1674799117]}, {"id": 1533861849, "title": "Understanding the difficulty of training deep feedforward neural networks", "abstract": "Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence. 1 Deep Neural Networks Deep learning methods aim at learning feature hierarchies with features from higher levels of the hierarchy formed by the composition of lower level features. They include Appearing in Proceedings of the 13 International Conference on Artificial Intelligence and Statistics (AISTATS) 2010, Chia Laguna Resort, Sardinia, Italy. Volume 9 of JMLR: WC Weston et al., 2008). Much attention has recently been devoted to them (see (Bengio, 2009) for a review), because of their theoretical appeal, inspiration from biology and human cognition, and because of empirical success in vision (Ranzato et al., 2007; Larochelle et al., 2007; Vincent et al., 2008) and natural language processing (NLP) (Collobert & Weston, 2008; Mnih & Hinton, 2009). Theoretical results reviewed and discussed by Bengio (2009), suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g. in vision, language, and other AI-level tasks), one may need deep architectures. Most of the recent experimental results with deep architecture are obtained with models that can be turned into deep supervised neural networks, but with initialization or training schemes different from the classical feedforward neural networks (Rumelhart et al., 1986). Why are these new algorithms working so much better than the standard random initialization and gradient-based optimization of a supervised training criterion? Part of the answer may be found in recent analyses of the effect of unsupervised pretraining (Erhan et al., 2009), showing that it acts as a regularizer that initializes the parameters in a \u201cbetter\u201d basin of attraction of the optimization procedure, corresponding to an apparent local minimum associated with better generalization. But earlier work (Bengio et al., 2007) had shown that even a purely supervised but greedy layer-wise procedure would give better results. So here instead of focusing on what unsupervised pre-training or semi-supervised criteria bring to deep architectures, we focus on analyzing what may be going wrong with good old (but deep) multilayer neural networks. Our analysis is driven by investigative experiments to monitor activations (watching for saturation of hidden units) and gradients, across layers and across training iterations. We also evaluate the effects on these of choices of activation function (with the idea that it might affect saturation) and initialization procedure (since unsupervised pretraining is a particular form of initialization and it has a drastic impact).", "date": "2010", "authors": ["Xavier Glorot , Yoshua Bengio"], "references": [2136922672, 3118608800, 2310919327, 2072128103, 2117130368, 2025768430, 2110798204, 1498436455, 1994197834, 2131462252]}, {"id": 3001118548, "title": "Clinical features of patients infected with 2019 novel coronavirus in Wuhan, China", "abstract": "A recent cluster of pneumonia cases in Wuhan, China, was caused by a novel betacoronavirus, the 2019 novel coronavirus (2019-nCoV). We report the epidemiological, clinical, laboratory, and radiological characteristics and treatment and clinical outcomes of these patients. All patients with suspected 2019-nCoV were admitted to a designated hospital in Wuhan. We prospectively collected and analysed data on patients with laboratory-confirmed 2019-nCoV infection by real-time RT-PCR and next-generation sequencing. Data were obtained with standardised data collection forms shared by the International Severe Acute Respiratory and Emerging Infection Consortium from electronic medical records. Researchers also directly communicated with patients or their families to ascertain epidemiological and symptom data. Outcomes were also compared between patients who had been admitted to the intensive care unit (ICU) and those who had not.", "date": "2020", "authors": ["Chaolin Huang 1, Yeming Wang 2, Xingwang Li 3, Lili Ren 4, Jianping Zhao 5, Yi Hu 5, Li Zhang 1, Guohui Fan 2, Jiuyang Xu 6, Xiaoying Gu 2, Zhenshun Cheng 7, Ting Yu 1, Jiaan Xia 1, Yuan Wei 1, Wenjuan Wu 1, Xuelei Xie 1, Wen Yin 5, Hui Li 2, Min Liu 2, Yan Xiao 4, Hong Gao 4, Li Guo 4, Jungang Xie 5, Guangfa Wang 8, Rongmeng Jiang 3, Zhancheng Gao 8, Qi Jin 4, Jianwei Wang 4, Bin Cao 2"], "references": [2903899730, 2166867592, 3000413850, 2026274122, 2132260239, 2104548316, 2131262274, 2006434809, 2725497285, 3011483298]}, {"id": 2962835968, "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "abstract": "Abstract: In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.", "date": "2014", "authors": ["Karen Simonyan , Andrew Zisserman"], "references": [2194775991, 639708223, 1901129140, 1903029394, 1536680647, 3106250896]}, {"id": 3008827533, "title": "Clinical Characteristics of Coronavirus Disease 2019 in China", "abstract": "Abstract Background Since December 2019, when coronavirus disease 2019 (Covid-19) emerged in Wuhan city and rapidly spread throughout China, data have been needed on the clinical characteristics of...", "date": "2020", "authors": ["Wei-Jie Guan , Zheng-Yi Ni , Yu Hu , Wen-Hua Liang , Chun-Quan Ou , Jian-Xing He , Lei Liu , Hong Shan , Chun-Liang Lei , David S C Hui , Bin Du , Lan-Juan Li , Guang Zeng , Kwok-Yung Yuen , Ru-Chong Chen , Chun-Li Tang , Tao Wang , Ping-Yan Chen , Jie Xiang , Shi-Yue Li , Jin-Lin Wang , Zi-Jing Liang , Yi-Xiang Peng , Li Wei , Yong Liu , Ya-Hua Hu , Peng Peng , Jian-Ming Wang , Ji-Yang Liu , Zhong Chen , Gang Li , Zhi-Jian Zheng , Shao-Qin Qiu , Jie Luo , Chang-Jiang Ye , Shao-Yong Zhu , Nan-Shan Zhong"], "references": [3001118548, 3001897055, 3005079553, 3003668884, 3002108456, 3002539152, 3004318991, 3003465021, 3004239190, 3003573988]}, {"id": 2919115771, "title": "Deep learning", "abstract": "Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.", "date": "2015", "authors": ["Yann LeCun 1, 2, Yoshua Bengio 3, Geoffrey Hinton 4, 5"], "references": [2145339207, 2136922672, 2100495367, 2310919327, 2064675550, 2163922914, 2160815625, 2022508996, 1993882792, 2108069432]}, {"id": 2108598243, "title": "ImageNet: A large-scale hierarchical image database", "abstract": "The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called \u201cImageNet\u201d, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.", "date": "2009", "authors": ["Jia Deng , Wei Dong , Richard Socher , Li-Jia Li , Kai Li , Li Fei-Fei"], "references": [2151103935, 2038721957, 2128017662, 2110764733, 1782590233, 1576445103, 2145607950, 2141282920, 2115733720, 1528789833]}, {"id": 3007497549, "title": "Correlation of Chest CT and RT-PCR Testing for Coronavirus Disease 2019 (COVID-19) in China: A Report of 1014 Cases.", "abstract": "Chest CT had higher sensitivity for diagnosis of COVID-19 as compared with initial reverse-transcription polymerase chain reaction from swab samples in the epidemic area of China.", "date": "2020", "authors": ["Tao Ai 1, Zhenlu Yang 2, Hongyan Hou 3, Chenao Zhan 1, Chong Chen 1, Wenzhi Lv 1, Qian Tao 1, Ziyong Sun 1, Liming Xia 1"], "references": [3001118548, 3008818676, 3004906315, 3006110666, 3006643024, 3006354146, 3003901880, 3005656138, 3004511262]}, {"id": 3010604545, "title": "Detection of SARS-CoV-2 in Different Types of Clinical Specimens.", "abstract": "This study describes results of PCR and viral RNA testing for SARS-CoV-2 in bronchoalveolar fluid, sputum, feces, blood, and urine specimens from patients with COVID-19 infection in China to identify possible means of non-respiratory transmission.", "date": "2020", "authors": ["Wenling Wang 1, Yanli Xu 2, Ruqin Gao 3, Roujian Lu 1, Kai Han 2, Guizhen Wu 1, Wenjie Tan 1"], "references": [3005079553, 3008962515, 3008452791, 3033453353, 3034408674, 3035275617, 3034059415, 3033952286, 3035464429, 3036958556]}, {"id": 3008985036, "title": "Sensitivity of Chest CT for COVID-19: Comparison to RT-PCR", "abstract": "In a series of 51 patients with chest CT and real-time polymerase chain reaction assay (RT-PCR) performed within 3 days, the sensitivity of CT for 2019 novel coronavirus infection was 98% and that ...", "date": "2020", "authors": ["Yicheng Fang , Huangqi Zhang , Jicheng Xie , Minjie Lin , Lingjun Ying , Peipei Pang , Wenbin Ji"], "references": [3002108456, 3006110666, 3006643024, 3028749392, 3032185657, 3042098369, 3037255629]}, {"id": 2964121744, "title": "Adam: A Method for Stochastic Optimization", "abstract": "Abstract: We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.", "date": "2014", "authors": ["Diederik P. Kingma 1, Jimmy Lei Ba 2"], "references": [2963403868, 2962739339, 2963073614, 2962793481, 2331128040, 2963470893, 2964015378, 1514535095, 2508457857]}, {"id": 2117539524, "title": "ImageNet Large Scale Visual Recognition Challenge", "abstract": "The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5 years of the challenge, and propose future directions and improvements.", "date": "2015", "authors": ["Olga Russakovsky 1, Jia Deng 2, Hao Su 1, Jonathan Krause 1, Sanjeev Satheesh 1, Sean Ma 1, Zhiheng Huang 1, Andrej Karpathy 1, Aditya Khosla 3, Michael Bernstein 1, Alexander C. Berg 4, Li Fei-Fei 1"], "references": [2618530766, 2962835968, 2151103935, 2097117768, 2102605133, 1614298861, 2108598243, 2155893237, 2168356304, 1849277567]}, {"id": 2099471712, "title": "Generative Adversarial Nets", "abstract": "We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to \u00bd everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.", "date": "2014", "authors": ["Ian Goodfellow 1, Jean Pouget-Abadie 1, Mehdi Mirza 1, Bing Xu 1, David Warde-Farley 1, Sherjil Ozair 2, Aaron Courville 1, Yoshua Bengio 1"], "references": []}, {"id": 2963073614, "title": "Image-to-Image Translation with Conditional Adversarial Networks", "abstract": "We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Moreover, since the release of the pix2pix software associated with this paper, hundreds of twitter users have posted their own artistic experiments using our system. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without handengineering our loss functions either.", "date": "2017", "authors": ["Phillip Isola , Jun-Yan Zhu , Tinghui Zhou , Alexei A. Efros"], "references": [2964121744, 1836465849, 2117539524, 1901129140, 1903029394, 2099471712, 2133665775, 2100495367, 2963684088, 2340897893]}, {"id": 2962793481, "title": "Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks", "abstract": "Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain X to a target domain Y in the absence of paired examples. Our goal is to learn a mapping G : X \u2192 Y such that the distribution of images from G(X) is indistinguishable from the distribution Y using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping F : Y \u2192 X and introduce a cycle consistency loss to push F(G(X)) \u2248 X (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.", "date": "2017", "authors": ["Jun-Yan Zhu 1, Taesung Park 2, Phillip Isola 2, Alexei A. Efros 2"], "references": []}, {"id": 2963684088, "title": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks", "abstract": "Abstract: In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.", "date": "2015", "authors": ["Alec Radford 1, Luke Metz 1, Soumith Chintala 2"], "references": [2962793481, 2331128040, 2963470893, 2963420272, 2405756170, 2963800363, 2963836885, 2738588019, 2962947361]}, {"id": 2963373786, "title": "Improved techniques for training GANs", "abstract": "We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.", "date": "2016", "authors": ["Tim Salimans 1, Ian Goodfellow 2, Wojciech Zaremba 3, Vicki Cheung , Alec Radford 1, Xi Chen 4"], "references": [1836465849, 2183341477, 2963684088, 2964153729, 2271840356, 648143168, 830076066, 2963685250, 2949416428, 1487641199]}, {"id": 2963470893, "title": "Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network", "abstract": "Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function. Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution. In this paper, we present SRGAN, a generative adversarial network (GAN) for image super-resolution (SR). To our knowledge, it is the first framework capable of inferring photo-realistic natural images for 4x upscaling factors. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space. Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks. An extensive mean-opinion-score (MOS) test shows hugely significant gains in perceptual quality using SRGAN. The MOS scores obtained with SRGAN are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method.", "date": "2017", "authors": ["Christian Ledig 1, Lucas Theis 1, Ferenc Huszar 2, Jose Caballero 2, Andrew Cunningham , Alejandro Acosta 2, Andrew Aitken 2, Alykhan Tejani 2, Johannes Totz 2, Zehan Wang 2, Wenzhe Shi 2"], "references": [2194775991, 2618530766, 2962835968, 2964121744, 2097117768, 1836465849, 2117539524, 2099471712, 1677182931, 2133665775]}, {"id": 2618530766, "title": "ImageNet classification with deep convolutional neural networks", "abstract": "We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called \"dropout\" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.", "date": "2017", "authors": ["Alex Krizhevsky 1, Ilya Sutskever 1, Geoffrey E. Hinton 2"], "references": [2194775991, 2097117768, 2108598243, 2911964244, 3118608800, 1904365287, 1665214252, 2546302380, 2110764733, 2130325614]}, {"id": 2097117768, "title": "Going deeper with convolutions", "abstract": "We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.", "date": "2015", "authors": ["Christian Szegedy 1, Wei Liu 2, Yangqing Jia 1, Pierre Sermanet 1, Scott Reed 3, Dragomir Anguelov 1, Dumitru Erhan 1, Vincent Vanhoucke 1, Andrew Rabinovich 4"], "references": [2618530766, 2102605133, 1849277567, 2963542991, 2310919327, 1904365287, 2963911037, 2168231600, 2068730032, 104184427]}, {"id": 639708223, "title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks", "abstract": "State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features\u2014using the recently popular terminology of neural networks with \u2019attention\u2019 mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model [3] , our detection system has a frame rate of 5 fps ( including all steps ) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.", "date": "2017", "authors": ["Shaoqing Ren 1, Kaiming He 2, Ross Girshick 3, Jian Sun 2"], "references": [2194775991, 2618530766, 2962835968, 2097117768, 2102605133, 2117539524, 1903029394, 2155893237, 1536680647, 2168356304]}, {"id": 2102605133, "title": "Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation", "abstract": "Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012 -- achieving a mAP of 53.3%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features. Source code for the complete system is available at http://www.cs.berkeley.edu/~rbg/rcnn.", "date": "2014", "authors": ["Ross Girshick , Jeff Donahue , Trevor Darrell , Jitendra Malik"], "references": [2618530766, 2151103935, 2161969291, 2108598243, 2168356304, 3118608800, 2310919327, 2031489346, 2088049833, 2155541015]}, {"id": 1903029394, "title": "Fully convolutional networks for semantic segmentation", "abstract": "Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build \u201cfully convolutional\u201d networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet [20], the VGG net [31], and GoogLeNet [32]) into fully convolutional networks and transfer their learned representations by fine-tuning [3] to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20% relative improvement to 62.2% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one fifth of a second for a typical image.", "date": "2015", "authors": ["Jonathan Long , Evan Shelhamer , Trevor Darrell"], "references": [2618530766, 2962835968, 2097117768, 2102605133, 2155893237, 1663973292, 1849277567, 2963542991, 2109255472, 2155541015]}, {"id": 2155893237, "title": "Caffe: Convolutional Architecture for Fast Feature Embedding", "abstract": "Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying general-purpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU (approx 2 ms per image). By separating model representation from actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments. Caffe is maintained and developed by the Berkeley Vision and Learning Center (BVLC) with the help of an active community of contributors on GitHub. It powers ongoing research projects, large-scale industrial applications, and startup prototypes in vision, speech, and multimedia.", "date": "2014", "authors": ["Yangqing Jia 1, Evan Shelhamer 2, Jeff Donahue 2, Sergey Karayev 2, Jonathan Long 2, Ross Girshick 2, Sergio Guadarrama 2, Trevor Darrell 2"], "references": [2618530766, 2102605133, 2963542991, 2088049833, 2155541015, 753012316, 1825604117, 2147414309, 1872489089, 2962883796]}, {"id": 1536680647, "title": "Fast R-CNN", "abstract": "This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.", "date": "2015", "authors": ["Ross Girshick"], "references": [2618530766, 2962835968, 2102605133, 2108598243, 2155893237, 2168356304, 2164598857, 1861492603, 2963542991, 2109255472]}, {"id": 2963341956, "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).", "date": "2018", "authors": ["Jacob Devlin , Ming-Wei Chang , Kenton Lee , Kristina N. Toutanova"], "references": [2963403868, 2153579005, 2250539671, 2108598243, 2962739339, 2131744502, 2251939518, 2963748441, 2117130368, 2025768430]}, {"id": 2965373594, "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "abstract": "Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.", "date": "2019", "authors": ["Yinhan Liu , Myle Ott , Naman Goyal , Jingfei Du , Mandar Joshi , Danqi Chen , Omer Levy , Mike Lewis , Luke Zettlemoyer , Veselin Stoyanov"], "references": [2964121744, 2963403868, 2963341956, 2251939518, 2899771611, 2963748441, 2962784628, 1840435438, 2970597249, 2963026768]}, {"id": 2970597249, "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "abstract": "With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment setting, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.", "date": "2019", "authors": ["Zhilin Yang 1, Zihang Dai 1, Yiming Yang 1, Jaime G. Carbonell 1, Ruslan Salakhutdinov 1, Quoc V. Le 2"], "references": [2996035354, 2990704537, 3100307207, 3105966348, 3034238904, 3099342932, 2995015695]}, {"id": 2963091558, "title": "Non-local Neural Networks", "abstract": "Both convolutional and recurrent operations are building blocks that process one local neighborhood at a time. In this paper, we present non-local operations as a generic family of building blocks for capturing long-range dependencies. Inspired by the classical non-local means method [4] in computer vision, our non-local operation computes the response at a position as a weighted sum of the features at all positions. This building block can be plugged into many computer vision architectures. On the task of video classification, even without any bells and whistles, our nonlocal models can compete or outperform current competition winners on both Kinetics and Charades datasets. In static image recognition, our non-local models improve object detection/segmentation and pose estimation on the COCO suite of tasks. Code will be made available.", "date": "2018", "authors": ["Xiaolong Wang 1, Ross Girshick 1, Abhinav Gupta 2, Kaiming He 1"], "references": [2194775991, 2962835968, 639708223, 2963403868, 1836465849, 2117539524, 1677182931, 2806070179, 1861492603, 1904365287]}, {"id": 2923014074, "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding", "abstract": "Human ability to understand language is general, flexible, and robust. In contrast, most NLU models above the word level are designed for a specific task and struggle with out-of-domain data. If we aspire to develop models with understanding beyond the detection of superficial correspondences between inputs and outputs, then it is critical to develop a unified model that can execute a range of linguistic tasks across different domains. To facilitate research in this direction, we present the General Language Understanding Evaluation (GLUE, gluebenchmark.com): a benchmark of nine diverse NLU tasks, an auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an online platform for evaluating and comparing models. For some benchmark tasks, training data is plentiful, but for others it is limited or does not match the genre of the test set. GLUE thus favors models that can represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks. While none of the datasets in GLUE were created from scratch for the benchmark, four of them feature privately-held test data, which is used to ensure that the benchmark is used fairly. We evaluate baselines that use ELMo (Peters et al., 2018), a powerful transfer learning technique, as well as state-of-the-art sentence representation models. The best models still achieve fairly low absolute scores. Analysis with our diagnostic dataset yields similarly weak performance over all phenomena tested, with some exceptions.", "date": "2018", "authors": ["Alex Wang 1, Amanpreet Singh 1, Julian Michael 2, Felix Hill 3, Omer Levy 4, Samuel R. Bowman 1"], "references": [2962739339, 2251939518, 2963748441, 1486649854, 2963918774, 2963846996, 2525127255, 2962736243, 3104033643, 2130158090]}, {"id": 2996428491, "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "abstract": "Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations, longer training times, and unexpected model degradation. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large.", "date": "2020", "authors": ["Zhenzhong Lan 1, Mingda Chen 2, Sebastian Goodman 1, Kevin Gimpel 3, Piyush Sharma 1, Radu Soricut 1"], "references": [2963403868, 2963341956, 2153579005, 2250539671, 2131744502, 2251939518, 2963748441, 2964350391, 2965373594, 2970597249]}, {"id": 2911489562, "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining.", "abstract": "Motivation Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows. With the progress in natural language processing (NLP), extracting valuable information from biomedical literature has gained popularity among researchers, and deep learning has boosted the development of effective biomedical text mining models. However, directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora. In this article, we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora. Results We introduce BioBERT (Bidirectional Encoder Representations from Transformers for Biomedical Text Mining), which is a domain-specific language representation model pre-trained on large-scale biomedical corpora. With almost the same architecture across tasks, BioBERT largely outperforms BERT and previous state-of-the-art models in a variety of biomedical text mining tasks when pre-trained on biomedical corpora. While BERT obtains performance comparable to that of previous state-of-the-art models, BioBERT significantly outperforms them on the following three representative biomedical text mining tasks: biomedical named entity recognition (0.62% F1 score improvement), biomedical relation extraction (2.80% F1 score improvement) and biomedical question answering (12.24% MRR improvement). Our analysis results show that pre-training BERT on biomedical corpora helps it to understand complex biomedical texts. Availability and implementation We make the pre-trained weights of BioBERT freely available at https://github.com/naver/biobert-pretrained, and the source code for fine-tuning BioBERT available at https://github.com/dmis-lab/biobert.", "date": "2019", "authors": ["Jinhyuk Lee 1, Wonjin Yoon 1, Sungdong Kim 2, Donghyeon Kim 1, Sunkyu Kim 1, Chan Ho So 1, Jaewoo Kang 1"], "references": []}, {"id": 2964110616, "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context.", "abstract": "Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.", "date": "2019", "authors": ["Zihang Dai 1, Zhilin Yang 1, Yiming Yang 1, Jaime G. Carbonell 1, Quoc Viet Le 1, Ruslan Salakhutdinov 2"], "references": [2963403868, 2963341956, 2964308564, 2962739339, 2064675550, 179875071, 2132339004, 1810943226, 2963374479, 2584341106]}, {"id": 2095705004, "title": "Dropout: a simple way to prevent neural networks from overfitting", "abstract": "Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different \"thinned\" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.", "date": "2013", "authors": ["Nitish Srivastava , Geoffrey Hinton , Alex Krizhevsky , Ilya Sutskever , Ruslan Salakhutdinov"], "references": [2618530766, 2136922672, 3118608800, 2100495367, 2135046866, 2546302380, 2294059674, 2145094598, 2025768430, 2131241448]}, {"id": 2146502635, "title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization", "abstract": "We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.", "date": "2011", "authors": ["John Duchi 1, Elad Hazan 2, Yoram Singer 3"], "references": [2296319761, 2108598243, 3120740533, 2798766386, 2610857016, 2167732364, 2150102617, 2124541940, 1992208280, 2160218441]}, {"id": 1665214252, "title": "Rectified Linear Units Improve Restricted Boltzmann Machines", "abstract": "Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an infinite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these \"Stepped Sigmoid Units\" are unchanged. They can be approximated efficiently by noisy, rectified linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face verification on the Labeled Faces in the Wild dataset. Unlike binary units, rectified linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.", "date": "2010", "authors": ["Vinod Nair , Geoffrey E. Hinton"], "references": [2136922672, 2100495367, 2116064496, 2546302380, 1782590233, 2134557905, 2099866409, 1994197834, 2536626143, 2157364932]}, {"id": 2168231600, "title": "Large Scale Distributed Deep Networks", "abstract": "Recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance. In this paper, we consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores. We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models. Within this framework, we have developed two algorithms for large-scale distributed training: (i) Downpour SGD, an asynchronous stochastic gradient descent procedure supporting a large number of model replicas, and (ii) Sandblaster, a framework that supports a variety of distributed batch optimization procedures, including a distributed implementation of L-BFGS. Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network training. We have successfully used our system to train a deep network 30x larger than previously reported in the literature, and achieves state-of-the-art performance on ImageNet, a visual object recognition task with 16 million images and 21k categories. We show that these same techniques dramatically accelerate the training of a more modestly- sized deep network for a commercial speech recognition service. Although we focus on and report performance of these methods as applied to training large neural networks, the underlying algorithms are applicable to any gradient-based machine learning algorithm.", "date": "2012", "authors": ["Jeffrey Dean , Greg Corrado , Rajat Monga , Kai Chen , Matthieu Devin , Mark Mao , Marc'aurelio Ranzato , Andrew Senior , Paul Tucker , Ke Yang , Quoc V. Le , Andrew Y. Ng"], "references": [2173213060, 2108598243, 2146502635, 3118608800, 2117130368, 2147768505, 2132339004, 2141125852, 2184045248, 2118858186]}, {"id": 104184427, "title": "On the importance of initialization and momentum in deep learning", "abstract": "Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter, it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned. Our success training these models suggests that previous attempts to train deep and recurrent neural networks from random initializations have likely failed due to poor initialization schemes. Furthermore, carefully tuned momentum methods suffice for dealing with the curvature issues in deep and recurrent network training objectives without the need for sophisticated second-order methods.", "date": "2013", "authors": ["Ilya Sutskever 1, James Martens 2, George Dahl 2, Geoffrey Hinton 2"], "references": [2618530766, 2136922672, 2100495367, 2064675550, 1533861849, 2147768505, 2110798204, 1993882792, 2184045248, 196761320]}, {"id": 2294059674, "title": "Maxout Networks", "abstract": "We consider the problem of designing models to leverage a recently introduced approximate model averaging technique called dropout. We define a simple new model called maxout (so named because its output is the max of a set of inputs, and because it is a natural companion to dropout) designed to both facilitate optimization by dropout and improve the accuracy of dropout's fast approximate model averaging technique. We empirically verify that the model successfully accomplishes both of these tasks. We use maxout and dropout to demonstrate state of the art classification performance on four benchmark datasets: MNIST, CIFAR-10, CIFAR-100, and SVHN.", "date": "2013", "authors": ["Ian Goodfellow , David Warde-Farley , Mehdi Mirza , Aaron Courville , Yoshua Bengio"], "references": [2618530766, 3118608800, 2310919327, 1904365287, 2912934387, 2546302380, 2131241448, 2335728318, 2156387975, 189596042]}, {"id": 2132339004, "title": "A neural probabilistic language model", "abstract": "A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.", "date": "2003", "authors": ["Yoshua Bengio , R\u00e9jean Ducharme , Pascal Vincent , Christian Janvin"], "references": [2038721957, 2116064496, 2147152072, 1631260214, 2096175520, 2110485445, 1575350781, 2158195707, 2121227244, 2914484425]}, {"id": 6908809, "title": "ADADELTA: An Adaptive Learning Rate Method", "abstract": "We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment.", "date": "2012", "authors": ["Matthew D. Zeiler"], "references": [2168231600, 2147768505, 1498436455, 2120420045, 19621276, 1994616650]}, {"id": 1753482797, "title": "Recurrent Continuous Translation Models", "abstract": "We introduce a class of probabilistic continuous translation models called Recurrent Continuous Translation Models that are purely based on continuous representations for words, phrases and sentences and do not rely on alignments or phrasal translation units. The models have a generation and a conditioning aspect. The generation of the translation is modelled with a target Recurrent Language Model, whereas the conditioning on the source sentence is modelled with a Convolutional Sentence Model. Through various experiments, we show first that our models obtain a perplexity with respect to gold translations that is > 43% lower than that of stateof-the-art alignment-based translation models. Secondly, we show that they are remarkably sensitive to the word order, syntax, and meaning of the source sentence despite lacking alignments. Finally we show that they match a state-of-the-art system when rescoring n-best lists of translations.", "date": "2013", "authors": ["Nal Kalchbrenner , Phil Blunsom"], "references": [2146502635, 2117130368, 179875071, 2132339004, 1889268436, 2006969979, 2171928131, 2103305545, 2251222643, 196214544]}, {"id": 1810943226, "title": "Generating Sequences With Recurrent Neural Networks", "abstract": "This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.", "date": "2013", "authors": ["Alex Graves"], "references": [2064675550, 1554663460, 2143612262, 44815768, 1632114991, 2131462252, 196214544, 2108677974, 2120861206, 3023071679]}, {"id": 2964199361, "title": "On the Properties of Neural Machine Translation: Encoder--Decoder Approaches", "abstract": "Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder\u2010Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically.", "date": "2014", "authors": ["Kyunghyun Cho 1, Bart van Merrienboer 1, Dzmitry Bahdanau 2, Yoshua Bengio 3, 4, 5"], "references": [2130942839, 2157331557, 6908809, 1753482797, 1810943226, 2153653739, 2395935897, 1905522558, 1828163288, 2341457423]}, {"id": 2153653739, "title": "Statistical phrase-based translation", "abstract": "We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models. Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models out-perform word-based models. Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations. Surprisingly, learning phrases longer than three words and learning phrases from high-accuracy word-level alignment models does not have a strong impact on performance. Learning only syntactically motivated phrases degrades the performance of our systems.", "date": "2003", "authors": ["Philipp Koehn , Franz Josef Och , Daniel Marcu"], "references": [2101105183, 2006969979, 1508165687, 1973923101, 1986543644, 2116316001, 1517947178, 2161792612, 1549285799, 2158388102]}, {"id": 1815076433, "title": "On the difficulty of training recurrent neural networks", "abstract": "There are two widely known issues with properly training recurrent neural networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.", "date": "2013", "authors": ["Razvan Pascanu 1, Tomas Mikolov 2, Yoshua Bengio 1"], "references": [2146502635, 2064675550, 1498436455, 1606347560, 196214544, 2110485445, 2171865010, 2118706537, 2122585011, 2107878631]}, {"id": 2183341477, "title": "Rethinking the Inception Architecture for Computer Vision", "abstract": "Convolutional networks are at the core of most state of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21:2% top-1 and 5:6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3:5% top-5 error and 17:3% top-1 error on the validation set and 3:6% top-5 error on the official test set.", "date": "2016", "authors": ["Christian Szegedy 1, Vincent Vanhoucke 1, Sergey Ioffe 1, Jon Shlens 1, Zbigniew Wojna 2"], "references": [2618530766, 2962835968, 2097117768, 1836465849, 2102605133, 2117539524, 1903029394, 1677182931, 2096733369, 2016053056]}, {"id": 2153579005, "title": "Distributed Representations of Words and Phrases and their Compositionality", "abstract": "The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.", "date": "2013", "authors": ["Tomas Mikolov , Ilya Sutskever , Kai Chen , Greg S Corrado , Jeff Dean"], "references": [1614298861, 2141599568, 2117130368, 2132339004, 2158139315, 1423339008, 1498436455, 1662133657, 1889268436, 2131462252]}, {"id": 2147768505, "title": "Context-Dependent Pre-Trained Deep Neural Networks for Large-Vocabulary Speech Recognition", "abstract": "We propose a novel context-dependent (CD) model for large-vocabulary speech recognition (LVSR) that leverages recent advances in using deep belief networks for phone recognition. We describe a pre-trained deep neural network hidden Markov model (DNN-HMM) hybrid architecture that trains the DNN to produce a distribution over senones (tied triphone states) as its output. The deep belief network pre-training algorithm is a robust and often helpful way to initialize deep neural networks generatively that can aid in optimization and reduce generalization error. We illustrate the key components of our model, describe the procedure for applying CD-DNN-HMMs to LVSR, and analyze the effects of various modeling choices on performance. Experiments on a challenging business search dataset demonstrate that CD-DNN-HMMs can significantly outperform the conventional context-dependent Gaussian mixture model (GMM)-HMMs, with an absolute sentence accuracy improvement of 5.8% and 9.2% (or relative error reduction of 16.0% and 23.2%) over the CD-GMM-HMMs trained using the minimum phone error rate (MPE) and maximum-likelihood (ML) criteria, respectively.", "date": "2011", "authors": ["G. E. Dahl 1, Dong Yu 2, Li Deng 2, A. Acero 2"], "references": [2136922672, 2100495367, 2072128103, 1533861849, 2116064496, 2117130368, 2546302380, 2025768430, 1993882792, 1498436455]}, {"id": 2156387975, "title": "Deep Sparse Rectifier Neural Networks", "abstract": "While logistic sigmoid neurons are more biologically plausible than hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-dierentiabil ity", "date": "2011", "authors": ["Xavier Glorot 1, Antoine Bordes 2, Yoshua Bengio 1"], "references": [2136922672, 3118608800, 2310919327, 1665214252, 2129131372, 2072128103, 1533861849, 2097726431, 2546302380, 2025768430]}, {"id": 2963504252, "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "abstract": "Abstract: Despite the widespread practical success of deep learning methods, our theoretical understanding of the dynamics of learning in deep neural networks remains quite sparse. We attempt to bridge the gap between the theory and practice of deep learning by systematically analyzing learning dynamics for the restricted case of deep linear neural networks. Despite the linearity of their input-output map, such networks have nonlinear gradient descent dynamics on weights that change with the addition of each new hidden layer. We show that deep linear networks exhibit nonlinear learning phenomena similar to those seen in simulations of nonlinear networks, including long plateaus followed by rapid transitions to lower error solutions, and faster convergence from greedy unsupervised pretraining initial conditions than from random initial conditions. We provide an analytical description of these phenomena by finding new exact solutions to the nonlinear dynamics of deep learning. Our theoretical analysis also reveals the surprising finding that as the depth of a network approaches infinity, learning speed can nevertheless remain finite: for a special class of initial conditions on the weights, very deep networks incur only a finite, depth independent, delay in learning speed relative to shallow networks. We show that, under certain conditions on the training data, unsupervised pretraining can find this special class of initial conditions, while scaled random Gaussian initializations cannot. We further exhibit a new class of random orthogonal initial conditions on weights that, like unsupervised pre-training, enjoys depth independent learning times. We further show that these initial conditions also lead to faithful propagation of gradients even in deep nonlinear networks, as long as they operate in a special regime known as the edge of chaos.", "date": "2013", "authors": ["Andrew M. Saxe , James L. McClelland , Surya Ganguli"], "references": [2618530766, 2100495367, 2072128103, 1533861849, 2117130368, 104184427, 2110798204, 1993882792, 2141125852, 1815076433]}, {"id": 2147880316, "title": "Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data", "abstract": "We present conditional random fields , a framework for building probabilistic models to segment and label sequence data. Conditional random fields offer several advantages over hidden Markov models and stochastic grammars for such tasks, including the ability to relax strong independence assumptions made in those models. Conditional random fields also avoid a fundamental limitation of maximum entropy Markov models (MEMMs) and other discriminative Markov models based on directed graphical models, which can be biased towards states with few successor states. We present iterative parameter estimation algorithms for conditional random fields and compare the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data.", "date": "2001", "authors": ["John D. Lafferty , Andrew McCallum , Fernando C. N. Pereira"], "references": [2310919327, 1988790447, 1574901103, 2009570821, 2096175520, 1934019294, 1773803948, 2160842254, 2117400858, 3021452258]}, {"id": 2156163116, "title": "Best practices for convolutional neural networks applied to visual document analysis", "abstract": "Neural networks are a powerful technology forclassification of visual inputs arising from documents.However, there is a confusing plethora of different neuralnetwork methods that are used in the literature and inindustry. This paper describes a set of concrete bestpractices that document analysis researchers can use toget good results with neural networks. The mostimportant practice is getting a training set as large aspossible: we expand the training set by adding a newform of distorted data. The next most important practiceis that convolutional neural networks are better suited forvisual document tasks than fully connected networks. Wepropose that a simple \"do-it-yourself\" implementation ofconvolution with a flexible architecture is suitable formany visual document problems. This simpleconvolutional neural network does not require complexmethods, such as momentum, weight decay, structure-dependentlearning rates, averaging layers, tangent prop,or even finely-tuning the architecture. The end result is avery simple yet general architecture which can yieldstate-of-the-art performance for document analysis. Weillustrate our claims on the MNIST set of English digitimages.", "date": "2003", "authors": ["P.Y. Simard , D. Steinkraus , J.C. Platt"], "references": [2310919327, 1554663460, 2159737176, 2027197837, 2068017609, 2147345686, 51975515, 2166469100]}, {"id": 2963399829, "title": "mixup: Beyond Empirical Risk Minimization", "abstract": "", "date": "2017", "authors": ["Hongyi Zhang 1, Moustapha Cisse 2, Yann N. Dauphin 2, David Lopez-Paz 2"], "references": [2964274690, 2966415767, 2963855133, 2970902013, 2987875759, 2971149989, 3098350627, 3034351824, 2963681653]}, {"id": 2964311892, "title": "Spectral Networks and Locally Connected Networks on Graphs", "abstract": "Abstract: Convolutional Neural Networks are extremely efficient architectures in image and audio recognition tasks, thanks to their ability to exploit the local translational invariance of signal classes over their domain. In this paper we consider possible generalizations of CNNs to signals defined on more general domains without the action of a translation group. In particular, we propose two constructions, one based upon a hierarchical clustering of the domain, and another based on the spectrum of the graph Laplacian. We show through experiments that for low-dimensional graphs it is possible to learn convolutional layers with a number of parameters independent of the input size, resulting in efficient deep architectures.", "date": "2013", "authors": ["Joan Bruna 1, Wojciech Zaremba 1, Arthur Szlam 2, Yann LeCun 1"], "references": [2618530766, 1554944419, 2310919327, 2160815625, 2132914434, 1578099820, 2156718197, 2962820688, 1999192586, 2147860648]}, {"id": 2157364932, "title": "Learning a similarity metric discriminatively, with application to face verification", "abstract": "We present a method for training a similarity metric from data. The method can be used for recognition or verification applications where the number of categories is very large and not known during training, and where the number of training samples for a single category is very small. The idea is to learn a function that maps input patterns into a target space such that the L/sub 1/ norm in the target space approximates the \"semantic\" distance in the input space. The method is applied to a face verification task. The learning process minimizes a discriminative loss function that drives the similarity metric to be small for pairs of faces from the same person, and large for pairs from different persons. The mapping from raw to the target space is a convolutional network whose architecture is designed for robustness to geometric distortions. The system is tested on the Purdue/AR face database which has a very high degree of variability in the pose, lighting, expression, position, and artificial occlusions such as dark glasses and obscuring scarves.", "date": "2005", "authors": ["S. Chopra , R. Hadsell , Y. LeCun"], "references": [2310919327, 2053186076, 2138451337, 2121647436, 2994340921, 2095757522, 2144354855, 2107369107, 1802356529, 10021998]}, {"id": 1510526001, "title": "Probability Estimates for Multi-class Classification by Pairwise Coupling", "abstract": "Pairwise coupling is a popular multi-class classification method that combines all comparisons for each pair of classes. This paper presents two approaches for obtaining class probabilities. Both methods can be reduced to linear systems and are easy to implement. We show conceptually and experimentally that the proposed approaches are more stable than the two existing popular methods: voting and the method by Hastie and Tibshirani (1998)", "date": "2004", "authors": ["Ting-Fan Wu , Chih-Jen Lin , Ruby C. Weng"], "references": [2153635508, 2911964244, 2310919327, 2119821739, 2084812512, 2087347434, 1618905105, 2101276256, 2019575783, 1988195734]}, {"id": 1944615693, "title": "Action recognition with trajectory-pooled deep-convolutional descriptors", "abstract": "Visual features are of vital importance for human action understanding in videos. This paper presents a new video representation, called trajectory-pooled deep-convolutional descriptor (TDD), which shares the merits of both hand-crafted features [31] and deep-learned features [24]. Specifically, we utilize deep architectures to learn discriminative convolutional feature maps, and conduct trajectory-constrained pooling to aggregate these convolutional features into effective descriptors. To enhance the robustness of TDDs, we design two normalization methods to transform convolutional feature maps, namely spatiotemporal normalization and channel normalization. The advantages of our features come from (i) TDDs are automatically learned and contain high discriminative capacity compared with those hand-crafted features; (ii) TDDs take account of the intrinsic characteristics of temporal dimension and introduce the strategies of trajectory-constrained sampling and pooling for aggregating deep-learned features. We conduct experiments on two challenging datasets: HMD-B51 and UCF101. Experimental results show that TDDs outperform previous hand-crafted features [31] and deep-learned features [24]. Our method also achieves superior performance to the state of the art on these datasets.", "date": "2015", "authors": ["Limin Wang 1, Yu Qiao 2, Xiaoou Tang 1"], "references": [2618530766, 2962835968, 2151103935, 2097117768, 2161969291, 2108598243, 2155893237, 1849277567, 2310919327, 1677409904]}, {"id": 2158778629, "title": "Toward automatic phenotyping of developing embryos from videos", "abstract": "We describe a trainable system for analyzing videos of developing C. elegans embryos. The system automatically detects, segments, and locates cells and nuclei in microscopic images. The system was designed as the central component of a fully automated phenotyping system. The system contains three modules 1) a convolutional network trained to classify each pixel into five categories: cell wall, cytoplasm, nucleus membrane, nucleus, outside medium; 2) an energy-based model, which cleans up the output of the convolutional network by learning local consistency constraints that must be satisfied by label images; 3) a set of elastic models of the embryo at various stages of development that are matched to the label images.", "date": "2005", "authors": ["Feng Ning 1, D. Delhomme 2, Y. LeCun 3, F. Piano 1, L. Bottou 4, P.E. Barbano 3"], "references": [2164598857, 2310919327, 2147880316, 2104095591, 1647075334, 2134557905, 2121927366, 2119823327, 1991848143, 2157364932]}, {"id": 2107878631, "title": "Learning long-term dependencies with gradient descent is difficult", "abstract": "Recurrent neural networks can be used to map input sequences to output sequences, such as for recognition, production or prediction problems. However, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases. These results expose a trade-off between efficient learning by gradient descent and latching on information for long periods. Based on an understanding of this problem, alternatives to standard gradient descent are considered. >", "date": "1994", "authors": ["Y. Bengio 1, P. Simard 2, P. Frasconi 3"], "references": [2581275558, 2154642048, 2016589492, 2128499899, 19621276, 2088978850, 2148099973, 1996741810, 2125329357, 1527772862]}, {"id": 2128499899, "title": "Induction of Multiscale Temporal Structure", "abstract": "Learning structure in temporally-extended sequences is a difficult computational problem because only a fraction of the relevant information is available at any instant. Although variants of back propagation can in principle be used to find structure in sequences, in practice they are not sufficiently powerful to discover arbitrary contingencies, especially those spanning long temporal intervals or involving high order statistics. For example, in designing a connectionist network for music composition, we have encountered the problem that the net is able to learn musical structure that occurs locally in time--e.g., relations among notes within a musical phrase--but not structure that occurs over longer time periods--e.g., relations among phrases. To address this problem, we require a means of constructing a reduced description of the sequence that makes global aspects more explicit or more readily detectable. I propose to achieve this using hidden units that operate with different time constants. Simulation experiments indicate that slower time-scale hidden units are able to pick up global structure, structure that simply can not be learned by standard back propagation.", "date": "1991", "authors": ["Michael C Mozer"], "references": [2154642048, 2016589492, 2007431958, 2143503258, 1959983357, 2028629011, 2053127376, 2167607759]}, {"id": 2007431958, "title": "Generalization of back-propagation to recurrent neural networks.", "abstract": "An adaptive neural network with asymmetric connections is introduced. This network is related to the Hopfield network with graded neurons and uses a recurrent generalization of the \\ensuremath{\\delta} rule of Rumelhart, Hinton, and Williams to modify adaptively the synaptic weights. The new network bears a resemblance to the master/slave network of Lapedes and Farber but it is architecturally simpler.", "date": "1987", "authors": ["Fernando J. Pineda"], "references": [1652505363, 2177721432, 2075510082]}, {"id": 2123716044, "title": "Neurocontrol of nonlinear dynamical systems with Kalman filter trained recurrent networks", "abstract": "Although the potential of the powerful mapping and representational capabilities of recurrent network architectures is generally recognized by the neural network research community, recurrent neural networks have not been widely used for the control of nonlinear dynamical systems, possibly due to the relative ineffectiveness of simple gradient descent training algorithms. Developments in the use of parameter-based extended Kalman filter algorithms for training recurrent networks may provide a mechanism by which these architectures will prove to be of practical value. This paper presents a decoupled extended Kalman filter (DEKF) algorithm for training of recurrent networks with special emphasis on application to control problems. We demonstrate in simulation the application of the DEKF algorithm to a series of example control problems ranging from the well-known cart-pole and bioreactor benchmark problems to an automotive subsystem, engine idle speed control. These simulations suggest that recurrent controller networks trained by Kalman filter methods can combine the traditional features of state-space controllers and observers in a homogeneous architecture for nonlinear dynamical systems, while simultaneously exhibiting less sensitivity than do purely feedforward controller networks to changes in plant parameters and measurement noise. >", "date": "1994", "authors": ["G.V. Puskorius , L.A. Feldkamp"], "references": [2138484437, 2016589492, 2150355110, 1583833196, 2143787696, 2057653135, 2132152975, 2112462566, 1529008516, 2090248140]}, {"id": 2143503258, "title": "Learning state space trajectories in recurrent neural networks", "abstract": "Many neural network learning procedures compute gradients of the errors on the output layer of units after they have settled to their final values. We describe a procedure for finding E/wij, where E is an error functional of the temporal trajectory of the states of a continuous recurrent network and wij are the weights of that network. Computing these quantities allows one to perform gradient descent in the weights to minimize E. Simulations in which networks are taught to move through limit cycles are shown. This type of recurrent network seems particularly suited for temporally continuous domains, such as signal processing, control, and speech.", "date": "1989", "authors": ["Barak A. Pearlmutter"], "references": [2016589492, 2007431958, 1959983357, 1971129545, 2028629011, 1996647346]}, {"id": 2154890045, "title": "Gradient calculations for dynamic recurrent neural networks: a survey", "abstract": "Surveys learning algorithms for recurrent neural networks with hidden units and puts the various techniques into a common framework. The authors discuss fixed point learning algorithms, namely recurrent backpropagation and deterministic Boltzmann machines, and nonfixed point algorithms, namely backpropagation through time, Elman's history cutoff, and Jordan's output feedback architecture. Forward propagation, an on-line technique that uses adjoint equations, and variations thereof, are also discussed. In many cases, the unified presentation leads to generalizations of various sorts. The author discusses advantages and disadvantages of temporally continuous neural networks in contrast to clocked ones continues with some \"tricks of the trade\" for training, using, and simulating continuous time and recurrent neural networks. The author presents some simulations, and at the end, addresses issues of computational complexity and learning speed. >", "date": "1995", "authors": ["B.A. Pearlmutter"], "references": [2581275558, 2154642048, 2138484437, 2110485445, 2895674046, 2147800946, 1597286183, 1535810436, 2173629880, 2016589492]}, {"id": 194249466, "title": "Untersuchungen zu dynamischen neuronalen Netzen", "abstract": "", "date": "1990", "authors": ["Sepp Hochreiter"], "references": [2194775991, 2964308564, 2130942839, 2076063813, 2064675550, 2072128103, 1924770834, 1026270304]}, {"id": 2048060899, "title": "A time-delay neural network architecture for isolated word recognition", "abstract": "Abstract A translation-invariant back-propagation network is described that performs better than a sophisticated continuous acoustic parameter hidden Markov model on a noisy, 100-speaker confusable vocabulary isolated word recognition task. The network's replicated architecture permits it to extract precise information from unaligned training patterns selected by a naive segmentation rule.", "date": "1989", "authors": ["Kevin J. Lang 1, Alex H. Waibel 1, Geoffrey E. Hinton 2"], "references": [1498436455, 3017143921, 2173629880, 1966812932, 2176028050, 2101926813, 1959983357, 1991133427, 1573503290, 2048330959]}, {"id": 2103452139, "title": "Learning long-term dependencies in NARX recurrent neural networks", "abstract": "It has previously been shown that gradient-descent learning algorithms for recurrent neural networks can perform poorly on tasks that involve long-term dependencies, i.e. those problems for which the desired output depends on inputs presented at times far in the past. We show that the long-term dependencies problem is lessened for a class of architectures called nonlinear autoregressive models with exogenous (NARX) recurrent neural networks, which have powerful representational capabilities. We have previously reported that gradient descent learning can be more effective in NARX networks than in recurrent neural network architectures that have \"hidden states\" on problems including grammatical inference and nonlinear system identification. Typically, the network converges much faster and generalizes better than other networks. The results in this paper are consistent with this phenomenon. We present some experimental results which show that NARX networks can often retain information for two to three times as long as conventional recurrent neural networks. We show that although NARX networks do not circumvent the problem of long-term dependencies, they can greatly improve performance on long-term dependency problems. We also describe in detail some of the assumptions regarding what it means to latch information robustly and suggest possible ways to loosen these assumptions.", "date": "1996", "authors": ["Tsungnan Lin , B.G. Horne , P. Tino , C.L. Giles"], "references": [2064675550, 2154642048, 2138484437, 2110485445, 2107878631, 2798813531, 2128499899, 2123716044, 1674799117, 2098398123]}, {"id": 1674799117, "title": "Gradient-based learning algorithms for recurrent networks and their computational complexity", "abstract": "", "date": "1994", "authors": ["Ronald J. Williams , David Zipser"], "references": [2064675550, 1810943226, 2144499799, 2136848157, 1828163288, 1735317348, 2079735306, 2147568880, 3099873379]}, {"id": 2136922672, "title": "A fast learning algorithm for deep belief nets", "abstract": "We show how to use \"complementary priors\" to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.", "date": "2006", "authors": ["Geoffrey E. Hinton 1, Simon Osindero 1, Yee-Whye Teh 2"], "references": [2310919327, 2116064496, 2057175746, 2159080219, 2156163116, 2131686571, 2567948266, 2158778629, 2159737176, 2124914669]}, {"id": 3118608800, "title": "Learning Multiple Layers of Features from Tiny Images", "abstract": "In this work we describe how to train a multi-layer generative model of natural images. We use a dataset of millions of tiny colour images, described in the next section. This has been attempted by several groups but without success. The models on which we focus are RBMs (Restricted Boltzmann Machines) and DBNs (Deep Belief Networks). These models learn interesting-looking filters, which we show are more useful to a classifier than the raw pixels. We train the classifier on a labeled subset that we have collected and call the CIFAR-10 dataset.", "date": "2008", "authors": ["Alex Krizhevsky"], "references": [2081580037, 2096192494, 2165225968]}, {"id": 2072128103, "title": "Learning Deep Architectures for AI", "abstract": "Can machine learning deliver AI? Theoretical results, inspiration from the brain and cognition, as well as machine learning experiments suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g. in vision, language, and other AI-level tasks), one would need deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers, graphical models with many levels of latent variables, or in complicated propositional formulae re-using many sub-formulae. Each level of the architecture represents features at a different level of abstraction, defined as a composition of lower-level features. Searching the parameter space of deep architectures is a difficult task, but new algorithms have been discovered and a new sub-area has emerged in the machine learning community since 2006, following these discoveries. Learning algorithms such as those for Deep Belief Networks and other related unsupervised learning algorithms have recently been proposed to train deep architectures, yielding exciting results and beating the state-of-the-art in certain areas. Learning Deep Architectures for AI discusses the motivations for and principles of learning algorithms for deep architectures. By analyzing and comparing recent results with different learning algorithms for deep architectures, explanations for their success are proposed and discussed, highlighting challenges and suggesting avenues for future explorations in this area.", "date": "2008", "authors": ["Yoshua Bengio"], "references": [2156909104, 2911964244, 2296616510, 2136922672, 2100495367, 2310919327, 2187089797, 2129131372, 2119821739, 2053186076]}, {"id": 2117130368, "title": "A unified architecture for natural language processing: deep neural networks with multitask learning", "abstract": "We describe a single convolutional neural network architecture that, given a sentence, outputs a host of language processing predictions: part-of-speech tags, chunks, named entity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense (grammatically and semantically) using a language model. The entire network is trained jointly on all these tasks using weight-sharing, an instance of multitask learning. All the tasks use labeled data except the language model which is learnt from unlabeled text and represents a novel form of semi-supervised learning for the shared tasks. We show how both multitask learning and semi-supervised learning improve the generalization of the shared tasks, resulting in state-of-the-art-performance.", "date": "2008", "authors": ["Ronan Collobert , Jason Weston"], "references": [2310919327, 2132339004, 2158847908, 2130903752, 2107008379, 2914746235, 2173629880, 2885050925, 2158823144, 2163568299]}, {"id": 2025768430, "title": "Extracting and composing robust features with denoising autoencoders", "abstract": "Previous work has shown that the difficulties in learning deep generative or discriminative models can be overcome by an initial unsupervised learning step that maps inputs to useful intermediate representations. We introduce and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern. This approach can be used to train autoencoders, and these denoising autoencoders can be stacked to initialize deep architectures. The algorithm can be motivated from a manifold learning and information theoretic perspective or from a generative model perspective. Comparative experiments clearly show the surprising advantage of corrupting the input of autoencoders on a pattern classification benchmark suite.", "date": "2008", "authors": ["Pascal Vincent , Hugo Larochelle , Yoshua Bengio , Pierre-Antoine Manzagol"], "references": [2136922672, 2100495367, 2072128103, 2110798204, 2153663612, 1652505363, 1498436455, 1994197834, 2293063825, 2172174689]}, {"id": 2110798204, "title": "Greedy Layer-Wise Training of Deep Networks", "abstract": "Complexity theory of circuits strongly suggests that deep architectures can be much more efficient (sometimes exponentially) than shallow architectures, in terms of computational elements required to represent some functions. Deep multi-layer neural networks have many levels of non-linearities allowing them to compactly represent highly non-linear and highly-varying functions. However, until recently it was not clear how to train such deep networks, since gradient-based optimization starting from random initialization appears to often get stuck in poor solutions. Hinton et al. recently introduced a greedy layer-wise unsupervised learning algorithm for Deep Belief Networks (DBN), a generative model with many layers of hidden causal variables. In the context of the above optimization problem, we study this algorithm empirically and explore variants to better understand its success and extend it to cases where the inputs are continuous or where the structure of the input distribution is not revealing enough about the variable to be predicted in a supervised task. Our experiments also confirm the hypothesis that the greedy layer-wise unsupervised training strategy mostly helps the optimization, by initializing weights in a region near a good local minimum, giving rise to internal distributed representations that are high-level abstractions of the input, bringing better generalization.", "date": "2006", "authors": ["Yoshua Bengio , Pascal Lamblin , Dan Popovici , Hugo Larochelle"], "references": [2136922672, 2100495367, 2116064496, 2613634265, 2124914669, 1993845689, 2109779438, 2103626435, 2125569215, 2130313186]}, {"id": 1498436455, "title": "Learning representations by back-propagating errors", "abstract": "We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal \u2018hidden\u2019 units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.", "date": "1987", "authors": ["David E. Rumelhart 1, Geoffrey E. Hinton 2, Ronald J. Williams 1"], "references": [1652505363, 2322002063]}, {"id": 1994197834, "title": "An empirical evaluation of deep architectures on problems with many factors of variation", "abstract": "Recently, several learning algorithms relying on models with deep architectures have been proposed. Though they have demonstrated impressive performance, to date, they have only been evaluated on relatively simple problems such as digit recognition in a controlled environment, for which many machine learning algorithms already report reasonable results. Here, we present a series of experiments which indicate that these models show promise in solving harder learning problems that exhibit many factors of variation. These models are compared with well-established algorithms such as Support Vector Machines and single hidden-layer feed-forward neural networks.", "date": "2007", "authors": ["Hugo Larochelle , Dumitru Erhan , Aaron Courville , James Bergstra , Yoshua Bengio"], "references": [2153635508, 2136922672, 2100495367, 2116064496, 2110798204, 2134557905, 2147800946, 2613634265, 2159737176, 2124914669]}, {"id": 2131462252, "title": "A Scalable Hierarchical Distributed Language Model", "abstract": "Neural probabilistic language models (NPLMs) have been shown to be competitive with and occasionally superior to the widely-used n-gram language models. The main drawback of NPLMs is their extremely long training and testing times. Morin and Bengio have proposed a hierarchical language model built around a binary tree of words, which was two orders of magnitude faster than the non-hierarchical model it was based on. However, it performed considerably worse than its non-hierarchical counterpart in spite of using a word tree created using expert knowledge. We introduce a fast hierarchical language model along with a simple feature-based algorithm for automatic construction of word trees from the data. We then show that the resulting models can outperform non-hierarchical neural models as well as the best n-gram models.", "date": "2008", "authors": ["Andriy Mnih , Geoffrey E. Hinton"], "references": []}, {"id": 2903899730, "title": "Origin and evolution of pathogenic coronaviruses", "abstract": "Severe acute respiratory syndrome coronavirus (SARS-CoV) and Middle East respiratory syndrome coronavirus (MERS-CoV) are two highly transmissible and pathogenic viruses that emerged in humans at the beginning of the 21st century. Both viruses likely originated in bats, and genetically diverse coronaviruses that are related to SARS-CoV and MERS-CoV were discovered in bats worldwide. In this Review, we summarize the current knowledge on the origin and evolution of these two pathogenic coronaviruses and discuss their receptor usage; we also highlight the diversity and potential of spillover of bat-borne coronaviruses, as evidenced by the recent spillover of swine acute diarrhoea syndrome coronavirus (SADS-CoV) to pigs. Coronaviruses have a broad host range and distribution, and some highly pathogenic lineages have spilled over to humans and animals. Here, Cui, Li and Shi explore the viral factors that enabled the emergence of diseases such as severe acute respiratory syndrome and Middle East respiratory syndrome.", "date": "2019", "authors": ["Jie Cui 1, Fang Li 2, Zheng Li Shi 1"], "references": [2144081223, 2166867592, 2111211467, 2470646526, 2132260239, 2104548316, 2306794997, 1993577573, 2775086803, 2119111857]}, {"id": 2166867592, "title": "Isolation of a Novel Coronavirus from a Man with Pneumonia in Saudi Arabia", "abstract": "A previously unknown coronavirus was isolated from the sputum of a 60-year-old man who presented with acute pneumonia and subsequent renal failure with a fatal outcome in Saudi Arabia. The virus (called HCoV-EMC) replicated readily in cell culture, producing cytopathic effects of rounding, detachment, and syncytium formation. The virus represents a novel betacoronavirus species. The closest known relatives are bat coronaviruses HKU4 and HKU5. Here, the clinical data, virus isolation, and molecular identification are presented. The clinical picture was remarkably similar to that of the severe acute respiratory syndrome (SARS) outbreak in 2003 and reminds us that animal coronaviruses can cause severe disease in humans.", "date": "2012", "authors": ["Ali Moh Zaki 1, Sander Van Boheemen 2, Theo M. Bestebroer 2, Albert D.M.E. Osterhaus , Ron A.M. Fouchier"], "references": [2132260239, 2025170735, 2129542667, 1703839189, 2116586125, 1987783718, 1963953102, 2111412754, 2170933940, 2181908191]}, {"id": 3000413850, "title": "Comparative therapeutic efficacy of remdesivir and combination lopinavir, ritonavir, and interferon beta against MERS-CoV.", "abstract": "Middle East respiratory syndrome coronavirus (MERS-CoV) is the causative agent of a severe respiratory disease associated with more than 2468 human infections and over 851 deaths in 27 countries since 2012. There are no approved treatments for MERS-CoV infection although a combination of lopinavir, ritonavir and interferon beta (LPV/RTV-IFNb) is currently being evaluated in humans in the Kingdom of Saudi Arabia. Here, we show that remdesivir (RDV) and IFNb have superior antiviral activity to LPV and RTV in vitro. In mice, both prophylactic and therapeutic RDV improve pulmonary function and reduce lung viral loads and severe lung pathology. In contrast, prophylactic LPV/RTV-IFNb slightly reduces viral loads without impacting other disease parameters. Therapeutic LPV/RTV-IFNb improves pulmonary function but does not reduce virus replication or severe lung pathology. Thus, we provide in vivo evidence of the potential for RDV to treat MERS-CoV infections.", "date": "2020", "authors": ["Timothy P. Sheahan 1, Amy C. Sims 1, Sarah R. Leist 1, Alexandra Sch\u00e4fer 1, John Won 1, Ariane J. Brown 1, Stephanie A. Montgomery 1, Alison Hogg 2, Darius Babusis 2, Michael O. Clarke 2, Jamie E. Spahn 2, Laura Bauer 2, Scott Sellers 2, Danielle Porter 2, Joy Y. Feng 2, Tomas Cihlar 2, Robert Jordan 2, Mark R. Denison 3, Ralph S. Baric 1"], "references": [2107277218, 2470646526, 2725497285, 1993577573, 2565805236, 2791599184, 2255243349, 2292021561, 2290466312, 2034462612]}, {"id": 2026274122, "title": "KDIGO clinical practice guidelines for acute kidney injury.", "abstract": "tion\u2019, implying that most patients \u2018should\u2019 receive a particular action. In contrast, level 2 guidelines are essentially \u2018suggestions\u2019 and are deemed to be \u2018weak\u2019 or discretionary, recognising that management decisions may vary in different clinical contexts. Each recommendation was further graded from A to D by the quality of evidence underpinning them, with grade A referring to a high quality of evidence whilst grade D recognised a \u2018very low\u2019 evidence base. The overall strength and quality of the supporting evidence is summarised in table 1 . The guidelines focused on 4 key domains: (1) AKI definition, (2) prevention and treatment of AKI, (3) contrastinduced AKI (CI-AKI) and (4) dialysis interventions for the treatment of AKI. The full summary of clinical practice statements is available at www.kdigo.org, but a few key recommendation statements will be highlighted here.", "date": "2012", "authors": ["Arif Khwaja"], "references": [1967300023, 2131419242, 2143432233, 2117958746, 1531106656, 2157775267, 2028701043, 2111704803, 2135163018, 2042074736]}, {"id": 2132260239, "title": "Identification of a novel coronavirus in patients with severe acute respiratory syndrome.", "abstract": "BACKGROUND: The severe acute respiratory syndrome (SARS) has recently been identified as a new clinical entity. SARS is thought to be caused by an unknown infectious agent. METHODS: Clinical specimens from patients with SARS were searched for unknown viruses with the use of cell cultures and molecular techniques. RESULTS: A novel coronavirus was identified in patients with SARS. The virus was isolated in cell culture, and a sequence 300 nucleotides in length was obtained by a polymerase-chain-reaction (PCR)-based random-amplification procedure. Genetic characterization indicated that the virus is only distantly related to known coronaviruses (identical in 50 to 60 percent of the nucleotide sequence). On the basis of the obtained sequence, conventional and real-time PCR assays for specific and sensitive detection of the novel virus were established. Virus was detected in a variety of clinical specimens from patients with SARS but not in controls. High concentrations of viral RNA of up to 100 million molecules per milliliter were found in sputum. Viral RNA was also detected at extremely low concentrations in plasma during the acute phase and in feces during the late convalescent phase. Infected patients showed seroconversion on the Vero cells in which the virus was isolated. CONCLUSIONS: The novel coronavirus might have a role in causing SARS.", "date": "2003", "authors": ["Christian Drosten 1, Stephan G\u00fcnther 1, Wolfgang Preiser 2, Sylvie van der Werf 3, Hans-Reinhard Brodt 4, Stephan Becker 5, Holger Rabenau 2, Marcus Panning 1, Larissa Kolesnikova 5, Ron A.M. Fouchier 6, Annemarie Berger 2, Ana-Maria Burgui\u00e8re 3, Jindrich Cinatl 2, Markus Eickmann 5, Nicolas Escriou 3, Klaus Grywna 1, Stefanie Kramme 1, Jean-Claude Manuguerra 3, Stefanie M\u00fcller 1, Volker Rickerts 4, Martin St\u00fcrmer 2, Simon Vieth 1, Hans-Dieter Klenk 5, Albert D.M.E. Osterhaus 6, Herbert Schmitz 1, Hans Wilhelm Doerr 2"], "references": [2100820722, 2125251240, 2107922358, 2127062009, 2084994773, 2149579937, 2090060897, 2004869546, 2030133843]}, {"id": 2104548316, "title": "A novel coronavirus associated with severe acute respiratory syndrome.", "abstract": "background A worldwide outbreak of severe acute respiratory syndrome (SARS) has been associated with exposures originating from a single ill health care worker from Guangdong Province, China. We conducted studies to identify the etiologic agent of this outbreak. methods We received clinical specimens from patients in six countries and tested them, using virus isolation techniques, electron-microscopical and histologic studies, and molecular and serologic assays, in an attempt to identify a wide range of potential pathogens. results No classic respiratory or bacterial respiratory pathogen was consistently identified. However, a novel coronavirus was isolated from patients who met the case definition of SARS. Cytopathological features were noted microscopically in Vero E6 cells inoculated with a throat-swab specimen. Electron-microscopical examination of cultures revealed ultrastructural features characteristic of coronaviruses. Immunohistochemical and immunofluorescence staining revealed reactivity with group I coronavirus polyclonal antibodies. Consensus coronavirus primers designed to amplify a fragment of the polymerase gene by reverse transcription\u2013polymerase chain reaction (RT-PCR) were used to obtain a sequence that clearly identified the isolate as a unique coronavirus only distantly related to previously sequenced coronaviruses. With specific diagnostic RT-PCR primers we identified several identical nucleotide sequences in 12 patients from several locations, a finding consistent with a point source outbreak. Indirect fluorescent antibody tests and enzyme-linked immunosorbent assays made with the new coronavirus isolate have been used to demonstrate a virus-specific serologic response. Preliminary studies suggest that this virus may never before have infected the U.S. population. conclusions A novel coronavirus is associated with this outbreak, and the evidence indicates that this virus has an etiologic role in SARS. The name Urbani SARS-associated coronavirus is proposed for the virus.", "date": "2003", "authors": ["Ksiazek Tg 1, Erdman D 1, Goldsmith Cs 1, Zaki 1, Peret T 1, Emery S 1, Tong S 1, Urbani C 2, Comer Ja 1, Lim W 3, Rollin Pe 1, Dowell Sf 4, Ling Ae 5, Humphrey Cd 1, Shieh Wj 1, Guarner J 1, Paddock Cd 1, Rota P 1, Fields B 1, DeRisi J 6, Yang Jy 1, Cox N 1, Hughes Jm 1, LeDuc Jw 1, Bellini Wj 1, Anderson Lj 1"], "references": [2106882534, 2131262274, 2100820722, 2125251240, 2463755683, 2403756321, 2127949919, 1576737979, 2128788856, 2076620790]}, {"id": 2131262274, "title": "A Major Outbreak of Severe Acute Respiratory Syndrome in Hong Kong", "abstract": "background There has been an outbreak of the severe acute respiratory syndrome (SARS) worldwide. We report the clinical, laboratory, and radiologic features of 138 cases of suspected SARS during a hospital outbreak in Hong Kong. methods From March 11 to 25, 2003, all patients with suspected SARS after exposure to an index patient or ward were admitted to the isolation wards of the Prince of Wales Hospital. Their demographic, clinical, laboratory, and radiologic characteristics were analyzed. Clinical end points included the need for intensive care and death. Univariate and multivariate analyses were performed. results There were 66 male patients and 72 female patients in this cohort, 69 of whom were health care workers. The most common symptoms included fever (in 100 percent of the patients); chills, rigors, or both (73.2 percent); and myalgia (60.9 percent). Cough and headache were also reported in more than 50 percent of the patients. Other common findings were lymphopenia (in 69.6 percent), thrombocytopenia (44.8 percent), and elevated lactate dehydrogenase and creatine kinase levels (71.0 percent and 32.1 percent, respectively). Peripheral air-space consolidation was commonly observed on thoracic computed tomographic scanning. A total of 32 patients (23.2 percent) were admitted to the intensive care unit; 5 patients died, all of whom had coexisting conditions. In a multivariate analysis, the independent predictors of an adverse outcome were advanced age (odds ratio per decade of life, 1.80; 95 percent confidence interval, 1.16 to 2.81; P=0.009), a high peak lactate dehydrogenase level (odds ratio per 100 U per liter, 2.09; 95 percent confidence interval, 1.28 to 3.42; P=0.003), and an absolute neutrophil count that exceeded the upper limit of the normal range on presentation (odds ratio, 1.60; 95 percent confidence interval, 1.03 to 2.50; P=0.04). conclusions SARS is a serious respiratory illness that led to significant morbidity and mortality in our cohort.", "date": "2003", "authors": ["Nelson Lee 1, David Hui 1, Alan Wu 1, Paul Chan 1, Peter Cameron 2, Gavin M Joynt 1, Anil Ahuja 1, Man Yee Yung 1, C B Leung 1, K F To 1, S F Lui 1, C C Szeto 1, Sydney Chung 1, Joseph J Y Sung 1"], "references": [2123324969, 2130141864, 2463755683, 1991467275, 1982444609]}, {"id": 2006434809, "title": "Epidemiological, demographic, and clinical characteristics of 47 cases of Middle East respiratory syndrome coronavirus disease from Saudi Arabia: a descriptive study", "abstract": "Summary Background Middle East respiratory syndrome (MERS) is a new human disease caused by a novel coronavirus (CoV). Clinical data on MERS-CoV infections are scarce. We report epidemiological, demographic, clinical, and laboratory characteristics of 47 cases of MERS-CoV infections, identify knowledge gaps, and define research priorities. Methods We abstracted and analysed epidemiological, demographic, clinical, and laboratory data from confirmed cases of sporadic, household, community, and health-care-associated MERS-CoV infections reported from Saudi Arabia between Sept 1, 2012, and June 15, 2013. Cases were confirmed as having MERS-CoV by real-time RT-PCR. Findings 47 individuals (46 adults, one child) with laboratory-confirmed MERS-CoV disease were identified; 36 (77%) were male (male:female ratio 3\u00b73:1). 28 patients died, a 60% case-fatality rate. The case-fatality rate rose with increasing age. Only two of the 47 cases were previously healthy; most patients (45 [96%]) had underlying comorbid medical disorders, including diabetes (32 [68%]), hypertension (16 [34%]), chronic cardiac disease (13 [28%]), and chronic renal disease (23 [49%]). Common symptoms at presentation were fever (46 [98%]), fever with chills or rigors (41 [87%]), cough (39 [83%]), shortness of breath (34 [72%]), and myalgia (15 [32%]). Gastrointestinal symptoms were also frequent, including diarrhoea (12 [26%]), vomiting (ten [21%]), and abdominal pain (eight [17%]). All patients had abnormal findings on chest radiography, ranging from subtle to extensive unilateral and bilateral abnormalities. Laboratory analyses showed raised concentrations of lactate dehydrogenase (23 [49%]) and aspartate aminotransferase (seven [15%]) and thrombocytopenia (17 [36%]) and lymphopenia (16 [34%]). Interpretation Disease caused by MERS-CoV presents with a wide range of clinical manifestations and is associated with substantial mortality in admitted patients who have medical comorbidities. Major gaps in our knowledge of the epidemiology, community prevalence, and clinical spectrum of infection and disease need urgent definition. Funding None.", "date": "2013", "authors": ["Abdullah Assiri 1, Jaffar A Al-Tawfiq 2, Abdullah A Al-Rabeeah 1, Fahad A Al-Rabiah 3, Sami Al-Hajjar 3, Ali Al-Barrak 4, Hesham Flemban 5, Wafa N Al-Nassir 6, Hanan H Balkhy 7, Rafat F Al-Hakeem 1, Hatem Q Makhdoom 8, Alimuddin I Zumla 9, 10, Ziad A Memish 1"], "references": [2166867592, 2107053896, 2131262274, 1703839189, 2112147913, 2045002682, 1852588318, 2163627712, 2140143765, 2119775949]}, {"id": 2725497285, "title": "Broad-spectrum antiviral GS-5734 inhibits both epidemic and zoonotic coronaviruses.", "abstract": "Emerging viral infections are difficult to control because heterogeneous members periodically cycle in and out of humans and zoonotic hosts, complicating the development of specific antiviral therapies and vaccines. Coronaviruses (CoVs) have a proclivity to spread rapidly into new host species causing severe disease. Severe acute respiratory syndrome CoV (SARS-CoV) and Middle East respiratory syndrome CoV (MERS-CoV) successively emerged, causing severe epidemic respiratory disease in immunologically naive human populations throughout the globe. Broad-spectrum therapies capable of inhibiting CoV infections would address an immediate unmet medical need and could be invaluable in the treatment of emerging and endemic CoV infections. We show that a nucleotide prodrug, GS-5734, currently in clinical development for treatment of Ebola virus disease, can inhibit SARS-CoV and MERS-CoV replication in multiple in vitro systems, including primary human airway epithelial cell cultures with submicromolar IC50 values. GS-5734 was also effective against bat CoVs, prepandemic bat CoVs, and circulating contemporary human CoV in primary human lung cells, thus demonstrating broad-spectrum anti-CoV activity. In a mouse model of SARS-CoV pathogenesis, prophylactic and early therapeutic administration of GS-5734 significantly reduced lung viral load and improved clinical signs of disease as well as respiratory function. These data provide substantive evidence that GS-5734 may prove effective against endemic MERS-CoV in the Middle East, circulating human CoV, and, possibly most importantly, emerging CoV of the future.", "date": "2017", "authors": ["Timothy P. Sheahan 1, Amy C. Sims 1, Rachel L. Graham 1, Vineet D. Menachery 1, Lisa E. Gralinski 1, James B. Case 2, Sarah R. Leist 1, Krzysztof Pyrc 3, Joy Y. Feng 4, Iva Trantcheva 4, Roy Bannister 4, Yeojin Park 4, Darius Babusis 4, Michael O. Clarke 4, Richard L. Mackman 4, Jamie E. Spahn 4, Christopher A. Palmiotti 4, Dustin Siegel 4, Adrian S. Ray 4, Tomas Cihlar 4, Robert Jordan 4, Mark R. Denison 5, Ralph S. Baric 1"], "references": [2470646526, 2129542667, 2195009776, 2255243349, 2292021561, 2115555188, 2298153446, 2525468044, 1945961678, 2099941783]}, {"id": 3011483298, "title": "SARS-CoV-2 entry factors are highly expressed in nasal epithelial cells together with innate immune genes.", "abstract": "We investigated SARS-CoV-2 potential tropism by surveying expression of viral entry-associated genes in single-cell RNA-sequencing data from multiple tissues from healthy human donors. We co-detected these transcripts in specific respiratory, corneal and intestinal epithelial cells, potentially explaining the high efficiency of SARS-CoV-2 transmission. These genes are co-expressed in nasal epithelial cells with genes involved in innate immunity, highlighting the cells' potential role in initial viral infection, spread and clearance. The study offers a useful resource for further lines of inquiry with valuable clinical samples from COVID-19 patients and we provide our data in a comprehensive, open and user-friendly fashion at www.covid19cellatlas.org.", "date": "2020", "authors": ["Waradon Sungnak 1, Ni Huang 1, Christophe B\u00e9cavin 2, Marijn Berg 3, Rachel Queen 4, Monika Litvinukova 5, Carlos Talavera-L\u00f3pez 1, Henrike Maatz 5, Daniel Reichart 6, Fotios Sampaziotis 7, Kaylee B. Worlock 8, Masahiro Yoshida 8, Josephine L. Barnes 8"], "references": [3001118548, 3001897055, 3008827533, 3003668884, 3002108456, 3002539152, 3004280078, 3004239190, 3009912996, 3007940623]}, {"id": 1901129140, "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation", "abstract": "There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .", "date": "2015", "authors": ["Olaf Ronneberger , Philipp Fischer , Thomas Brox"], "references": [2618530766, 2962835968, 1903029394, 2155893237, 1677182931, 1948751323, 2167510172, 1893585201, 2148349024, 2147800946]}, {"id": 3106250896, "title": "SSD: Single Shot MultiBox Detector", "abstract": "We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. SSD is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stages and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, COCO, and ILSVRC datasets confirm that SSD has competitive accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. For \\(300 \\times 300\\) input, SSD achieves 74.3 % mAP on VOC2007 test at 59 FPS on a Nvidia Titan X and for \\(512 \\times 512\\) input, SSD achieves 76.9 % mAP, outperforming a comparable state of the art Faster R-CNN model. Compared to other single stage methods, SSD has much better accuracy even with a smaller input image size. Code is available at https://github.com/weiliu89/caffe/tree/ssd.", "date": "2016", "authors": ["Wei Liu 1, Dragomir Anguelov 2, Dumitru Erhan 3, Christian Szegedy 3, Scott E. Reed 4, Cheng-Yang Fu 1, Alexander C. Berg 1"], "references": [2194775991, 2618530766, 2962835968, 2097117768, 639708223, 1836465849, 2102605133, 2117539524, 1903029394, 2155893237]}, {"id": 3001897055, "title": "A Novel Coronavirus from Patients with Pneumonia in China, 2019.", "abstract": "In December 2019, a cluster of patients with pneumonia of unknown cause was linked to a seafood wholesale market in Wuhan, China. A previously unknown betacoronavirus was discovered through the use of unbiased sequencing in samples from patients with pneumonia. Human airway epithelial cells were used to isolate a novel coronavirus, named 2019-nCoV, which formed a clade within the subgenus sarbecovirus, Orthocoronavirinae subfamily. Different from both MERS-CoV and SARS-CoV, 2019-nCoV is the seventh member of the family of coronaviruses that infect humans. Enhanced surveillance and further investigation are ongoing. (Funded by the National Key Research and Development Program of China and the National Major Project for Control and Prevention of Infectious Disease in China.).", "date": "2020", "authors": ["Na Zhu 1, Dingyu Zhang 2, Wenling Wang 1, Xingwang Li 3, Bo Yang 1, Jingdong Song 1, Xiang Zhao 1, Baoying Huang 1, Weifeng Shi 4, Roujian Lu 1, Peihua Niu 1, Faxian Zhan 1, Xuejun Ma 1, Dayan Wang 1, Wenbo Xu 1, 5, Guizhen Wu 1, George F. Gao 6, Wenjie Tan 1"], "references": [2903899730, 2166867592, 2132260239, 2104548316, 2306794997, 1909499787, 3027518954, 2792024998, 2955025503, 2257005270]}, {"id": 3005079553, "title": "Clinical Characteristics of 138 Hospitalized Patients With 2019 Novel Coronavirus-Infected Pneumonia in Wuhan, China.", "abstract": "Importance In December 2019, novel coronavirus (2019-nCoV)\u2013infected pneumonia (NCIP) occurred in Wuhan, China. The number of cases has increased rapidly but information on the clinical characteristics of affected patients is limited. Objective To describe the epidemiological and clinical characteristics of NCIP. Design, Setting, and Participants Retrospective, single-center case series of the 138 consecutive hospitalized patients with confirmed NCIP at Zhongnan Hospital of Wuhan University in Wuhan, China, from January 1 to January 28, 2020; final date of follow-up was February 3, 2020. Exposures Documented NCIP. Main Outcomes and Measures Epidemiological, demographic, clinical, laboratory, radiological, and treatment data were collected and analyzed. Outcomes of critically ill patients and noncritically ill patients were compared. Presumed hospital-related transmission was suspected if a cluster of health professionals or hospitalized patients in the same wards became infected and a possible source of infection could be tracked. Results Of 138 hospitalized patients with NCIP, the median age was 56 years (interquartile range, 42-68; range, 22-92 years) and 75 (54.3%) were men. Hospital-associated transmission was suspected as the presumed mechanism of infection for affected health professionals (40 [29%]) and hospitalized patients (17 [12.3%]). Common symptoms included fever (136 [98.6%]), fatigue (96 [69.6%]), and dry cough (82 [59.4%]). Lymphopenia (lymphocyte count, 0.8\u2009\u00d7\u2009109/L [interquartile range {IQR}, 0.6-1.1]) occurred in 97 patients (70.3%), prolonged prothrombin time (13.0 seconds [IQR, 12.3-13.7]) in 80 patients (58%), and elevated lactate dehydrogenase (261 U/L [IQR, 182-403]) in 55 patients (39.9%). Chest computed tomographic scans showed bilateral patchy shadows or ground glass opacity in the lungs of all patients. Most patients received antiviral therapy (oseltamivir, 124 [89.9%]), and many received antibacterial therapy (moxifloxacin, 89 [64.4%]; ceftriaxone, 34 [24.6%]; azithromycin, 25 [18.1%]) and glucocorticoid therapy (62 [44.9%]). Thirty-six patients (26.1%) were transferred to the intensive care unit (ICU) because of complications, including acute respiratory distress syndrome (22 [61.1%]), arrhythmia (16 [44.4%]), and shock (11 [30.6%]). The median time from first symptom to dyspnea was 5.0 days, to hospital admission was 7.0 days, and to ARDS was 8.0 days. Patients treated in the ICU (n\u2009=\u200936), compared with patients not treated in the ICU (n\u2009=\u2009102), were older (median age, 66 years vs 51 years), were more likely to have underlying comorbidities (26 [72.2%] vs 38 [37.3%]), and were more likely to have dyspnea (23 [63.9%] vs 20 [19.6%]), and anorexia (24 [66.7%] vs 31 [30.4%]). Of the 36 cases in the ICU, 4 (11.1%) received high-flow oxygen therapy, 15 (41.7%) received noninvasive ventilation, and 17 (47.2%) received invasive ventilation (4 were switched to extracorporeal membrane oxygenation). As of February 3, 47 patients (34.1%) were discharged and 6 died (overall mortality, 4.3%), but the remaining patients are still hospitalized. Among those discharged alive (n\u2009=\u200947), the median hospital stay was 10 days (IQR, 7.0-14.0). Conclusions and Relevance In this single-center case series of 138 hospitalized patients with confirmed NCIP in Wuhan, China, presumed hospital-related transmission of 2019-nCoV was suspected in 41% of patients, 26% of patients received ICU care, and mortality was 4.3%.", "date": "2020", "authors": ["Dawei Wang , Bo Hu , Chang Hu , Fangfang Zhu , Xing Liu , Jing Zhang , Binbin Wang , Hui Xiang , Zhenshun Cheng , Yong Xiong , Yan Zhao , Yirong Li , Xinghuan Wang , Zhiyong Peng"], "references": [3001118548, 3001897055, 3003668884, 3002108456, 3002539152, 3000834295, 3003951199, 2999409984, 1803784511, 2999318660]}, {"id": 3003668884, "title": "Early Transmission Dynamics in Wuhan, China, of Novel Coronavirus-Infected Pneumonia.", "abstract": "Abstract Background The initial cases of novel coronavirus (2019-nCoV)\u2013infected pneumonia (NCIP) occurred in Wuhan, Hubei Province, China, in December 2019 and January 2020. We analyzed data on the...", "date": "2020", "authors": ["Qun Li 1, Xuhua Guan 1, Peng Wu 2, Xiaoye Wang 1, Lei Zhou 1, Yeqing Tong 1, Ruiqi Ren 1, Kathy S.M. Leung 2, Eric H.Y. Lau 2, Jessica Y. Wong 2, Xuesen Xing 1, Nijuan Xiang 1, Yang Wu 1, Chao Li 1, Qi Chen 1, Dan Li 1, Tian Liu 1, Jing Zhao 1, Man Liu 1, Wenxiao Tu 1, Chuding Chen 1, Lianmei Jin 1, Rui Yang 1, Qi Wang 1, Suhua Zhou 1, Rui Wang 1, Hui Liu 1, Yingbo Luo 1, Yuan Liu 1, Ge Shao 1, Huan Li 1, Zhongfa Tao 1, Yang Yang 3, Zhiqiang Deng 3, Boxi Liu 3, Zhitao Ma 3, Yanping Zhang 1, Guoqing Shi 1, Tommy T.Y. Lam 2, Joseph T. Wu 2, George F. Gao 1, Benjamin J. Cowling 2, Bo Yang 3, Gabriel M. Leung 2, Zijian Feng 1"], "references": [3001897055, 3002539152, 3000834295, 3002533507, 2470646526, 3002715510, 1909499787, 3001971765, 2147166346, 2149508011]}, {"id": 3002108456, "title": "Epidemiological and clinical characteristics of 99 cases of 2019 novel coronavirus pneumonia in Wuhan, China: a descriptive study", "abstract": "In December, 2019, a pneumonia associated with the 2019 novel coronavirus (2019-nCoV) emerged in Wuhan, China. We aimed to further clarify the epidemiological and clinical characteristics of 2019-nCoV pneumonia. In this retrospective, single-centre study, we included all confirmed cases of 2019-nCoV in Wuhan Jinyintan Hospital from Jan 1 to Jan 20, 2020. Cases were confirmed by real-time RT-PCR and were analysed for epidemiological, demographic, clinical, and radiological features and laboratory data. Outcomes were followed up until Jan 25, 2020.", "date": "2020", "authors": ["Nanshan Chen 1, Min Zhou 2, Xuan Dong 1, Jieming Qu 2, Fengyun Gong 1, Yang Han 1, Yang Qiu 3, Jingli Wang 1, Ying Liu 1, Yuan Wei 1, Jia'an Xia 1, Ting Yu 1, Xinxin Zhang 2, Li Zhang 1"], "references": [3001118548, 2903899730, 2166867592, 2999409984, 2999318660, 2132260239, 2999364275, 2991899552, 2909194930, 2775086803]}, {"id": 3002539152, "title": "A familial cluster of pneumonia associated with the 2019 novel coronavirus indicating person-to-person transmission: a study of a family cluster.", "abstract": "Summary Background An ongoing outbreak of pneumonia associated with a novel coronavirus was reported in Wuhan city, Hubei province, China. Affected patients were geographically linked with a local wet market as a potential source. No data on person-to-person or nosocomial transmission have been published to date. Methods In this study, we report the epidemiological, clinical, laboratory, radiological, and microbiological findings of five patients in a family cluster who presented with unexplained pneumonia after returning to Shenzhen, Guangdong province, China, after a visit to Wuhan, and an additional family member who did not travel to Wuhan. Phylogenetic analysis of genetic sequences from these patients were done. Findings From Jan 10, 2020, we enrolled a family of six patients who travelled to Wuhan from Shenzhen between Dec 29, 2019 and Jan 4, 2020. Of six family members who travelled to Wuhan, five were identified as infected with the novel coronavirus. Additionally, one family member, who did not travel to Wuhan, became infected with the virus after several days of contact with four of the family members. None of the family members had contacts with Wuhan markets or animals, although two had visited a Wuhan hospital. Five family members (aged 36\u201366 years) presented with fever, upper or lower respiratory tract symptoms, or diarrhoea, or a combination of these 3\u20136 days after exposure. They presented to our hospital (The University of Hong Kong-Shenzhen Hospital, Shenzhen) 6\u201310 days after symptom onset. They and one asymptomatic child (aged 10 years) had radiological ground-glass lung opacities. Older patients (aged >60 years) had more systemic symptoms, extensive radiological ground-glass lung changes, lymphopenia, thrombocytopenia, and increased C-reactive protein and lactate dehydrogenase levels. The nasopharyngeal or throat swabs of these six patients were negative for known respiratory microbes by point-of-care multiplex RT-PCR, but five patients (four adults and the child) were RT-PCR positive for genes encoding the internal RNA-dependent RNA polymerase and surface Spike protein of this novel coronavirus, which were confirmed by Sanger sequencing. Phylogenetic analysis of these five patients' RT-PCR amplicons and two full genomes by next-generation sequencing showed that this is a novel coronavirus, which is closest to the bat severe acute respiatory syndrome (SARS)-related coronaviruses found in Chinese horseshoe bats. Interpretation Our findings are consistent with person-to-person transmission of this novel coronavirus in hospital and family settings, and the reports of infected travellers in other geographical regions. Funding The Shaw Foundation Hong Kong, Michael Seak-Kan Tong, Respiratory Viral Research Foundation Limited, Hui Ming, Hui Hoy and Chow Sin Lan Charity Fund Limited, Marina Man-Wai Lee, the Hong Kong Hainan Commercial Association South China Microbiology Research Fund, Sanming Project of Medicine (Shenzhen), and High Level-Hospital Program (Guangdong Health Commission).", "date": "2020", "authors": ["Jasper Fuk Woo Chan 1, Shuofeng Yuan 1, Kin Hang Kok 1, Kelvin Kai Wang To 1, 2, Hin Chu 1, Jin Yang 2, Fanfan Xing 2, Jieling Liu 2, Cyril Chik Yan Yip 1, Rosana Wing Shan Poon 1, Hoi Wah Tsoi 1, Simon Kam Fai Lo 2, Kwok Hung Chan 1, Vincent Kwok Man Poon 1, Wan Mui Chan 1, Jonathan Daniel Ip 1, Jian Piao Cai 1, Vincent Chi Chung Cheng 1, Honglin Chen 1, 2, Christopher Kim Ming Hui 2, Kwok Yung Yuen 2"], "references": [2025170735, 2129542667, 2103503670, 2115555188, 2105637133, 2807736175, 2889758689, 2769543984, 2140338292, 2170933940]}, {"id": 3004318991, "title": "Genomic characterisation and epidemiology of 2019 novel coronavirus: implications for virus origins and receptor binding.", "abstract": "Summary Background In late December, 2019, patients presenting with viral pneumonia due to an unidentified microbial agent were reported in Wuhan, China. A novel coronavirus was subsequently identified as the causative pathogen, provisionally named 2019 novel coronavirus (2019-nCoV). As of Jan 26, 2020, more than 2000 cases of 2019-nCoV infection have been confirmed, most of which involved people living in or visiting Wuhan, and human-to-human transmission has been confirmed. Methods We did next-generation sequencing of samples from bronchoalveolar lavage fluid and cultured isolates from nine inpatients, eight of whom had visited the Huanan seafood market in Wuhan. Complete and partial 2019-nCoV genome sequences were obtained from these individuals. Viral contigs were connected using Sanger sequencing to obtain the full-length genomes, with the terminal regions determined by rapid amplification of cDNA ends. Phylogenetic analysis of these 2019-nCoV genomes and those of other coronaviruses was used to determine the evolutionary history of the virus and help infer its likely origin. Homology modelling was done to explore the likely receptor-binding properties of the virus. Findings The ten genome sequences of 2019-nCoV obtained from the nine patients were extremely similar, exhibiting more than 99\u00b798% sequence identity. Notably, 2019-nCoV was closely related (with 88% identity) to two bat-derived severe acute respiratory syndrome (SARS)-like coronaviruses, bat-SL-CoVZC45 and bat-SL-CoVZXC21, collected in 2018 in Zhoushan, eastern China, but were more distant from SARS-CoV (about 79%) and MERS-CoV (about 50%). Phylogenetic analysis revealed that 2019-nCoV fell within the subgenus Sarbecovirus of the genus Betacoronavirus, with a relatively long branch length to its closest relatives bat-SL-CoVZC45 and bat-SL-CoVZXC21, and was genetically distinct from SARS-CoV. Notably, homology modelling revealed that 2019-nCoV had a similar receptor-binding domain structure to that of SARS-CoV, despite amino acid variation at some key residues. Interpretation 2019-nCoV is sufficiently divergent from SARS-CoV to be considered a new human-infecting betacoronavirus. Although our phylogenetic analysis suggests that bats might be the original host of this virus, an animal sold at the seafood market in Wuhan might represent an intermediate host facilitating the emergence of the virus in humans. Importantly, structural analysis suggests that 2019-nCoV might be able to bind to the angiotensin-converting enzyme 2 receptor in humans. The future evolution, adaptation, and spread of this virus warrant urgent investigation. Funding National Key Research and Development Program of China, National Major Project for Control and Prevention of Infectious Disease in China, Chinese Academy of Sciences, Shandong First Medical University.", "date": "2020", "authors": ["Roujian Lu 1, Xiang Zhao 1, Juan Li 2, Peihua Niu 1, Bo Yang 3, Honglong Wu 4, Wenling Wang 1, Hao Song 5, Baoying Huang 1, Na Zhu 1, Yuhai Bi 5, Xuejun Ma 1, Faxian Zhan 3, Liang Wang 5, Tao Hu 2, Hong Zhou 2, Zhenhong Hu 6, Weimin Zhou 1, Li Zhao 1, Jing Chen 7, Yao Meng 1, Ji Wang 1, Yang Lin 4, Jianying Yuan 4, Zhihao Xie 4, Jinmin Ma 4, William J Liu 1, Dayan Wang 1, Wenbo Xu 1, Edward C Holmes 8, George F Gao 1, 5, Guizhen Wu 1, Weijun Chen 4, Weifeng Shi 2, Wenjie Tan 1, 5"], "references": [3001118548, 3001897055, 3002539152, 3004280078, 2103441770, 2141052558, 2166867592, 2306794997, 2804822363, 3017468735]}, {"id": 3003465021, "title": "First Case of 2019 Novel Coronavirus in the United States.", "abstract": "An outbreak of novel coronavirus (2019-nCoV) that began in Wuhan, China, has spread rapidly, with cases now confirmed in multiple countries. We report the first case of 2019-nCoV infection confirmed in the United States and describe the identification, diagnosis, clinical course, and management of the case, including the patient's initial mild symptoms at presentation with progression to pneumonia on day 9 of illness. This case highlights the importance of close coordination between clinicians and public health authorities at the local, state, and federal levels, as well as the need for rapid dissemination of clinical information related to the care of patients with this emerging infection.", "date": "2020", "authors": ["Michelle L Holshue 1, Chas DeBolt 2, Scott Lindquist , Kathy H Lofy , John Wiesman , Hollianne Bruce , Christopher Spitters , Keith Ericson , Sara Wilkerson , Ahmet Tural , George Diaz , Amanda Cohn , LeAnne Fox , Anita Patel , Susan I Gerber , Lindsay Kim , Suxiang Tong , Xiaoyan Lu , Steve Lindstrom , Mark A Pallansch , William C Weldon , Holly M Biggs , Timothy M Uyeki , Satish K Pillai"], "references": [3001118548, 3001897055, 3002539152, 3003951199, 3000413850, 2991491848, 2605343262]}, {"id": 3004239190, "title": "Transmission of 2019-nCoV Infection from an Asymptomatic Contact in Germany.", "abstract": "2019-nCoV Transmission from Asymptomatic Patient In this report, investigators in Germany detected the spread of the novel coronavirus (2019-nCoV) from a person who had recently traveled from China...", "date": "2020", "authors": ["Camilla Rothe 1, Mirjam Schunk 1, Peter Sothmann 1, Gisela Bretzel 1, Guenter Froeschl 1, Claudia Wallrauch 1, Thorbj\u00f6rn Zimmer 1, Verena Thiel 1, Christian Janke 1, Wolfgang Guggemos 2, Michael Seilmaier 2, Christian Drosten 3, Patrick Vollmar 4, Katrin Zwirglmaier 4, Sabine Zange 4, Roman W\u00f6lfel 4, Michael Hoelscher 1"], "references": [3001897055, 3001388158]}, {"id": 3003573988, "title": "Nowcasting and forecasting the potential domestic and international spread of the 2019-nCoV outbreak originating in Wuhan, China: a modelling study.", "abstract": "Summary Background Since Dec 31, 2019, the Chinese city of Wuhan has reported an outbreak of atypical pneumonia caused by the 2019 novel coronavirus (2019-nCoV). Cases have been exported to other Chinese cities, as well as internationally, threatening to trigger a global outbreak. Here, we provide an estimate of the size of the epidemic in Wuhan on the basis of the number of cases exported from Wuhan to cities outside mainland China and forecast the extent of the domestic and global public health risks of epidemics, accounting for social and non-pharmaceutical prevention interventions. Methods We used data from Dec 31, 2019, to Jan 28, 2020, on the number of cases exported from Wuhan internationally (known days of symptom onset from Dec 25, 2019, to Jan 19, 2020) to infer the number of infections in Wuhan from Dec 1, 2019, to Jan 25, 2020. Cases exported domestically were then estimated. We forecasted the national and global spread of 2019-nCoV, accounting for the effect of the metropolitan-wide quarantine of Wuhan and surrounding cities, which began Jan 23\u201324, 2020. We used data on monthly flight bookings from the Official Aviation Guide and data on human mobility across more than 300 prefecture-level cities in mainland China from the Tencent database. Data on confirmed cases were obtained from the reports published by the Chinese Center for Disease Control and Prevention. Serial interval estimates were based on previous studies of severe acute respiratory syndrome coronavirus (SARS-CoV). A susceptible-exposed-infectious-recovered metapopulation model was used to simulate the epidemics across all major cities in China. The basic reproductive number was estimated using Markov Chain Monte Carlo methods and presented using the resulting posterior mean and 95% credibile interval (CrI). Findings In our baseline scenario, we estimated that the basic reproductive number for 2019-nCoV was 2\u00b768 (95% CrI 2\u00b747\u20132\u00b786) and that 75\u2008815 individuals (95% CrI 37\u2008304\u2013130\u2008330) have been infected in Wuhan as of Jan 25, 2020. The epidemic doubling time was 6\u00b74 days (95% CrI 5\u00b78\u20137\u00b71). We estimated that in the baseline scenario, Chongqing, Beijing, Shanghai, Guangzhou, and Shenzhen had imported 461 (95% CrI 227\u2013805), 113 (57\u2013193), 98 (49\u2013168), 111 (56\u2013191), and 80 (40\u2013139) infections from Wuhan, respectively. If the transmissibility of 2019-nCoV were similar everywhere domestically and over time, we inferred that epidemics are already growing exponentially in multiple major cities of China with a lag time behind the Wuhan outbreak of about 1\u20132 weeks. Interpretation Given that 2019-nCoV is no longer contained within Wuhan, other major Chinese cities are probably sustaining localised outbreaks. Large cities overseas with close transport links to China could also become outbreak epicentres, unless substantial public health interventions at both the population and personal levels are implemented immediately. Independent self-sustaining outbreaks in major cities globally could become inevitable because of substantial exportation of presymptomatic cases and in the absence of large-scale public health interventions. Preparedness plans and mitigation interventions should be readied for quick deployment globally. Funding Health and Medical Research Fund (Hong Kong, China).", "date": "2020", "authors": ["Joseph T Wu , Kathy Leung , Gabriel M Leung"], "references": [3003668884, 3004397688, 3002764620, 2147166346, 3002533591, 2069251911, 2096145431, 1815575713, 2104595316, 1998725525]}, {"id": 2145339207, "title": "Human-level control through deep reinforcement learning", "abstract": "The theory of reinforcement learning provides a normative account, deeply rooted in psychological and neuroscientific perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory processing systems, the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopaminergic neurons and temporal difference reinforcement learning algorithms. While reinforcement learning agents have achieved some successes in a variety of domains, their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games. We demonstrate that the deep Q-network agent, receiving only the pixels and the game score as inputs, was able to surpass the performance of all previous algorithms and achieve a level comparable to that of a professional human games tester across a set of 49 games, using the same algorithm, network architecture and hyperparameters. This work bridges the divide between high-dimensional sensory inputs and actions, resulting in the first artificial agent that is capable of learning to excel at a diverse array of challenging tasks.", "date": "2015", "authors": ["Volodymyr Mnih , Koray Kavukcuoglu , David Silver , Andrei A. Rusu , Joel Veness , Marc G. Bellemare , Alex Graves , Martin Riedmiller , Andreas K. Fidjeland , Georg Ostrovski , Stig Petersen , Charles Beattie , Amir Sadik , Ioannis Antonoglou , Helen King , Dharshan Kumaran , Daan Wierstra , Shane Legg , Demis Hassabis"], "references": [2618530766, 2100495367, 2310919327, 2187089797, 1665214252, 2072128103, 2546302380, 1652505363, 2121863487, 2952509347]}, {"id": 2100495367, "title": "Reducing the Dimensionality of Data with Neural Networks", "abstract": "High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such \"autoencoder\" networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.", "date": "2006", "authors": ["G. E. Hinton , R. R. Salakhutdinov"], "references": [2136922672, 2053186076, 2001141328, 2293063825, 2121122425, 2032647857, 2021774695]}, {"id": 2163922914, "title": "Representation Learning: A Review and New Perspectives", "abstract": "The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning.", "date": "2013", "authors": ["Y. Bengio , A. Courville , P. Vincent"], "references": [2618530766, 2136922672, 3118608800, 2100495367, 2310919327, 2158899491, 2162915993, 2187089797, 1665214252, 2160815625]}, {"id": 2160815625, "title": "Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups", "abstract": "Most current speech recognition systems use hidden Markov models (HMMs) to deal with the temporal variability of speech and Gaussian mixture models (GMMs) to determine how well each state of each HMM fits a frame or a short window of frames of coefficients that represents the acoustic input. An alternative way to evaluate the fit is to use a feed-forward neural network that takes several frames of coefficients as input and produces posterior probabilities over HMM states as output. Deep neural networks (DNNs) that have many hidden layers and are trained using new methods have been shown to outperform GMMs on a variety of speech recognition benchmarks, sometimes by a large margin. This article provides an overview of this progress and represents the shared views of four research groups that have had recent successes in using DNNs for acoustic modeling in speech recognition.", "date": "2012", "authors": ["G. Hinton 1, Li Deng 2, Dong Yu 2, G. E. Dahl 1, A. Mohamed 1, N. Jaitly 1, Andrew Senior 3, V. Vanhoucke 3, P. Nguyen 3, T. N. Sainath 4, B. Kingsbury 4"], "references": [2136922672, 2100495367, 1533861849, 2116064496, 2147768505, 2145094598, 1993882792, 44815768, 1498436455, 1994197834]}, {"id": 2022508996, "title": "Learning Hierarchical Features for Scene Labeling", "abstract": "Scene labeling consists of labeling each pixel in an image with the category of the object it belongs to. We propose a method that uses a multiscale convolutional network trained from raw pixels to extract dense feature vectors that encode regions of multiple sizes centered on each pixel. The method alleviates the need for engineered features, and produces a powerful representation that captures texture, shape, and contextual information. We report results using multiple postprocessing methods to produce the final labeling. Among those, we propose a technique to automatically retrieve, from a pool of segmentation components, an optimal set of components that best explain the scene; these components are arbitrary, for example, they can be taken from a segmentation tree or from any family of oversegmentations. The system yields record accuracies on the SIFT Flow dataset (33 classes) and the Barcelona dataset (170 classes) and near-record accuracy on Stanford background dataset (eight classes), while being an order of magnitude faster than competing approaches, producing a 320\u00d7240 image labeling in less than a second, including feature extraction.", "date": "2013", "authors": ["C. Farabet 1, C. Couprie 1, L. Najman 2, Y. LeCun 1"], "references": []}, {"id": 1993882792, "title": "Acoustic Modeling Using Deep Belief Networks", "abstract": "Gaussian mixture models are currently the dominant technique for modeling the emission distribution of hidden Markov models for speech recognition. We show that better phone recognition on the TIMIT dataset can be achieved by replacing Gaussian mixture models by deep neural networks that contain many layers of features and a very large number of parameters. These networks are first pre-trained as a multi-layer generative model of a window of spectral feature vectors without making use of any discriminative information. Once the generative pre-training has designed the features, we perform discriminative fine-tuning using backpropagation to adjust the features slightly to make them better at predicting a probability distribution over the states of monophone hidden Markov models.", "date": "2011", "authors": ["A. Mohamed , G. E. Dahl , G. Hinton"], "references": [2136922672, 3118608800, 2100495367, 2116064496, 2147768505, 2159080219, 44815768, 1994197834, 2913932916, 2103359087]}, {"id": 2108069432, "title": "2012 Special Issue: Multi-column deep neural network for traffic sign classification", "abstract": "We describe the approach that won the final phase of the German traffic sign recognition benchmark. Our method is the only one that achieved a better-than-human recognition rate of 99.46%. We use a fast, fully parameterizable GPU implementation of a Deep Neural Network (DNN) that does not require careful design of pre-wired feature extractors, which are rather learned in a supervised way. Combining various DNNs trained on differently preprocessed data into a Multi-Column DNN (MCDNN) further boosts recognition performance, making the system insensitive also to variations in contrast and illumination.", "date": "2012", "authors": ["Dan Cire\u015fAn , Ueli Meier , Jonathan Masci , J\u00fcRgen Schmidhuber"], "references": []}, {"id": 2151103935, "title": "Distinctive Image Features from Scale-Invariant Keypoints", "abstract": "This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are invariant to image scale and rotation, and are shown to provide robust matching across a substantial range of affine distortion, change in 3D viewpoint, addition of noise, and change in illumination. The features are highly distinctive, in the sense that a single feature can be correctly matched with high probability against a large database of features from many images. This paper also describes an approach to using these features for object recognition. The recognition proceeds by matching individual features to a database of features from known objects using a fast nearest-neighbor algorithm, followed by a Hough transform to identify clusters belonging to a single object, and finally performing verification through least-squares solution for consistent pose parameters. This approach to recognition can robustly identify objects among clutter and occlusion while achieving near real-time performance.", "date": "2004", "authors": ["David G. Lowe"], "references": [2033819227, 2124386111, 2154422044, 2012778485, 2124404372, 1676552347, 2124087378, 2111308925, 2165497495, 1949116567]}, {"id": 2038721957, "title": "WordNet : an electronic lexical database", "abstract": "Part 1 The lexical database: nouns in WordNet, George A. Miller modifiers in WordNet, Katherine J. Miller a semantic network of English verbs, Christiane Fellbaum design and implementation of the WordNet lexical database and searching software, Randee I. Tengi. Part 2: automated discovery of WordNet relations, Marti A. Hearst representing verb alterations in WordNet, Karen T. Kohl et al the formalization of WordNet by methods of relational concept analysis, Uta E. Priss. Part 3 Applications of WordNet: building semantic concordances, Shari Landes et al performance and confidence in a semantic annotation task, Christiane Fellbaum et al WordNet and class-based probabilities, Philip Resnik combining local context and WordNet similarity for word sense identification, Claudia Leacock and Martin Chodorow using WordNet for text retrieval, Ellen M. Voorhees lexical chains as representations of context for the detection and correction of malapropisms, Graeme Hirst and David St-Onge temporal indexing through lexical chaining, Reem Al-Halimi and Rick Kazman COLOR-X - using knowledge from WordNet for conceptual modelling, J.F.M. Burg and R.P. van de Riet knowledge processing on an extended WordNet, Sanda M. Harabagiu and Dan I Moldovan appendix - obtaining and using WordNet.", "date": "2000", "authors": ["Christiane Fellbaum"], "references": []}, {"id": 2128017662, "title": "Scalable Recognition with a Vocabulary Tree", "abstract": "A recognition scheme that scales efficiently to a large number of objects is presented. The efficiency and quality is exhibited in a live demonstration that recognizes CD-covers from a database of 40000 images of popular music CD\u0092s. The scheme builds upon popular techniques of indexing descriptors extracted from local regions, and is robust to background clutter and occlusion. The local region descriptors are hierarchically quantized in a vocabulary tree. The vocabulary tree allows a larger and more discriminatory vocabulary to be used efficiently, which we show experimentally leads to a dramatic improvement in retrieval quality. The most significant property of the scheme is that the tree directly defines the quantization. The quantization and the indexing are therefore fully integrated, essentially being one and the same. The recognition quality is evaluated through retrieval on a database with ground truth, showing the power of the vocabulary tree approach, going as high as 1 million images.", "date": "2006", "authors": ["D. Nister , H. Stewenius"], "references": [2151103935, 2177274842, 2131846894, 1980911747, 2104978738, 2172188317, 2124404372, 2147717514, 2162006472, 2165497495]}, {"id": 2110764733, "title": "LabelMe: A Database and Web-Based Tool for Image Annotation", "abstract": "We seek to build a large collection of images with ground truth labels to be used for object detection and recognition research. Such data is useful for supervised learning and quantitative evaluation. To achieve this, we developed a web-based tool that allows easy image annotation and instant sharing of such annotations. Using this annotation tool, we have collected a large dataset that spans many object categories, often containing multiple instances over a wide variety of images. We quantify the contents of the dataset and compare against existing state of the art datasets used for object recognition and detection. Also, we show how to extend the dataset to automatically enhance object labels with WordNet, discover object parts, recover a depth ordering of objects in a scene, and increase the number of labels using minimal user supervision and images from the web.", "date": "2008", "authors": ["Bryan C. Russell 1, Antonio Torralba 1, Kevin P. Murphy 2, William T. Freeman 1"], "references": [2156909104, 2164598857, 2038721957, 2138451337, 2154422044, 2107034620, 1566135517, 2166049352, 2156598602, 2134557905]}, {"id": 1782590233, "title": "Labeled Faces in the Wild: A Database forStudying Face Recognition in Unconstrained Environments", "abstract": "Most face databases have been created under controlled conditions to facilitate the study of specific parameters on the face recognition problem. These parameters include such variables as position, pose, lighting, background, camera quality, and gender. While there are many applications for face recognition technology in which one can control the parameters of image acquisition, there are also many applications in which the practitioner has little or no control over such parameters. This database, Labeled Faces in the Wild, is provided as an aid in studying the latter, unconstrained, recognition problem. The database contains labeled face photographs spanning the range of conditions typically encountered in everyday life. The database exhibits \u201cnatural\u201d variability in factors such as pose, lighting, race, accessories, occlusions, and background. In addition to describing the details of the database, we provide specific experimental paradigms for which the database is suitable. This is done in an effort to make research performed with the database as consistent and comparable as possible. We provide baseline results, including results of a state of the art face recognition system combined with a face alignment system. To facilitate experimentation on the database, we provide several parallel databases, including an aligned version.", "date": "2008", "authors": ["Gary B. Huang 1, Marwan Mattar 1, Tamara Berg 2, Eric Learned-Miller 1"], "references": [3097096317, 1999478155, 2121647436, 2033419168, 2137659841, 3111480503, 2098693229, 2125310925, 2994340921, 2006793117]}, {"id": 1576445103, "title": "Caltech-256 Object Category Dataset", "abstract": "We introduce a challenging set of 256 object categories containing a total of 30607 images. The original Caltech-101 [1] was collected by choosing a set of object categories, downloading examples from Google Images and then manually screening out all images that did not fit the category. Caltech-256 is collected in a similar manner with several improvements: a) the number of categories is more than doubled, b) the minimum number of images in any category is increased from 31 to 80, c) artifacts due to image rotation are avoided and d) a new and larger clutter category is introduced for testing background rejection. We suggest several testing paradigms to measure classification performance, then benchmark the dataset using two simple metrics as well as a state-of-the-art spatial pyramid matching [2] algorithm. Finally we use the clutter category to train an interest detector which rejects uninformative background regions.", "date": "2007", "authors": ["Gregory Griffin , Alex Holub , Pietro Perona"], "references": []}, {"id": 2145607950, "title": "80 Million Tiny Images: A Large Data Set for Nonparametric Object and Scene Recognition", "abstract": "With the advent of the Internet, billions of images are now freely available online and constitute a dense sampling of the visual world. Using a variety of non-parametric methods, we explore this world with the aid of a large dataset of 79,302,017 images collected from the Internet. Motivated by psychophysical results showing the remarkable tolerance of the human visual system to degradations in image resolution, the images in the dataset are stored as 32 x 32 color images. Each image is loosely labeled with one of the 75,062 non-abstract nouns in English, as listed in the Wordnet lexical database. Hence the image database gives a comprehensive coverage of all object categories and scenes. The semantic information from Wordnet can be used in conjunction with nearest-neighbor methods to perform object classification over a range of semantic levels minimizing the effects of labeling noise. For certain classes that are particularly prevalent in the dataset, such as people, we are able to demonstrate a recognition performance comparable to class-specific Viola-Jones style detectors.", "date": "2008", "authors": ["A. Torralba 1, R. Fergus 2, W.T. Freeman 1"], "references": []}, {"id": 2141282920, "title": "Labeling images with a computer game", "abstract": "We introduce a new interactive system: a game that is fun and can be used to create valuable output. When people play the game they help determine the contents of images by providing meaningful labels for them. If the game is played as much as popular online games, we estimate that most images on the Web can be labeled in a few months. Having proper labels associated with each image on the Web would allow for more accurate image search, improve the accessibility of sites (by providing descriptions of images to visually impaired individuals), and help users block inappropriate images. Our system makes a significant contribution because of its valuable output and because of the way it addresses the image-labeling problem. Rather than using computer vision techniques, which don't work well enough, we encourage people to do the work by taking advantage of their desire to be entertained.", "date": "2004", "authors": ["Luis von Ahn , Laura Dabbish"], "references": [1666447063, 1934863104, 2166770390, 1587328194, 2293605478, 2055225264, 2050457084, 181417509, 2612148268]}, {"id": 2115733720, "title": "One-shot learning of object categories", "abstract": "Learning visual models of object categories notoriously requires hundreds or thousands of training examples. We show that it is possible to learn much information about a category from just one, or a handful, of images. The key insight is that, rather than learning from scratch, one can take advantage of knowledge coming from previously learned categories, no matter how different these categories might be. We explore a Bayesian implementation of this idea. Object categories are represented by probabilistic models. Prior knowledge is represented as a probability density function on the parameters of these models. The posterior model for an object category is obtained by updating the prior in the light of one or more observations. We test a simple implementation of our algorithm on a database of 101 diverse object categories. We compare category models learned by an implementation of our Bayesian approach to models learned from by maximum likelihood (ML) and maximum a posteriori (MAP) methods. We find that on a database of more than 100 categories, the Bayesian approach produces informative models when the number of training examples is too small for other methods to operate successfully.", "date": "2006", "authors": ["Li Fei-Fei 1, R. Fergus 2, P. Perona 2"], "references": [2164598857, 2310919327, 2124386111, 2217896605, 2154422044, 2045656233, 2166049352, 2134557905, 2130416410, 2049633694]}, {"id": 1528789833, "title": "TextonBoost : joint appearance, shape and context modeling for multi-class object recognition and segmentation", "abstract": "This paper proposes a new approach to learning a discriminative model of object classes, incorporating appearance, shape and context information efficiently. The learned model is used for automatic visual recognition and semantic segmentation of photographs. Our discriminative model exploits novel features, based on textons, which jointly model shape and texture. Unary classification and feature selection is achieved using shared boosting to give an efficient classifier which can be applied to a large number of classes. Accurate image segmentation is achieved by incorporating these classifiers in a conditional random field. Efficient training of the model on very large datasets is achieved by exploiting both random feature selection and piecewise training methods. High classification and segmentation accuracy are demonstrated on three different databases: i) our own 21-object class database of photographs of real objects viewed under general lighting conditions, poses and viewpoints, ii) the 7-class Corel subset and iii) the 7-class Sowerby database used in [1]. The proposed algorithm gives competitive results both for highly textured (e.g. grass, trees), highly structured (e.g. cars, faces, bikes, aeroplanes) and articulated objects (e.g. body, cow).", "date": "2006", "authors": ["Jamie Shotton 1, John Winn 2, Carsten Rother 2, Antonio Criminisi 2"], "references": [2164598857, 2147880316, 2057175746, 2154422044, 2124351162, 2024046085, 2169551590, 1666447063, 2168002178, 1484228140]}, {"id": 3008818676, "title": "The epidemiological characteristics of an outbreak of 2019 novel coronavirus diseases (COVID-19) in China", "abstract": "Objective An outbreak of 2019 novel coronavirus diseases (COVID-19) in Wuhan, China has spread quickly nationwide. Here, we report results of a descriptive, exploratory analysis of all cases diagnosed as of February 11, 2020. Methods All COVID-19 cases reported through February 11, 2020 were extracted from China\u2019s Infectious Disease Information System. Analyses included: 1) summary of patient characteristics; 2) examination of age distributions and sex ratios; 3) calculation of case fatality and mortality rates; 4) geo-temporal analysis of viral spread; 5) epidemiological curve construction; and 6) subgroup analysis. Results A total of 72 314 patient records-44 672 (61.8%) confirmed cases, 16 186 (22.4%) suspected cases, 10567 (14.6%) clinical diagnosed cases (Hubei only), and 889 asymptomatic cases (1.2%)-contributed data for the analysis. Among confirmed cases, most were aged 30-79 years (86.6%), diagnosed in Hubei (74.7%), and considered mild/mild pneumonia (80.9%). A total of 1 023 deaths occurred among confirmed cases for an overall case-fatality rate of 2.3%. The COVID-19 spread outward from Hubei sometime after December 2019 and by February 11, 2020, 1 386 counties across all 31 provinces were affected. The epidemic curve of onset of symptoms peaked in January 23-26, then began to decline leading up to February 11. A total of 1 716 health workers have become infected and 5 have died (0.3%). Conclusions The COVID-19 epidemic has spread very quickly. It only took 30 days to expand from Hubei to the rest of Mainland China. With many people returning from a long holiday, China needs to prepare for the possible rebound of the epidemic. Key words: 2019 Novel Coronavirus; Outbreak; Epidemiological characteristics", "date": "2020", "authors": ["Novel Coronavirus Pneumonia Emergency Response Epidemiology Team"], "references": [3033453353, 3034593359, 3035018050, 3033301213, 3021916232, 3037552531, 3031029566, 3037451072, 3037851904]}, {"id": 3004906315, "title": "CT Imaging Features of 2019 Novel Coronavirus (2019-nCoV).", "abstract": "In this retrospective case series, chest CT scans of 21 symptomatic patients from China infected with the 2019 novel coronavirus (2019-nCoV) were reviewed, with emphasis on identifying and characterizing the most common findings. Typical CT findings included bilateral pulmonary parenchymal ground-glass and consolidative pulmonary opacities, sometimes with a rounded morphology and a peripheral lung distribution. Notably, lung cavitation, discrete pulmonary nodules, pleural effusions, and lymphadenopathy were absent. Follow-up imaging in a subset of patients during the study time window often demonstrated mild or moderate progression of disease, as manifested by increasing extent and density of lung opacities.", "date": "2020", "authors": ["Michael Chung 1, Adam Bernheim 1, Xueyan Mei 1, Ning Zhang 2, Mingqian Huang 1, Xianjun Zeng 2, Jiufa Cui 3, Wenjian Xu 3, Yang Yang 1, Zahi A. Fayad 1, Adam Jacobi 1, Kunwei Li 4, Shaolin Li 4, Hong Shan 4"], "references": [2102634410, 2800783955, 2112136274, 2056155046, 2279340859, 2080286891, 2162899218]}, {"id": 3006110666, "title": "Chest CT for Typical 2019-nCoV Pneumonia: Relationship to Negative RT-PCR Testing.", "abstract": "Some patients with positive chest CT findings may present with negative results of real-time reverse-transcription polymerase chain reaction (RT-PCR) tests for coronavirus disease 2019 (COVID-19). In this study, the authors present chest CT findings from five patients with COVID-19 infection who had initial negative RT-PCR results. All five patients had typical imaging findings, including ground-glass opacity (five patients) and/or mixed ground-glass opacity and mixed consolidation (two patients). After isolation for presumed COVID-19 pneumonia, all patients were eventually confirmed to have COVID-19 infection by means of repeated swab tests. A combination of repeated swab tests and CT scanning may be helpful for individuals with a high clinical suspicion of COVID-19 infection but negative findings at RT-PCR screening.", "date": "2020", "authors": ["Xingzhi Xie , Zheng Zhong , Wei Zhao , Chao Zheng , Fei Wang , Jun Liu"], "references": [3004906315, 3005272159, 2112136274, 2056155046]}, {"id": 3006643024, "title": "Time Course of Lung Changes at Chest CT during Recovery from Coronavirus Disease 2019 (COVID-19).", "abstract": "Background Chest CT is used to assess the severity of lung involvement in coronavirus disease 2019 (COVID-19). Purpose To determine the changes in chest CT findings associated with COVID-19 from initial diagnosis until patient recovery. Materials and Methods This retrospective review included patients with real-time polymerase chain reaction-confirmed COVID-19 who presented between January 12, 2020, and February 6, 2020. Patients with severe respiratory distress and/or oxygen requirement at any time during the disease course were excluded. Repeat chest CT was performed at approximately 4-day intervals. Each of the five lung lobes was visually scored on a scale of 0 to 5, with 0 indicating no involvement and 5 indicating more than 75% involvement. The total CT score was determined as the sum of lung involvement, ranging from 0 (no involvement) to 25 (maximum involvement). Results Twenty-one patients (six men and 15 women aged 25-63 years) with confirmed COVID-19 were evaluated. A total of 82 chest CT scans were obtained in these patients, with a mean interval (\u00b1standard deviation) of 4 days \u00b1 1 (range, 1-8 days). All patients were discharged after a mean hospitalization period of 17 days \u00b1 4 (range, 11-26 days). Maximum lung involved peaked at approximately 10 days (with a calculated total CT score of 6) from the onset of initial symptoms (R2 = 0.25, P < .001). Based on quartiles of chest CT scans from day 0 to day 26 involvement, four stages of lung CT findings were defined. CT scans obtained in stage 1 (0-4 days) showed ground-glass opacities (18 of 24 scans [75%]), with a mean total CT score of 2 \u00b1 2; scans obtained in stage 2 (5-8 days) showed an increase in both the crazy-paving pattern (nine of 17 scans [53%]) and total CT score (mean, 6 \u00b1 4; P = .002); scans obtained in stage 3 (9-13 days) showed consolidation (19 of 21 scans [91%]) and a peak in the total CT score (mean, 7 \u00b1 4); and scans obtained in stage 4 (\u226514 days) showed gradual resolution of consolidation (15 of 20 scans [75%]) and a decrease in the total CT score (mean, 6 \u00b1 4) without crazy-paving pattern. Conclusion In patients recovering from coronavirus disease 2019 (without severe respiratory distress during the disease course), lung abnormalities on chest CT scans showed greatest severity approximately 10 days after initial onset of symptoms. \u00a9 RSNA, 2020.", "date": "2020", "authors": ["Feng Pan , Tianhe Ye , Peng Sun , Shan Gui , Bo Liang , Lingli Li , Dandan Zheng , Jiazheng Wang , Richard L Hesketh , Lian Yang , Chuansheng Zheng"], "references": [3001118548, 3001897055, 3005079553, 3003668884, 3004906315, 2102634410, 2800783955, 3004802901, 2092969802, 2056155046]}, {"id": 3006354146, "title": "Initial CT findings and temporal changes in patients with the novel coronavirus pneumonia (2019-nCoV): a study of 63 patients in Wuhan, China.", "abstract": "The purpose of this study was to observe the imaging characteristics of the novel coronavirus pneumonia. Sixty-three confirmed patients were enrolled from December 30, 2019 to January 31, 2020. High-resolution CT (HRCT) of the chest was performed. The number of affected lobes, ground glass nodules (GGO), patchy/punctate ground glass opacities, patchy consolidation, fibrous stripes and irregular solid nodules in each patient's chest CT image were recorded. Additionally, we performed imaging follow-up of these patients. CT images of 63 confirmed patients were collected. M/F ratio: 33/30. The mean age was 44.9 \u00b1 15.2 years. The mean number of affected lobes was 3.3 \u00b1 1.8. Nineteen (30.2%) patients had one affected lobe, five (7.9%) patients had two affected lobes, four (6.3%) patients had three affected lobes, seven (11.1%) patients had four affected lobes while 28 (44.4%) patients had 5 affected lobes. Fifty-four (85.7%) patients had patchy/punctate ground glass opacities, 14 (22.2%) patients had GGO, 12 (19.0%) patients had patchy consolidation, 11 (17.5%) patients had fibrous stripes and 8 (12.7%) patients had irregular solid nodules. Fifty-four (85.7%) patients progressed, including single GGO increased, enlarged and consolidated; fibrous stripe enlarged, while solid nodules increased and enlarged. Imaging changes in novel viral pneumonia are rapid. The manifestations of the novel coronavirus pneumonia are diverse. Imaging changes of typical viral pneumonia and some specific imaging features were observed. Therefore, we need to strengthen the recognition of image changes to help clinicians to diagnose quickly and accurately. \u2022 High-resolution CT (HRCT) of the chest is critical for early detection, evaluation of disease severity and follow-up of patients with the novel coronavirus pneumonia. \u2022 The manifestations of the novel coronavirus pneumonia are diverse and change rapidly. \u2022 Radiologists should be aware of the various features of the disease and temporal changes.", "date": "2020", "authors": ["Yueying Pan , Hanxiong Guan , Shuchang Zhou , Yujin Wang , Qian Li , Tingting Zhu , Qiongjie Hu , Liming Xia"], "references": [3001118548, 3002539152, 3001465255, 3001456238]}, {"id": 3003901880, "title": "CT Imaging of the 2019 Novel Coronavirus (2019-nCoV) Pneumonia.", "abstract": "", "date": "2020", "authors": ["Junqiang Lei , Junfeng Li , Xun Li , Xiaolong Qi"], "references": [3002533507]}, {"id": 3005656138, "title": "Use of Chest CT in Combination with Negative RT-PCR Assay for the 2019 Novel Coronavirus but High Clinical Suspicion.", "abstract": "", "date": "2020", "authors": ["Peikai Huang , Tianzhu Liu , Lesheng Huang , Hailong Liu , Ming Lei , Wangdong Xu , Xiaolu Hu , Jun Chen , Bo Liu"], "references": [3001897055, 3004668429]}, {"id": 3004511262, "title": "Evolution of CT Manifestations in a Patient Recovered from 2019 Novel Coronavirus (2019-nCoV) Pneumonia in Wuhan, China.", "abstract": "", "date": "2020", "authors": ["Heshui Shi 1, Xiaoyu Han 2, Chuansheng Zheng"], "references": [3007497549, 3008627141, 3025334942, 3009992310, 3008928918, 3008801544, 3034593359, 3011414603, 3013468450]}, {"id": 3008962515, "title": "Molecular and serological investigation of 2019-nCoV infected patients: implication of multiple shedding routes.", "abstract": "In December 2019, a novel coronavirus (2019-nCoV) caused an outbreak in Wuhan, China, and soon spread to other parts of the world. It was believed that 2019-nCoV was transmitted through respiratory tract and then induced pneumonia, thus molecular diagnosis based on oral swabs was used for confirmation of this disease. Likewise, patient will be released upon two times of negative detection from oral swabs. However, many coronaviruses can also be transmitted through oral-fecal route by infecting intestines. Whether 2019-nCoV infected patients also carry virus in other organs like intestine need to be tested. We conducted investigation on patients in a local hospital who were infected with this virus. We found the presence of 2019-nCoV in anal swabs and blood as well, and more anal swab positives than oral swab positives in a later stage of infection, suggesting shedding and thereby transmitted through oral-fecal route. We also showed serology test can improve detection positive rate thus should be used in future epidemiology. Our report provides a cautionary warning that 2019-nCoV may be shed through multiple routes.", "date": "2020", "authors": ["Wei Zhang 1, Rong-Hui Du 2, Bei Li 1, Xiao-Shuang Zheng 1, Xing-Lou Yang 1, Ben Hu 1, Yan-Yi Wang 1, Geng-Fu Xiao 1, Bing Yan 1, Zheng-Li Shi 1, Peng Zhou 1"], "references": [3001118548, 3001897055, 3003668884, 3004280078, 2786098272, 2769543984, 2021442163, 3025232310, 3028321619, 3027541845]}, {"id": 3008452791, "title": "Viral load of SARS-CoV-2 in clinical samples.", "abstract": "", "date": "2020", "authors": ["Yang Pan 1, Daitao Zhang 2, Peng Yang 1, Leo L M Poon 3, Quanyi Wang 2"], "references": [3004318991, 2129542667, 3003637715]}, {"id": 3033453353, "title": "Recent Understandings Toward Coronavirus Disease 2019 (COVID-19): From Bench to Bedside", "abstract": "In late December 2019, an unprecedented outbreak of coronavirus disease 2019 (COVID-19) caused by SARS coronavirus 2 (SARS-CoV-2) (previously named 2019-nCoV) in Wuhan became the most challenging health emergency. Since its rapid spread in China and many other countries, the World Health Organization (WHO) declared COVID-19 a public health emergency of international concern (PHEIC) on 30th January 2020 and a pandemic on 11th March 2020. Thousands of people have died, and there are currently no vaccines or specific antiviral drugs for COVID-19. Therefore, it is critical to have a comprehensive understanding of the virus. In this review, we highlight the etiology, epidemiology, pathogenesis and pathology, clinical characteristics, diagnosis, clinical management, prognosis, infection control and prevention of COVID-19 based on recent studies.", "date": "2020", "authors": ["Jie Yu , Peiwei Chai , Shengfang Ge , Xianqun Fan"], "references": [3001118548, 3001897055, 3005079553, 3003668884, 3002108456, 3009885589, 3004280078, 3004318991, 3003465021, 3004239190]}, {"id": 3034408674, "title": "Risk assessment of mixed and displacement ventilation (LAF) during orthopedic and trauma surgery on COVID-19 patients with increased release of infectious aerosols", "abstract": "No abstract available Keywords: Displacement ventilation; SARS-CoV-2 spread; laminar air flow; mixed ventilation; negative pressure; no ventilation.", "date": "2020", "authors": ["Axel Kramer 1, R\u00fcdiger K\u00fclpmann 2, Arnold Brunner 3, Michael M\u00fcller 4, Georgi Wassilew 5"], "references": [3006961006, 3010604545]}, {"id": 3035275617, "title": "Recreational waters - A potential transmission route for SARS-CoV-2 to humans?", "abstract": "Coronavirus disease 2019 (COVID-19), the respiratory illness caused by the novel virus, severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), which has lead to high morbidity and mortality rates worldwide, has been causing major public health concerns since first detected in late 2019. Following identification of novel pathogens, questions in relation to dissemination of the pathogen and transmission routes begin to emerge. This rapidly spreading SARS-CoV-2 virus has been detected in both faecal and wastewater samples across the globe, highlighting the potential for faecal-oral transmission of the virus. As a result, concerns regarding the transmission of the virus in the environment and the risk associated with contracting the virus in recreational waters, particularly where inadequately treated wastewater is discharged, have been emerging in recent weeks. This paper highlights the need for further research to be carried out to investigate the presence, infectivity and viability of this newly identified SARS-CoV-2 virus in wastewater effluent and receiving recreational waters.", "date": "2020", "authors": ["Niamh Cahill , Dearbh\u00e1ile Morris"], "references": [3004280078, 3003465021, 3010604545, 3013893137, 3004824173, 3003464757, 3009834387, 3011863580, 3006846061, 3010096538]}, {"id": 3034059415, "title": "Impact of Lockdown on the Epidemic Dynamics of COVID-19 in France", "abstract": "The COVID-19 epidemic was reported in the Hubei province in China in December 2019 and then spread around the world reaching the pandemic stage at the beginning of March 2020. Since then, several countries went into lockdown. Using a mechanistic-statistical formalism, we estimate the effect of the lockdown in France on the contact rate and the effective reproduction number R e of the COVID-19. We obtain a reduction by a factor 7 (R e = 0.47, 95%-CI: 0.45-0.50), compared to the estimates carried out in France at the early stage of the epidemic. We also estimate the fraction of the population that would be infected by the beginning of May, at the official date at which the lockdown should be relaxed. We find a fraction of 3.7% (95%-CI: 3.0-4.8%) of the total French population, without taking into account the number of recovered individuals before April 1st, which is not known. This proportion is seemingly too low to reach herd immunity. Thus, even if the lockdown strongly mitigated the first epidemic wave, keeping a low value of R e is crucial to avoid an uncontrolled second wave (initiated with much more infectious cases than the first wave) and to hence avoid the saturation of hospital facilities.", "date": "2020", "authors": ["Lionel Roques 1, Etienne K. Klein 1, Julien Papa\u00efx 1, Antoine Sar 2, Samuel Soubeyrand 1"], "references": [3003668884, 3009885589, 3008443627, 3010604545, 3013967887, 3015571324, 3006642361, 3004397688, 3012789146, 3013594674]}, {"id": 3033952286, "title": "Real-time reverse transcription loop-mediated isothermal amplification for rapid detection of SARS-CoV-2", "abstract": "Background Highly sensitive real-time reverse transcription polymerase chain reaction (RT-qPCR) methods have been developed for the detection of SARS-CoV-2. However, they are costly. Loop-mediated isothermal amplification (LAMP) assay has emerged as a novel alternative isothermal amplification method for the detection of nucleic acid. Methods A rapid, sensitive and specific real-time reverse transcription LAMP (RT-LAMP) assay was developed for SARS-CoV-2 detection. Results This assay detected one copy/reaction of SARS-CoV-2 RNA in 30 min. Both the clinical sensitivity and specificity of this assay were 100%. The RT-LAMP showed comparable performance with RT-qPCR. Combining simplicity and cost-effectiveness, this assay is therefore recommended for use in resource resource-limited settings.", "date": "2020", "authors": ["Yee Ling Lau 1, Ilyiana Ismail 2, Nur Izati Mustapa 2, Meng Yee Lai 1, Tuan Suhaila Tuan Soh 2, Afifah Hassan 2, Kalaiarasu M Peariasamy 3, Yee Leng Lee 3, Yoong Min Chong 1, I-Ching Sam 1, Pik Pin Goh 4"], "references": [3001195213, 3010604545, 2105275554, 3011969828, 2770752141, 2263084061, 2175815746, 1991420168, 2207764089, 2073600962]}, {"id": 3035464429, "title": "Sampling and detection of corona viruses in air: A mini review.", "abstract": "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) is a strain of coronaviruses that causes coronavirus disease 2019 (COVID-19). In these days, the spread of the SARS-CoV-2 virus through the air has become a controversial topic among scientists. Various organizations provide standard methods for monitoring biological agents in the air. Nevertheless, there has been no standard recommended method for sampling and determination of viruses in air. This manuscript aimed at reviewing published papers for sampling and detection of corona viruses, especially SARS-Cov-2 as a global health concern. It was found that SARS-Cov 2 was present in some air samples that were collected from patient's rooms in hospitals. This result warrants its airborne transmission potential. However, due to the fact that in the most reviewed studies, sampling was performed in the patient's room, it seems difficult to discriminate whether it is airborne or is transmitted through respiratory droplets. Moreover, some other disrupting factors such as patient distance from the sampler, using protective or oxygen masks by patients, patient activities, coughing and sneezing during sampling time, air movement, air conditioning, sampler type, sampling conditions, storage and transferring conditions, can affect the results. About the sampling methods, most of the used samplers such as PTFE filters, gelatin filers and cyclones showed suitable performance for trapping SARS-Co and MERS-Cov viruses followed by PCR analysis.", "date": "2020", "authors": ["Ali Reza Rahmani 1, Mostafa Leili 1, Ghasem Azarian 1, Ali Poormohammadi 2"], "references": [3010604545, 2132260239, 3010449299, 1981646999, 2103503670, 3018334611, 3015704123, 2158121945, 3030968929, 2337456675]}, {"id": 3036958556, "title": "Involvement of digestive system in COVID-19: manifestations, pathology, management and challenges", "abstract": "The pandemic of novel coronavirus disease (COVID-19) has developed as a tremendous threat to global health. Although most COVID-19 patients present with respiratory symptoms, some present with gastrointestinal (GI) symptoms like diarrhoea, loss of appetite, nausea/vomiting and abdominal pain as the major complaints. These features may be attributable to the following facts: (a) COVID-19 is caused by the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), and its receptor angiotensin converting enzyme 2 (ACE2) was found to be highly expressed in GI epithelial cells, providing a prerequisite for SARS-CoV-2 infection; (b) SARS-CoV-2 viral RNA has been found in stool specimens of infected patients, and 20% of patients showed prolonged presence of SARS-CoV-2 RNA in faecal samples after the virus converting to negative in the respiratory system. These findings suggest that SARS-CoV-2 may be able to actively infect and replicate in the GI tract. Moreover, GI infection could be the first manifestation antedating respiratory symptoms; patients suffering only digestive symptoms but no respiratory symptoms as clinical manifestation have also been reported. Thus, the implications of digestive symptoms in patients with COVID-19 is of great importance. In this review, we summarise recent findings on the epidemiology of GI tract involvement, potential mechanisms of faecal-oral transmission, GI and liver manifestation, pathological/histological features in patients with COVID-19 and the diagnosis, management of patients with pre-existing GI and liver diseases as well as precautions for preventing SARS-CoV-2 infection during GI endoscopy procedures.", "date": "2020", "authors": ["Song Su 1, Jun Shen 2, Liangru Zhu 3, Yun Qiu 4, Jin-Shen He 4, Jin-Yu Tan 4, Marietta Iacucci 5, Siew C Ng 6, Subrata Ghosh 5, Ren Mao 4, Jie Liang 7"], "references": [3001118548, 3008827533, 3005079553, 3002108456, 3002539152, 3003465021, 3008090866, 3007940623, 3010604545, 3011242477]}, {"id": 3028749392, "title": "A Collaborative Multidisciplinary Approach to the Management of Coronavirus Disease 2019 in the Hospital Setting.", "abstract": "The novel severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) causes coronavirus disease 2019 (COVID-19), which presents an unprecedented challenge to medical providers worldwide. Although most SARS-CoV-2-infected individuals manifest with a self-limited mild disease that resolves with supportive care in the outpatient setting, patients with moderate to severe COVID-19 will require a multidisciplinary collaborative management approach for optimal care in the hospital setting. Laboratory and radiologic studies provide critical information on disease severity, management options, and overall prognosis. Medical management is mostly supportive with antipyretics, hydration, oxygen supplementation, and other measures as dictated by clinical need. Among its medical complications is a characteristic proinflammatory cytokine storm often associated with end-organ dysfunction, including respiratory failure, liver and renal insufficiency, cardiac injury, and coagulopathy. Specific recommendations for the management of these medical complications are discussed. Despite the issuance of emergency use authorization for remdesivir, there are still no proven effective antiviral and immunomodulatory therapies, and their use in COVID-19 management should be guided by clinical trial protocols or treatment registries. The medical care of patients with COVID-19 extends beyond their hospitalization. Postdischarge follow-up and monitoring should be performed, preferably using telemedicine, until the patients have fully recovered from their illness and are released from home quarantine protocols.", "date": "2020", "authors": ["Raymund R. Razonable , Kelly M. Pennington , Anne M. Meehan , John W. Wilson , Adam T. Froemming , Courtney E. Bennett , Ariela L. Marshall , Abinash Virk , Eva M. Carmona"], "references": [3001118548, 3001897055, 3008827533, 3005079553, 3003668884, 3002108456, 3009885589, 3002539152, 3008028633, 3004318991]}, {"id": 3032185657, "title": "Trajectory of the COVID-19 pandemic: chasing a moving target", "abstract": "The spread of COVID-19 has already taken a pandemic form, affecting over 180 countries in a matter of three months. The full continuum of disease ranges from mild, self-limiting illness to severe progressive COVID-19 pneumonia, multiorgan failure, cytokine storm and death. Younger and healthy population is now getting affected than before. Possibilities of airborne and fecal oral routes of transmission has increased the concern. In the absence of any specific therapeutic agent for coronavirus infections, the most effective manner to contain this pandemic is probably the non-pharmacological interventions (NPIs). The damage due to the pandemic disease is multifaceted and crippling to economy, trade, and health of the citizens of the countries. The extent of damage in such scenarios is something that is beyond calculation by Gross Domestic Product rate or currency value of the country. Unfortunately, unlike many other diseases, we are still away from the target antiviral drug and vaccine for severe acute respiratory syndrome (SARS-CoV-2) infection. The prime importance of NPIs like social distancing, staying in home, work from home, self-monitoring, public awareness, self-quarantine, etc. are constantly being emphasized by CDC, WHO, health ministries of all countries and social media houses. This is time of introspection and learning from our mistakes. Countries like China and South Korea who were initially the most hit countries could contain the disease spread by liberal testing of their population, stringent quarantine of people under investigation and isolation of the positive cases. Rest of the countries need to act urgently as well to bring an immediate halt in the community transmission.", "date": "2020", "authors": ["Kamal Kant Sahu 1, Ajay Kumar Mishra 1, Amos Lal 2"], "references": [3001118548, 3008827533, 3005079553, 3003668884, 3009885589, 3008028633, 3008090866, 3007497549, 3010930696, 3011242477]}, {"id": 3042098369, "title": "Laboratory Tests for COVID-19: A Review of Peer-Reviewed Publications and Implications for Clinical UIse", "abstract": "Diagnostic tests for the coronavirus infection 2019 (COVID-19) are critical for prompt diagnosis, treatment and isolation to break the cycle of transmission. A positive real-time reverse-transcriptase polymerase chain reaction (RT-PCR), in conjunction with clinical and epidemiologic data, is the current standard for diagnosis, but several challenges still exist. Serological assays help to understand epidemiology better and to evaluate vaccine responses but they are unreliable for diagnosis in the acute phase of illness or assuming protective immunity. Serology is gaining attention, mainly because of convalescent plasma gaining importance as treatment for clinically worsening COVID-19 patients. We provide a narrative review of peer-reviewed research studies on RT-PCR, serology and antigen immune-assays for COVID-19, briefly describe their lab methods and discuss their limitations for clinical practice.", "date": "2020", "authors": ["Daniel Shyu , James Dorroh , Caleb Holtmeyer , Detlef Ritter , Anandhi Upendran , Raghuraman Kannan , Dima Dandachi , Christian Rojas-Moreno , Stevan P Whitt , Hariharan Regunath"], "references": [3001118548, 3005079553, 3003668884, 3009885589, 3002539152, 3004280078, 3001195213, 3007497549, 3010604545, 3013893137]}, {"id": 3037255629, "title": "COVID-19 in children: An ample review", "abstract": "The aim of this review was to describe the current knowledge about coronavirus disease 2019 (COVID-19, which is caused by severe acute respiratory syndrome coronavirus 2 [SARS-CoV-2]) in children, from epidemiological, clinical, and laboratory perspectives, including knowledge on the disease course, treatment, and prognosis. An extensive literature search was performed to identify papers on COVID-19 (SARS-CoV-2 infection) in children, published between January 1, 2020 and April 1, 2020. There were 44 relevant papers on COVID-19 in children. The results showed that COVID-19 occurs in 0.39-12.3% of children. Clinical signs and symptoms are comparable to those in adults, but milder forms and a large percentage of asymptomatic carriers are found among children. Elevated inflammatory markers are associated with complications and linked to various co-infections. Chest computed tomography (CT) scans in children revealed structural changes similar to those found in adults, with consolidations surrounded by halos being somewhat specific for children with COVID-19. The recommended treatment includes providing symptomatic therapy, with no specific drug recommendations for children. The prognosis is much better for children compared to adults. This review highlights that COVID-19 in children is similar to the disease in the adult population, but with particularities regarding clinical manifestations, laboratory test results, chest imaging, and treatment. The prognosis is much better for children compared to adults, but with the progression of the pandemic; the cases in children might change in the future.", "date": "2020", "authors": ["Ioana M Ciuca"], "references": [3001118548, 3001897055, 3008827533, 3005079553, 3002108456, 3008028633, 3007940623, 3007497549, 3010604545, 3010930696]}, {"id": 2962739339, "title": "Deep contextualized word representations", "abstract": "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.", "date": "2018", "authors": ["Matthew E. Peters 1, Mark Neumann 1, Mohit Iyyer 2, Matt Gardner 3, Christopher Clark 1, Kenton Lee 4, Luke Zettlemoyer 5"], "references": [2964121744, 2153579005, 2095705004, 2250539671, 2158899491, 2064675550, 2147880316, 2251939518, 2493916176, 2963748441]}, {"id": 2331128040, "title": "Perceptual Losses for Real-Time Style Transfer and Super-Resolution", "abstract": "We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks using a per-pixel loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by defining and optimizing perceptual loss functions based on high-level features extracted from pretrained networks. We combine the benefits of both approaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al. in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results.", "date": "2016", "authors": ["Justin Johnson , Alexandre Alahi , Li Fei-Fei"], "references": [2194775991, 2962835968, 2964121744, 1836465849, 2117539524, 1903029394, 2133665775, 1861492603, 2963684088, 2964153729]}, {"id": 2964015378, "title": "Semi-Supervised Classification with Graph Convolutional Networks", "abstract": "", "date": "2016", "authors": ["Thomas N. Kipf , Max Welling"], "references": [2962711740, 2907492528, 3100848837, 2963224980, 2962883549, 2963184176, 2796426482, 3100278010, 2786016794, 2966149470]}, {"id": 1514535095, "title": "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention", "abstract": "Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr9k, Flickr30k and MS COCO.", "date": "2015", "authors": ["Kelvin Xu 1, Jimmy Ba 2, Ryan Kiros 2, Kyunghyun Cho 1, Aaron Courville 1, Ruslan Salakhudinov 2, 3, Rich Zemel 2, 3, Yoshua Bengio 1, 3"], "references": [2618530766, 2962835968, 2964121744, 2097117768, 2117539524, 2964308564, 2095705004, 2130942839, 2157331557, 1861492603]}, {"id": 2508457857, "title": "Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising", "abstract": "The discriminative model learning for image denoising has been recently attracting considerable attentions due to its favorable denoising performance. In this paper, we take one step forward by investigating the construction of feed-forward denoising convolutional neural networks (DnCNNs) to embrace the progress in very deep architecture, learning algorithm, and regularization method into image denoising. Specifically, residual learning and batch normalization are utilized to speed up the training process as well as boost the denoising performance. Different from the existing discriminative denoising models which usually train a specific model for additive white Gaussian noise at a certain noise level, our DnCNN model is able to handle Gaussian denoising with unknown noise level (i.e., blind Gaussian denoising). With the residual learning strategy, DnCNN implicitly removes the latent clean image in the hidden layers. This property motivates us to train a single DnCNN model to tackle with several general image denoising tasks, such as Gaussian denoising, single image super-resolution, and JPEG image deblocking. Our extensive experiments demonstrate that our DnCNN model can not only exhibit high effectiveness in several general image denoising tasks, but also be efficiently implemented by benefiting from GPU computing.", "date": "2017", "authors": ["Kai Zhang 1, Wangmeng Zuo 1, Yunjin Chen 2, Deyu Meng 3, Lei Zhang 4"], "references": [2194775991, 2618530766, 2962835968, 2964121744, 2097117768, 1836465849, 1677182931, 2146502635, 2121058967, 2056370875]}, {"id": 1614298861, "title": "Efficient Estimation of Word Representations in Vector Space", "abstract": "We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.", "date": "2013", "authors": ["Tomas Mikolov 1, Kai Chen 2, Greg S. Corrado 2, Jeffrey Dean 2"], "references": [2153579005, 2250539671, 2271840356, 1895577753, 3104097132, 1888005072, 1486649854, 2964321699, 2100664567, 2123024445]}, {"id": 2168356304, "title": "Object Detection with Discriminatively Trained Part-Based Models", "abstract": "We describe an object detection system based on mixtures of multiscale deformable part models. Our system is able to represent highly variable object classes and achieves state-of-the-art results in the PASCAL object detection challenges. While deformable part models have become quite popular, their value had not been demonstrated on difficult benchmarks such as the PASCAL data sets. Our system relies on new methods for discriminative training with partially labeled data. We combine a margin-sensitive approach for data-mining hard negative examples with a formalism we call latent SVM. A latent SVM is a reformulation of MI--SVM in terms of latent variables. A latent SVM is semiconvex, and the training problem becomes convex once latent information is specified for the positive examples. This leads to an iterative training algorithm that alternates between fixing latent values for positive examples and optimizing the latent SVM objective function.", "date": "2010", "authors": ["P F Felzenszwalb 1, R B Girshick 1, D McAllester 2, D Ramanan 3"], "references": [2151103935, 2161969291, 3097096317, 2154422044, 2120419212, 3111950349, 3021469268, 2145072179, 2030536784, 2115763357]}, {"id": 1849277567, "title": "Visualizing and Understanding Convolutional Networks", "abstract": "Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.", "date": "2014", "authors": ["Matthew D. Zeiler , Rob Fergus"], "references": [2618530766, 2102605133, 2108598243, 2136922672, 1904365287, 2155541015, 2546302380, 2025768430, 2110798204, 2097018403]}, {"id": 2133665775, "title": "Image quality assessment: from error visibility to structural similarity", "abstract": "Objective methods for assessing perceptual image quality traditionally attempted to quantify the visibility of errors (differences) between a distorted image and a reference image using a variety of known properties of the human visual system. Under the assumption that human visual perception is highly adapted for extracting structural information from a scene, we introduce an alternative complementary framework for quality assessment based on the degradation of structural information. As a specific example of this concept, we develop a structural similarity index and demonstrate its promise through a set of intuitive examples, as well as comparison to both subjective ratings and state-of-the-art objective methods on a database of images compressed with JPEG and JPEG2000. A MATLAB implementation of the proposed algorithm is available online at http://www.cns.nyu.edu//spl sim/lcv/ssim/.", "date": "2004", "authors": ["Zhou Wang 1, A.C. Bovik 2, H.R. Sheikh 2, E.P. Simoncelli 3"], "references": [2159269332, 2142276208, 2053691921, 2118217749, 2153777140, 2912116903, 2107790757, 2158564760, 2124731682, 2115838129]}, {"id": 2340897893, "title": "The Cityscapes Dataset for Semantic Urban Scene Understanding", "abstract": "Visual understanding of complex urban street scenes is an enabling factor for a wide range of applications. Object detection has benefited enormously from large-scale datasets, especially in the context of deep learning. For semantic urban scene understanding, however, no current dataset adequately captures the complexity of real-world urban scenes. To address this, we introduce Cityscapes, a benchmark suite and large-scale dataset to train and test approaches for pixel-level and instance-level semantic labeling. Cityscapes is comprised of a large, diverse set of stereo video sequences recorded in streets from 50 different cities. 5000 of these images have high quality pixel-level annotations, 20 000 additional images have coarse annotations to enable methods that leverage large volumes of weakly-labeled data. Crucially, our effort exceeds previous attempts in terms of dataset size, annotation richness, scene variability, and complexity. Our accompanying empirical study provides an in-depth analysis of the dataset characteristics, as well as a performance evaluation of several state-of-the-art approaches based on our benchmark.", "date": "2016", "authors": ["Marius Cordts 1, Mohamed Omran 2, Sebastian Ramos 3, Timo Rehfeld 1, Markus Enzweiler 3, Rodrigo Benenson 2, Uwe Franke 3, Stefan Roth 1, Bernt Schiele 2"], "references": [2618530766, 2962835968, 639708223, 2919115771, 2102605133, 2117539524, 1903029394, 1536680647, 2168356304, 1861492603]}, {"id": 2963420272, "title": "Context Encoders: Feature Learning by Inpainting", "abstract": "We present an unsupervised visual feature learning algorithm driven by context-based pixel prediction. By analogy with auto-encoders, we propose Context Encoders \u2013 a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings. In order to succeed at this task, context encoders need to both understand the content of the entire image, as well as produce a plausible hypothesis for the missing part(s). When training context encoders, we have experimented with both a standard pixel-wise reconstruction loss, as well as a reconstruction plus an adversarial loss. The latter produces much sharper results because it can better handle multiple modes in the output. We found that a context encoder learns a representation that captures not just appearance but also the semantics of visual structures. We quantitatively demonstrate the effectiveness of our learned features for CNN pre-training on classification, detection, and segmentation tasks. Furthermore, context encoders can be used for semantic inpainting tasks, either stand-alone or as initialization for non-parametric methods.", "date": "2016", "authors": ["Deepak Pathak , Philipp Krahenbuhl , Jeff Donahue , Trevor Darrell , Alexei A. Efros"], "references": [2618530766, 2964121744, 2102605133, 2117539524, 2153579005, 1903029394, 2099471712, 2155893237, 1536680647, 1849277567]}, {"id": 2405756170, "title": "Generative adversarial text to image synthesis", "abstract": "Automatic synthesis of realistic images from text would be interesting and useful, but current AI systems are still far from this goal. However, in recent years generic and powerful recurrent neural network architectures have been developed to learn discriminative text feature representations. Meanwhile, deep convolutional generative adversarial networks (GANs) have begun to generate highly compelling images of specific categories, such as faces, album covers, and room interiors. In this work, we develop a novel deep architecture and GAN formulation to effectively bridge these advances in text and image modeling, translating visual concepts from characters to pixels. We demonstrate the capability of our model to generate plausible images of birds and flowers from detailed text descriptions.", "date": "2016", "authors": ["Scott Reed 1, Zeynep Akata 2, Xinchen Yan 1, Lajanugen Logeswaran 1, Bernt Schiele 2, Honglak Lee 1"], "references": [2964121744, 2097117768, 1836465849, 2099471712, 2963684088, 2064675550, 1895577753, 1514535095, 2481240925, 1947481528]}, {"id": 2963800363, "title": "High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs", "abstract": "We present a new method for synthesizing high-resolution photo-realistic images from semantic label maps using conditional generative adversarial networks (conditional GANs). Conditional GANs have enabled a variety of applications, but the results are often limited to low-resolution and still far from realistic. In this work, we generate 2048 A\u2014 1024 visually appealing results with a novel adversarial loss, as well as new multi-scale generator and discriminator architectures. Furthermore, we extend our framework to interactive visual manipulation with two additional features. First, we incorporate object instance segmentation information, which enables object manipulations such as removing/adding objects and changing the object category. Second, we propose a method to generate diverse results given the same input, allowing users to edit the object appearance interactively. Human opinion studies demonstrate that our method significantly outperforms existing methods, advancing both the quality and the resolution of deep image synthesis and editing.", "date": "2018", "authors": ["Ting-Chun Wang 1, Ming-Yu Liu 1, Jun-Yan Zhu 2, Andrew Tao 1, Jan Kautz 1, Bryan Catanzaro 1"], "references": [2194775991, 2962835968, 1901129140, 1903029394, 1959608418, 2962793481, 2963684088, 2340897893, 2963373786, 2331128040]}, {"id": 2963836885, "title": "Spectral Normalization for Generative Adversarial Networks", "abstract": "", "date": "2018", "authors": ["Takeru Miyato 1, Toshiki Kataoka , Masanori Koyama 2, Yuichi Yoshida 3"], "references": [3003301247, 2893749619, 2962974533, 2804078698, 3035574324, 2982763192, 2962754210, 2963841322, 2933374552]}, {"id": 2738588019, "title": "Globally and locally consistent image completion", "abstract": "We present a novel approach for image completion that results in images that are both locally and globally consistent. With a fully-convolutional neural network, we can complete images of arbitrary resolutions by filling-in missing regions of any shape. To train this image completion network to be consistent, we use global and local context discriminators that are trained to distinguish real images from completed ones. The global discriminator looks at the entire image to assess if it is coherent as a whole, while the local discriminator looks only at a small area centered at the completed region to ensure the local consistency of the generated patches. The image completion network is then trained to fool the both context discriminator networks, which requires it to generate images that are indistinguishable from real ones with regard to overall consistency as well as in details. We show that our approach can be used to complete a wide variety of scenes. Furthermore, in contrast with the patch-based approaches such as PatchMatch, our approach can generate fragments that do not appear elsewhere in the image, which allows us to naturally complete the images of objects with familiar and highly specific structures, such as faces.", "date": "2017", "authors": ["Satoshi Iizuka , Edgar Simo-Serra , Hiroshi Ishikawa"], "references": [1836465849, 1903029394, 2099471712, 2108598243, 2963073614, 2963684088, 1665214252, 2963373786, 2963840672, 6908809]}, {"id": 2962947361, "title": "Unsupervised Image-to-Image Translation Networks", "abstract": "Unsupervised image-to-image translation aims at learning a joint distribution of images in different domains by using images from the marginal distributions in individual domains. Since there exists an infinite set of joint distributions that can arrive the given marginal distributions, one could infer nothing about the joint distribution from the marginal distributions without additional assumptions. To address the problem, we make a shared-latent space assumption and propose an unsupervised image-to-image translation framework based on Coupled GANs. We compare the proposed framework with competing approaches and present high quality image translation results on various challenging unsupervised image translation tasks, including street scene image translation, animal image translation, and face image translation. We also apply the proposed framework to domain adaptation and achieve state-of-the-art performance on benchmark datasets. Code and additional results are available in https://github.com/mingyuliutw/unit.", "date": "2017", "authors": ["Ming-Yu Liu 1, Thomas Breuel 2, Jan Kautz 3"], "references": [2962793481, 2795155917, 2984529706, 2963626105, 3098418424, 2989855043, 2970902013, 2981988113]}, {"id": 2964153729, "title": "Intriguing properties of neural networks", "abstract": "Abstract: Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.", "date": "2013", "authors": ["Christian Szegedy 1, Wojciech Zaremba 2, Ilya Sutskever 1, Joan Bruna 2, Dumitru Erhan 1, Ian Goodfellow 3, Rob Fergus 2, 4"], "references": [2618530766, 2102605133, 1614298861, 2108598243, 2160815625, 2072128103, 2120419212, 2206858481, 2120480077, 2150165932]}, {"id": 2271840356, "title": "TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems", "abstract": "TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.", "date": "2014", "authors": ["Mart\u00edn Abadi , Ashish Agarwal , Paul Barham , Eugene Brevdo , Zhifeng Chen , Craig Citro , Gregory S. Corrado , Andy Davis , Jeffrey Dean , Matthieu Devin , Sanjay Ghemawat , Ian J. Goodfellow , Andrew Harp , Geoffrey Irving , Michael Isard , Yangqing Jia , Rafal J\u00f3zefowicz , Lukasz Kaiser , Manjunath Kudlur , Josh Levenberg , Dan Man\u00e9 , Rajat Monga , Sherry Moore , Derek Gordon Murray , Chris Olah , Mike Schuster , Jonathon Shlens , Benoit Steiner , Ilya Sutskever , Kunal Talwar , Paul A. Tucker , Vincent Vanhoucke , Vijay Vasudevan , Fernanda B. Vi\u00e9gas , Oriol Vinyals , Pete Warden , Martin Wattenberg , Martin Wicke , Yuan Yu , Xiaoqiang Zheng"], "references": [2097117768, 1836465849, 1614298861, 2130942839, 2155893237, 2064675550, 2160815625, 2168231600, 2016053056, 2131975293]}, {"id": 648143168, "title": "Deep generative image models using a Laplacian pyramid of adversarial networks", "abstract": "In this paper we introduce a generative parametric model capable of producing high quality samples of natural images. Our approach uses a cascade of convolutional networks within a Laplacian pyramid framework to generate images in a coarse-to-fine fashion. At each level of the pyramid, a separate generative convnet model is trained using the Generative Adversarial Nets (GAN) approach [11]. Samples drawn from our model are of significantly higher quality than alternate approaches. In a quantitative assessment by human evaluators, our CIFAR10 samples were mistaken for real images around 40% of the time, compared to 10% for samples drawn from a GAN baseline model. We also show samples from models trained on the higher resolution images of the LSUN scene dataset.", "date": "2015", "authors": ["Emily Denton 1, Soumith Chintala 2, Arthur Szlam 2, Rob Fergus 2"], "references": [1836465849, 2099471712, 2108598243, 1959608418, 3118608800, 2100495367, 2025768430, 2125389028, 2118858186, 189596042]}, {"id": 830076066, "title": "Semi-supervised learning with Ladder networks", "abstract": "We combine supervised learning with unsupervised learning in deep neural networks. The proposed model is trained to simultaneously minimize the sum of supervised and unsupervised cost functions by backpropagation, avoiding the need for layer-wise pre-training. Our work builds on top of the Ladder network proposed by Valpola [1] which we extend by combining the model with supervision. We show that the resulting model reaches state-of-the-art performance in semi-supervised MNIST and CIFAR-10 classification in addition to permutation-invariant MNIST classification with all labels.", "date": "2015", "authors": ["Antti Rasmus 1, Harri Valpola 1, Mikko Honkala 2, Mathias Berglund 3, Tapani Raiko 3"], "references": [2964121744, 1836465849, 2095705004, 2100495367, 2963207607, 2294059674, 2145094598, 2963382180, 1606347560, 2048679005]}, {"id": 2963685250, "title": "Weight normalization: a simple reparameterization to accelerate training of deep neural networks", "abstract": "We present weight normalization: a reparameterization of the weight vectors in a neural network that decouples the length of those weight vectors from their direction. By reparameterizing the weights in this way we improve the conditioning of the optimization problem and we speed up convergence of stochastic gradient descent. Our reparameterization is inspired by batch normalization but does not introduce any dependencies between the examples in a minibatch. This means that our method can also be applied successfully to recurrent models such as LSTMs and to noise-sensitive applications such as deep reinforcement learning or generative models, for which batch normalization is less well suited. Although our method is much simpler, it still provides much of the speed-up of full batch normalization. In addition, the computational overhead of our method is lower, permitting more optimization steps to be taken in the same amount of time. We demonstrate the usefulness of our method on applications in supervised image recognition, generative modelling, and deep reinforcement learning.", "date": "2016", "authors": ["Tim Salimans , Diederik P. Kingma"], "references": [2194775991, 1836465849, 2145339207, 1959608418, 3118608800, 2064675550, 2963911037, 1533861849, 2294059674, 104184427]}, {"id": 2949416428, "title": "Semi-Supervised Learning with Deep Generative Models", "abstract": "The ever-increasing size of modern data sets combined with the difficulty of obtaining label information has made semi-supervised learning one of the problems of significant practical importance in modern data analysis. We revisit the approach to semi-supervised learning with generative models and develop new models that allow for effective generalisation from small labelled data sets to large unlabelled ones. Generative approaches have thus far been either inflexible, inefficient or non-scalable. We show that deep generative models and approximate Bayesian inference exploiting recent advances in variational methods can be used to provide significant improvements, making generative approaches highly competitive for semi-supervised learning.", "date": "2014", "authors": ["Diederik P. Kingma , Danilo J. Rezende , Shakir Mohamed , Max Welling"], "references": [1959608418, 2146502635, 2962897886, 2335728318, 2136504847, 2107008379, 1676820704, 2407712691, 2158049734, 2122457239]}, {"id": 1487641199, "title": "Generative Moment Matching Networks", "abstract": "We consider the problem of learning deep generative models from data. We formulate a method that generates an independent sample via a single feedforward pass through a multilayer perceptron, as in the recently proposed generative adversarial networks (Goodfellow et al., 2014). Training a generative adversarial network, however, requires careful optimization of a difficult minimax program. Instead, we utilize a technique from statistical hypothesis testing known as maximum mean discrepancy (MMD), which leads to a simple objective that can be interpreted as matching all orders of statistics between a dataset and samples from the model, and can be trained by backpropagation. We further boost the performance of this approach by combining our generative network with an auto-encoder network, using MMD to learn to generate codes that can then be decoded to produce samples. We show that the combination of these techniques yields excellent generative models compared to baseline approaches as measured on MNIST and the Toronto Face Database.", "date": "2015", "authors": ["Yujia Li 1, Kevin Swersky 1, Rich Zemel 1, 2"], "references": [2618530766, 2097117768, 2099471712, 2130942839, 2157331557, 2963542991, 1959608418, 2310919327, 1904365287, 1665214252]}, {"id": 2911964244, "title": "Random Forests", "abstract": "Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund & R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, aaa, 148\u2013156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.", "date": "2001", "authors": ["Leo Breiman"], "references": [2912934387, 2112076978, 1975846642, 2152761983, 2113242816, 1605688901, 2120240539, 2099968818, 2067885219, 1580948147]}, {"id": 1904365287, "title": "Improving neural networks by preventing co-adaptation of feature detectors", "abstract": "When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This \"overfitting\" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random \"dropout\" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.", "date": "2012", "authors": ["Geoffrey E. Hinton , Nitish Srivastava , Alex Krizhevsky , Ilya Sutskever , Ruslan R. Salakhutdinov"], "references": [2108598243, 2911964244, 3118608800, 2100495367, 2310919327, 2912934387, 2116064496, 2147768505, 1993882792, 4919037]}, {"id": 2546302380, "title": "What is the best multi-stage architecture for object recognition?", "abstract": "In many recent object recognition systems, feature extraction stages are generally composed of a filter bank, a non-linear transformation, and some sort of feature pooling layer. Most systems use only one stage of feature extraction in which the filters are hard-wired, or two stages where the filters in one or both stages are learned in supervised or unsupervised mode. This paper addresses three questions: 1. How does the non-linearities that follow the filter banks influence the recognition accuracy? 2. does learning the filter banks in an unsupervised or supervised manner improve the performance over random filters or hardwired filters? 3. Is there any advantage to using an architecture with two stages of feature extraction, rather than one? We show that using non-linearities that include rectification and local contrast normalization is the single most important ingredient for good accuracy on object recognition benchmarks. We show that two stages of feature extraction yield better accuracy than one. Most surprisingly, we show that a two-stage system with random filters can yield almost 63% recognition rate on Caltech-101, provided that the proper non-linearities and pooling layers are used. Finally, we show that with supervised refinement, the system achieves state-of-the-art performance on NORB dataset (5.6%) and unsupervised pre-training followed by supervised refinement produces good accuracy on Caltech-101 (\u226b 65%), and the lowest known error rate on the undistorted, unprocessed MNIST dataset (0.53%).", "date": "2009", "authors": ["Kevin Jarrett , Koray Kavukcuoglu , Marc'Aurelio Ranzato , Yann LeCun"], "references": [2151103935, 2161969291, 2100495367, 2310919327, 2162915993, 2110798204, 2130325614, 2097018403, 2166049352, 2134557905]}, {"id": 2130325614, "title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations", "abstract": "There has been much interest in unsupervised learning of hierarchical generative models such as deep belief networks. Scaling such models to full-sized, high-dimensional images remains a difficult problem. To address this problem, we present the convolutional deep belief network, a hierarchical generative model which scales to realistic image sizes. This model is translation-invariant and supports efficient bottom-up and top-down probabilistic inference. Key to our approach is probabilistic max-pooling, a novel technique which shrinks the representations of higher layers in a probabilistically sound way. Our experiments show that the algorithm learns useful high-level visual features, such as object parts, from unlabeled images of objects and natural scenes. We demonstrate excellent performance on several visual recognition tasks and show that our model can perform hierarchical (bottom-up and top-down) inference over full-sized images.", "date": "2009", "authors": ["Honglak Lee , Roger Grosse , Rajesh Ranganath , Andrew Y. Ng"], "references": [2136922672, 2100495367, 2162915993, 2116064496, 2110798204, 2166049352, 2145889472, 2122922389, 2147800946, 2168002178]}, {"id": 2963542991, "title": "OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks", "abstract": "Abstract: We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.", "date": "2014", "authors": ["Pierre Sermanet , David Eigen , Xiang Zhang , Michael Mathieu , Rob Fergus , Yann LeCun"], "references": [2194775991, 2962835968, 2097117768, 639708223, 1903029394, 2155893237]}, {"id": 2963911037, "title": "Network In Network", "abstract": "Abstract: We propose a novel deep network structure called \"Network In Network\" (NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.", "date": "2014", "authors": ["Min Lin , Qiang Chen , Shuicheng Yan"], "references": [2962835968, 2097117768, 2117539524, 1536680647, 2963446712, 2963037989, 2109255472, 2096733369, 2302255633]}, {"id": 2068730032, "title": "Scalable Object Detection Using Deep Neural Networks", "abstract": "Deep convolutional neural networks have recently achieved state-of-the-art performance on a number of image recognition benchmarks, including the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC-2012). The winning model on the localization sub-task was a network that predicts a single bounding box and a confidence score for each object category in the image. Such a model captures the whole-image context around the objects but cannot handle multiple instances of the same object in the image without naively replicating the number of outputs for each instance. In this work, we propose a saliency-inspired neural network model for detection, which predicts a set of class-agnostic bounding boxes along with a single score for each box, corresponding to its likelihood of containing any object of interest. The model naturally handles a variable number of instances for each class and allows for cross-class generalization at the highest levels of the network. We are able to obtain competitive recognition performance on VOC2007 and ILSVRC2012, while using only the top few predicted locations in each image and a small number of neural network evaluations.", "date": "2014", "authors": ["Dumitru Erhan , Christian Szegedy , Alexander Toshev , Dragomir Anguelov"], "references": [2618530766, 2102605133, 2168356304, 2031489346, 2088049833, 2130306094, 2129305389, 2017691720, 2128715914, 2113201641]}, {"id": 2161969291, "title": "Histograms of oriented gradients for human detection", "abstract": "We study the question of feature sets for robust visual object recognition; adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds.", "date": "2005", "authors": ["N. Dalal , B. Triggs"], "references": [2151103935, 2177274842, 3021469268, 2145072179, 2172188317, 2152473410, 1576520375, 1608462934, 1992825118, 2295106276]}, {"id": 2031489346, "title": "The Pascal Visual Object Classes (VOC) Challenge", "abstract": "The Pascal Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection. This paper describes the dataset and evaluation procedure. We review the state-of-the-art in evaluated methods for both classification and detection, analyse whether the methods are statistically different, what they are learning from the images (e.g. the object or its context), and what the methods find easy or confuse. The paper concludes with lessons learnt in the three year history of the challenge, and proposes directions for future improvement and extension.", "date": "2010", "authors": ["Mark Everingham 1, Luc Gool 2, Christopher K. Williams 3, John Winn 4, Andrew Zisserman 5"], "references": [2151103935, 2161969291, 3097096317, 2162915993, 2038721957, 2131846894, 2104974755, 2110764733, 1576445103, 1565746575]}, {"id": 2088049833, "title": "Selective Search for Object Recognition", "abstract": "This paper addresses the problem of generating possible object locations for use in object recognition. We introduce selective search which combines the strength of both an exhaustive search and segmentation. Like segmentation, we use the image structure to guide our sampling process. Like exhaustive search, we aim to capture all possible object locations. Instead of a single technique to generate possible object locations, we diversify our search and use a variety of complementary image partitionings to deal with as many image conditions as possible. Our selective search results in a small set of data-driven, class-independent, high quality locations, yielding 99 % recall and a Mean Average Best Overlap of 0.879 at 10,097 locations. The reduced number of locations compared to an exhaustive search enables the use of stronger machine learning techniques and stronger appearance models for object recognition. In this paper we show that our selective search enables the use of the powerful Bag-of-Words model for recognition. The selective search software is made publicly available (Software: http://disi.unitn.it/~uijlings/SelectiveSearch.html ).", "date": "2013", "authors": ["J. R. Uijlings 1, K. E. Sande 2, T. Gevers 2, A. W. Smeulders 2"], "references": [2151103935, 2161969291, 2168356304, 2164598857, 2031489346, 3097096317, 2162915993, 2163352848, 2067191022, 2121947440]}, {"id": 2155541015, "title": "DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition", "abstract": "We evaluate whether features extracted from the activation of a deep convolutional network trained in a fully supervised fashion on a large, fixed set of object recognition tasks can be repurposed to novel generic tasks. Our generic tasks may differ significantly from the originally trained tasks and there may be insufficient labeled or unlabeled data to conventionally train or adapt a deep architecture to the new tasks. We investigate and visualize the semantic clustering of deep convolutional features with respect to a variety of such tasks, including scene recognition, domain adaptation, and fine-grained recognition challenges. We compare the efficacy of relying on various network levels to define a fixed feature, and report novel results that significantly outperform the state-of-the-art on several important vision challenges. We are releasing DeCAF, an open-source implementation of these deep convolutional activation features, along with all associated network parameters to enable vision researchers to be able to conduct experimentation with deep representations across a range of visual concept learning paradigms.", "date": "2014", "authors": ["Jeff Donahue , Yangqing Jia , Oriol Vinyals , Judy Hoffman , Ning Zhang , Eric Tzeng , Trevor Darrell"], "references": [2618530766, 2161969291, 2108598243, 2168356304, 2100495367, 2310919327, 1677409904, 1904365287, 2187089797, 2546302380]}, {"id": 1663973292, "title": "Pattern Recognition and Machine Learning", "abstract": "Probability Distributions.- Linear Models for Regression.- Linear Models for Classification.- Neural Networks.- Kernel Methods.- Sparse Kernel Machines.- Graphical Models.- Mixture Models and EM.- Approximate Inference.- Sampling Methods.- Continuous Latent Variables.- Sequential Data.- Combining Models.", "date": "2006", "authors": ["Christopher M. Bishop"], "references": [2117812871, 1496357020]}, {"id": 2109255472, "title": "Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition", "abstract": "Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g., 224 $\\times$ 224) input image. This requirement is \u201cartificial\u201d and may reduce the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with another pooling strategy, \u201cspatial pyramid pooling\u201d, to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. Pyramid pooling is also robust to object deformations. With these advantages, SPP-net should in general improve all CNN-based image classification methods. On the ImageNet 2012 dataset, we demonstrate that SPP-net boosts the accuracy of a variety of CNN architectures despite their different designs. On the Pascal VOC 2007 and Caltech101 datasets, SPP-net achieves state-of-the-art classification results using a single full-image representation and no fine-tuning. The power of SPP-net is also significant in object detection. Using SPP-net, we compute the feature maps from the entire image only once, and then pool features in arbitrary regions (sub-images) to generate fixed-length representations for training the detectors. This method avoids repeatedly computing the convolutional features. In processing test images, our method is 24-102 $\\times$ faster than the R-CNN method, while achieving better or comparable accuracy on Pascal VOC 2007. In ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2014, our methods rank #2 in object detection and #3 in image classification among all 38 teams. This manuscript also introduces the improvement made for this competition.", "date": "2015", "authors": ["Kaiming He 1, Xiangyu Zhang 2, Shaoqing Ren 3, Jian Sun 1"], "references": [2618530766, 2962835968, 2151103935, 2097117768, 2153635508, 2102605133, 2117539524, 2161969291, 2108598243, 2168356304]}, {"id": 753012316, "title": "Torch7: A Matlab-like Environment for Machine Learning", "abstract": "Torch7 is a versatile numeric computing framework and machine learning library that extends Lua. Its goal is to provide a flexible environment to design and train learning machines. Flexibility is obtained via Lua, an extremely lightweight scripting language. High performance is obtained via efficient OpenMP/SSE and CUDA implementations of low-level numeric routines. Torch7 can easily be interfaced to third-party software thanks to Lua\u2019s light interface.", "date": "2010", "authors": ["Ronan Collobert 1, Koray Kavukcuoglu 2, Cl\u00e9ment Farabet 2"], "references": [2606594511]}, {"id": 1825604117, "title": "Open-vocabulary Object Retrieval", "abstract": "", "date": "2014", "authors": ["Sergio Guadarrama 1, Erik Rodner 2, Kate Saenko 3, Ning Zhang 1, Ryan Farrell 1, Jeff Donahue 1, Trevor Darrell 1"], "references": [2088049833, 2131846894, 2128017662, 2141362318, 2094728533, 1889268436, 1618905105, 21006490, 1897761818, 2066134726]}, {"id": 2147414309, "title": "PANDA: Pose Aligned Networks for Deep Attribute Modeling", "abstract": "We propose a method for inferring human attributes (such as gender, hair style, clothes style, expression, action) from images of people under large variation of viewpoint, pose, appearance, articulation and occlusion. Convolutional Neural Nets (CNN) have been shown to perform very well on large scale object recognition problems. In the context of attribute classification, however, the signal is often subtle and it may cover only a small part of the image, while the image is dominated by the effects of pose and viewpoint. Discounting for pose variation would require training on very large labeled datasets which are not presently available. Part-based models, such as poselets [4] and DPM [12] have been shown to perform well for this problem but they are limited by shallow low-level features. We propose a new method which combines part-based models and deep learning by training pose-normalized CNNs. We show substantial improvement vs. state-of-the-art methods on challenging attribute classification tasks in unconstrained settings. Experiments confirm that our method outperforms both the best part-based methods on this problem and conventional CNNs trained on the full bounding box of the person.", "date": "2014", "authors": ["Ning Zhang 1, Manohar Paluri 2, Marc'Aurelio Ranzato 2, Trevor Darrell 1, Lubomir Bourdev 2"], "references": [2618530766, 2168356304, 2310919327, 2155541015, 2162915993, 2546302380, 1498436455, 2536626143, 2134270519, 2098411764]}, {"id": 1872489089, "title": "Pylearn2: a machine learning research library", "abstract": "Pylearn2 is a machine learning research library. This does not just mean that it is a collection of machine learning algorithms that share a common API; it means that it has been designed for flexibility and extensibility in order to facilitate research projects that involve new or unusual use cases. In this paper we give a brief history of the library, an overview of its basic philosophy, a summary of the library's architecture, and a description of how the Pylearn2 community functions socially.", "date": "2013", "authors": ["Ian J. Goodfellow , David Warde-Farley , Pascal Lamblin , Vincent Dumoulin , Mehdi Mirza , Razvan Pascanu , James Bergstra , Fr\u00e9d\u00e9ric Bastien , Yoshua Bengio"], "references": [2618530766, 2101234009, 3118608800, 2310919327, 1904365287, 2168231600, 2119821739, 2116064496, 2294059674, 2025768430]}, {"id": 2962883796, "title": "Recognizing Image Style.", "abstract": "The style of an image plays a significant role in how it is viewed, but style has received little attention in computer vision research. We describe an approach to predicting style of images, and perform a thorough evaluation of different image features for these tasks. We find that features learned in a multi-layer network generally perform best \u2013 even when trained with object class (not style) labels. Our large-scale learning methods results in the best published performance on an existing dataset of aesthetic ratings and photographic style annotations. We present two novel datasets: 80K Flickr photographs annotated with 20 curated style labels, and 85K paintings annotated with 25 style/genre labels. Our approach shows excellent classification performance on both datasets. We use the learned classifiers to extend traditional tag-based image search to consider stylistic constraints, and demonstrate cross-dataset understanding of style.", "date": "2013", "authors": ["Sergey Karayev 1, Matthew Trentacoste 2, Helen Han , Aseem Agarwala 3, Trevor Darrell 1, Aaron Hertzmann 3, Holger Winnemoeller 3"], "references": [2618530766, 2108598243, 2146502635, 2155541015, 1566135517, 2135957164, 1511924373, 2075456404, 2078807908, 2157462866]}, {"id": 2164598857, "title": "Rapid object detection using a boosted cascade of simple features", "abstract": "This paper describes a machine learning approach for visual object detection which is capable of processing images extremely rapidly and achieving high detection rates. This work is distinguished by three key contributions. The first is the introduction of a new image representation called the \"integral image\" which allows the features used by our detector to be computed very quickly. The second is a learning algorithm, based on AdaBoost, which selects a small number of critical visual features from a larger set and yields extremely efficient classifiers. The third contribution is a method for combining increasingly more complex classifiers in a \"cascade\" which allows background regions of the image to be quickly discarded while spending more computation on promising object-like regions. The cascade can be viewed as an object specific focus-of-attention mechanism which unlike previous approaches provides statistical guarantees that discarded regions are unlikely to contain the object of interest. In the domain of face detection the system yields detection rates comparable to the best previous systems. Used in real-time applications, the detector runs at 15 frames per second without resorting to image differencing or skin color detection.", "date": "2001", "authors": ["P. Viola 1, M. Jones 2"], "references": [1988790447, 2128272608, 2217896605, 2115763357, 1975846642, 2124351082, 2159686933, 2155511848, 2101522199, 1588351438]}, {"id": 1861492603, "title": "Microsoft COCO: Common Objects in Context", "abstract": "We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.", "date": "2014", "authors": ["Tsung-Yi Lin 1, Michael Maire 2, Serge J. Belongie 1, James Hays 3, Pietro Perona 2, Deva Ramanan 4, Piotr Doll\u00e1r 5, C. Lawrence Zitnick 5"], "references": [2618530766, 2102605133, 2161969291, 2108598243, 2168356304, 2963542991, 3118608800, 2031489346, 2038721957, 2110158442]}, {"id": 2250539671, "title": "Glove: Global Vectors for Word Representation", "abstract": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.", "date": "2014", "authors": ["Jeffrey Pennington 1, Richard Socher 2, Christopher Manning 1"], "references": [2153579005, 1614298861, 2146502635, 2158899491, 2072128103, 2141599568, 2117130368, 2118020653, 2132339004, 2158139315]}, {"id": 2131744502, "title": "Distributed Representations of Sentences and Documents", "abstract": "Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, \"powerful,\" \"strong\" and \"Paris\" are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperforms bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.", "date": "2014", "authors": ["Quoc Le , Tomas Mikolov"], "references": [2153579005, 1614298861, 2158899491, 2131744502, 2251939518, 2141599568, 2117130368, 2132339004, 2158139315, 2113459411]}, {"id": 2251939518, "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank", "abstract": "Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.", "date": "2013", "authors": ["Richard Socher 1, Alex Perelygin , Jean Wu 1, Jason Chuang 2, Christopher D. Manning 1, Andrew Ng 1, Christopher Potts 1"], "references": [2146502635, 2097726431, 2117130368, 2132339004, 1423339008, 71795751, 1662133657, 1889268436, 2164019165, 2097606805]}, {"id": 2963748441, "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text", "abstract": "", "date": "2016", "authors": ["Pranav Rajpurkar , Jian Zhang , Konstantin Lopyrev , Percy Liang"], "references": [2108598243, 1544827683, 1632114991, 2125436846, 2964267515, 2962809918, 2171278097, 2962790689, 2251818205, 2251349042]}, {"id": 2899771611, "title": "Automatic differentiation in PyTorch", "abstract": "", "date": "2017", "authors": ["Adam Paszke , Sam Gross , Soumith Chintala , Gregory Chanan , Edward Yang , Zachary DeVito , Zeming Lin , Alban Desmaison , Luca Antiga , Adam Lerer"], "references": [2754835109, 1585773866, 1579027943]}, {"id": 2962784628, "title": "Neural Machine Translation of Rare Words with Subword Units", "abstract": "Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character ngram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English!German and English!Russian by up to 1.1 and 1.3 BLEU, respectively.", "date": "2016", "authors": ["Rico Sennrich , Barry Haddow , Alexandra Birch"], "references": [2964308564, 2130942839, 2157331557, 1902237438, 6908809, 1753482797, 2124807415, 2251012068, 1815076433, 2100664567]}, {"id": 1840435438, "title": "A large annotated corpus for learning natural language inference", "abstract": "Understanding entailment and contradiction is fundamental to understanding natural language, and inference about entailment and contradiction is a valuable testing ground for the development of semantic representations. However, machine learning research in this area has been dramatically limited by the lack of large-scale resources. To address this, we introduce the Stanford Natural Language Inference corpus, a new, freely available collection of labeled sentence pairs, written by humans doing a novel grounded task based on image captioning. At 570K pairs, it is two orders of magnitude larger than all other resources of its type. This increase in scale allows lexicalized classifiers to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the first time.", "date": "2015", "authors": ["Samuel R. Bowman , Gabor Angeli , Christopher Potts , Christopher D. Manning"], "references": [2095705004, 2250539671, 2064675550, 2251939518, 6908809, 2081580037, 2097606805, 2584341106, 2185175083, 2154359981]}, {"id": 2963026768, "title": "Universal Language Model Fine-tuning for Text Classification", "abstract": "Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100 times more data. We open-source our pretrained models and code.", "date": "2018", "authors": ["Jeremy Howard , Sebastian Ruder"], "references": [2194775991, 1836465849, 2153579005, 1903029394, 2963446712, 2962739339, 2155541015, 3112605745, 2062118960, 2963012544]}, {"id": 2996035354, "title": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators", "abstract": "While masked language modeling (MLM) pre-training methods such as BERT produce excellent results on downstream NLP tasks, they require large amounts of compute to be effective. These approaches corrupt the input by replacing some tokens with [MASK] and then train a model to reconstruct the original tokens. As an alternative, we propose a more sample-efficient pre-training task called replaced token detection. Instead of masking the input, our approach corrupts it by replacing some input tokens with plausible alternatives sampled from a small generator network. Then, instead of training a model that predicts the original identities of the corrupted tokens, we train a discriminative model that predicts whether each token in the corrupted input was replaced by a generator sample or not. Thorough experiments demonstrate this new pre-training task is more efficient than MLM because the model learns from all input tokens rather than just the small subset that was masked out. As a result, the contextual representations learned by our approach substantially outperform the ones learned by methods such as BERT and XLNet given the same model size, data, and compute. The gains are particularly strong for small models; for example, we train a model on one GPU for 4 days that outperforms GPT (trained using 30x more compute) on the GLUE natural language understanding benchmark. Our approach also works well at scale, where we match the performance of RoBERTa, the current state-of-the-art pre-trained transformer, while using less than 1/4 of the compute.", "date": "2020", "authors": ["Kevin Clark 1, Minh-Thang Luong 2, Quoc V. Le 2, Christopher D. Manning 1"], "references": [2964121744, 2963403868, 2963341956, 2099471712, 1614298861, 2250539671, 2962739339, 2963684088, 2158899491, 2251939518]}, {"id": 2990704537, "title": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems", "abstract": "In the last year, new models and methods for pretraining and transfer learning have driven striking performance improvements across a range of language understanding tasks. The GLUE benchmark, introduced a little over one year ago, offers a single-number metric that summarizes progress on a diverse set of such tasks, but performance on the benchmark has recently surpassed the level of non-expert humans, suggesting limited headroom for further research. In this paper we present SuperGLUE, a new benchmark styled after GLUE with a new set of more difficult language understanding tasks, a software toolkit, and a public leaderboard. SuperGLUE is available at https://super.gluebenchmark.com.", "date": "2019", "authors": ["Alex Wang 1, Yada Pruksachatkun 1, Nikita Nangia 1, Amanpreet Singh 2, Julian Michael 3, Felix Hill 4, Omer Levy 2, Samuel R. Bowman 1"], "references": [2965373594, 3098903812, 2980282514, 3100307207, 3034238904, 2970986510, 3007332492, 3035503910]}, {"id": 3100307207, "title": "Experience Grounds Language", "abstract": "Language understanding research is held back by a failure to relate language to the physical world it describes and to the social interactions it facilitates. Despite the incredible effectiveness of language processing models to tackle tasks after being trained on text alone, successful linguistic communication relies on a shared experience of the world. It is this shared experience that makes utterances meaningful. Natural language processing is a diverse field, and progress throughout its development has come from new representational theories, modeling techniques, data collection paradigms, and tasks. We posit that the present success of representation learning approaches trained on large, text-only corpora requires the parallel tradition of research on the broader physical and social context of language to address the deeper questions of communication.", "date": "2020", "authors": ["Yonatan Bisk 1, Ari Holtzman 2, Jesse Thomason 2, Jacob Andreas 3, Yoshua Bengio 4, Joyce Chai 5, Mirella Lapata 6, Angeliki Lazaridou 7, Jonathan May 8, Aleksandr Nisnevich 9, Nicolas Pinto 3, Joseph P. Turian 4"], "references": [2618530766, 2963403868, 2963341956, 2117539524, 2153579005, 2250539671, 1880262756, 2962739339, 2117130368, 2132339004]}, {"id": 3105966348, "title": "TinyBERT: Distilling BERT for Natural Language Understanding", "abstract": "Language model pre-training, such as BERT, has significantly improved the performances of many natural language processing tasks. However, pre-trained language models are usually computationally expensive, so it is difficult to efficiently execute them on resource-restricted devices. To accelerate inference and reduce model size while maintaining accuracy, we first propose a novel Transformer distillation method that is specially designed for knowledge distillation (KD) of the Transformer-based models. By leveraging this new KD method, the plenty of knowledge encoded in a large \u201cteacher\u201d BERT can be effectively transferred to a small \u201cstudent\u201d TinyBERT. Then, we introduce a new two-stage learning framework for TinyBERT, which performs Transformer distillation at both the pre-training and task-specific learning stages. This framework ensures that TinyBERT can capture the general-domain as well as the task-specific knowledge in BERT. TinyBERT4 with 4 layers is empirically effective and achieves more than 96.8% the performance of its teacher BERT-Base on GLUE benchmark, while being 7.5x smaller and 9.4x faster on inference. TinyBERT4 is also significantly better than 4-layer state-of-the-art baselines on BERT distillation, with only ~28% parameters and ~31% inference time of them. Moreover, TinyBERT6 with 6 layers performs on-par with its teacher BERT-Base.", "date": "2020", "authors": ["Xiaoqi Jiao 1, Yichun Yin 2, Lifeng Shang 2, Xin Jiang 2, Xiao Chen 2, Linlin Li 2, Fang Wang 2, Qun Liu 2"], "references": [3101045333, 3100985894, 3022810465, 3117450517, 3015609966, 3103473439, 3115789797, 3110662498, 3106429660]}, {"id": 3034238904, "title": "Don't Stop Pretraining: Adapt Language Models to Domains and Tasks", "abstract": "Language models pretrained on text from a wide variety of sources form the foundation of today\u2019s NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining in-domain (domain-adaptive pretraining) leads to performance gains, under both high- and low-resource settings. Moreover, adapting to the task\u2019s unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multi-phase adaptive pretraining offers large gains in task performance.", "date": "2020", "authors": ["Suchin Gururangan , Ana Marasovi\u0107 , Swabha Swayamdipta , Kyle Lo , Iz Beltagy , Doug Downey , Noah A. Smith"], "references": [2964121744, 2963403868, 2963341956, 2965373594, 2970597249, 2525778437, 2963012544, 2963026768, 2113459411, 2923014074]}, {"id": 3099342932, "title": "WNUT-2020 Task 2: Identification of Informative COVID-19 English Tweets", "abstract": "In this paper, we provide an overview of the WNUT-2020 shared task on the identification of informative COVID-19 English Tweets. We describe how we construct a corpus of 10K Tweets and organize the development and evaluation phases for this task. In addition, we also present a brief summary of results obtained from the final system evaluation submissions of 55 teams, finding that (i) many systems obtain very high performance, up to 0.91 F1 score, (ii) the majority of the submissions achieve substantially higher results than the baseline fastText (Joulin et al., 2017), and (iii) fine-tuning pre-trained language models on relevant language data followed by supervised training performs well in this task.", "date": "2020", "authors": ["Dat Quoc Nguyen 1, Thanh Vu 2, Afshin Rahimi 3, Mai Hoang Dao , Long Doan"], "references": [2963341956, 2965373594, 2970597249, 2963626623, 2164777277, 3024622987, 1975879668, 2125980212, 3104186312, 3105987139]}, {"id": 2995015695, "title": "Cross-Lingual Ability of Multilingual BERT: An Empirical Study", "abstract": "Recent work has exhibited the surprising cross-lingual abilities of multilingual BERT (M-BERT) -- surprising since it is trained without any cross-lingual objective and with no aligned data. In this work, we provide a comprehensive study of the contribution of different components in M-BERT to its cross-lingual ability. We study the impact of linguistic properties of the languages, the architecture of the model, and of the learning objectives. The experimental study is done in the context of three typologically different languages -- Spanish, Hindi, and Russian -- and using two conceptually different NLP tasks, textual entailment and named entity recognition. Among our key conclusions is the fact that lexical overlap between languages plays a negligible role in the cross-lingual success, while the depth of the network is an important part of it", "date": "2020", "authors": ["Karthikeyan K 1, Zihan Wang 2, Stephen Mayhew 3, Dan Roth 3"], "references": [2963341956, 2965373594, 2972324944, 3011411500, 2914120296, 2891555348, 2952638691, 2962795068, 2154474435, 2573062194]}, {"id": 2806070179, "title": "Mask R-CNN", "abstract": "We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron .", "date": "2020", "authors": ["Kaiming He , Georgia Gkioxari , Piotr Dollar , Ross Girshick"], "references": [2194775991, 2618530766, 639708223, 2102605133, 1903029394, 1536680647, 2806070179, 1861492603, 2109255472, 2088049833]}, {"id": 1486649854, "title": "Skip-thought vectors", "abstract": "We describe an approach for unsupervised learning of a generic, distributed sentence encoder. Using the continuity of text from books, we train an encoder-decoder model that tries to reconstruct the surrounding sentences of an encoded passage. Sentences that share semantic and syntactic properties are thus mapped to similar vector representations. We next introduce a simple vocabulary expansion method to encode words that were not seen as part of training, allowing us to expand our vocabulary to a million words. After training our model, we extract and evaluate our vectors with linear models on 8 tasks: semantic relatedness, paraphrase detection, image-sentence ranking, question-type classification and 4 benchmark sentiment and subjectivity datasets. The end result is an off-the-shelf encoder that can produce highly generic sentence representations that are robust and perform well in practice.", "date": "2015", "authors": ["Ryan Kiros 1, Yukun Zhu 1, Ruslan Salakhutdinov 2, Richard S. Zemel 2, Antonio Torralba 3, Raquel Urtasun 1, Sanja Fidler 1"], "references": [2962835968, 2964121744, 2964308564, 1614298861, 2130942839, 2157331557, 1861492603, 1832693441, 2064675550, 2187089797]}, {"id": 2963918774, "title": "Supervised Learning of Universal Sentence Representations from Natural Language Inference Data", "abstract": "Many modern NLP systems rely on word embeddings, previously trained in an unsupervised manner on large corpora, as base features. Efforts to obtain embeddings for larger chunks of text, such as sentences, have however not been so successful. Several attempts at learning unsupervised representations of sentences have not reached satisfactory enough performance to be widely adopted. In this paper, we show how universal sentence representations trained using the supervised data of the Stanford Natural Language Inference datasets can consistently outperform unsupervised methods like SkipThought vectors on a wide range of transfer tasks. Much like how computer vision uses ImageNet to obtain features, which can then be transferred to other tasks, our work tends to indicate the suitability of natural language inference for transfer learning to other NLP tasks. Our encoder is publicly available.", "date": "2017", "authors": ["Alexis Conneau 1, Douwe Kiela 2, Holger Schwenk 3, Lo\u00efc Barrault 3, Antoine Bordes 1"], "references": [2194775991, 2964121744, 2153579005, 2250539671, 2130942839, 2108598243, 2158899491, 2064675550, 2131744502, 2145287260]}, {"id": 2963846996, "title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference", "abstract": "This paper introduces the Multi-Genre Natural Language Inference (MultiNLI) corpus, a dataset designed for use in the development and evaluation of machine learning models for sentence understanding. At 433k examples, this resource is one of the largest corpora available for natural language inference (a.k.a. recognizing textual entailment), improving upon available resources in both its coverage and difficulty. MultiNLI accomplishes this by offering data from ten distinct genres of written and spoken English, making it possible to evaluate systems on nearly the full complexity of the language, while supplying an explicit setting for evaluating cross-genre domain adaptation. In addition, an evaluation using existing machine learning models designed for the Stanford NLI corpus shows that it represents a substantially more difficult task than does that corpus, despite the two showing similar levels of inter-annotator agreement.", "date": "2018", "authors": ["Adina Williams , Nikita Nangia , Samuel R. Bowman"], "references": [2618530766, 2964121744, 2095705004, 2250539671, 1849277567, 2155541015, 2064675550, 1840435438, 2963918774, 1632114991]}, {"id": 2525127255, "title": "The PASCAL Recognising Textual Entailment Challenge", "abstract": "This paper describes the PASCAL Network of Excellence first Recognising Textual Entailment (RTE-1) Challenge benchmark 1 . The RTE task is defined as recognizing, given two text fragments, whether the meaning of one text can be inferred (entailed) from the other. This application-independent task is suggested as capturing major inferences about the variability of semantic expression which are commonly needed across multiple applications. The Challenge has raised noticeable attention in the research community, attracting 17 submissions from diverse groups, suggesting the generic relevance of the task.", "date": "2005", "authors": ["Ido Dagan , Oren Glickman , Bernardo Magnini"], "references": [1840435438, 2965373594, 1970381522, 2923014074, 2185175083, 2963846996, 2125436846, 2996428491]}, {"id": 2962736243, "title": "Annotation Artifacts in Natural Language Inference Data", "abstract": "Large-scale datasets for natural language inference are created by presenting crowd workers with a sentence (premise), and asking them to generate three new sentences (hypotheses) that it entails, contradicts, or is logically neutral with respect to. We show that, in a significant portion of such data, this protocol leaves clues that make it possible to identify the label by looking only at the hypothesis, without observing the premise. Specifically, we show that a simple text categorization model can correctly classify the hypothesis alone in about 67% of SNLI (Bowman et. al, 2015) and 53% of MultiNLI (Williams et. al, 2017). Our analysis reveals that specific linguistic phenomena such as negation and vagueness are highly correlated with certain inference classes. Our findings suggest that the success of natural language inference models to date has been overestimated, and that the task remains a hard open problem.", "date": "2018", "authors": ["Suchin Gururangan 1, Swabha Swayamdipta 2, Omer Levy 1, Roy Schwartz 1, 3, Samuel R. Bowman 4, Noah A. Smith 1"], "references": [2963748441, 1840435438, 1544827683, 1933349210, 2963626623, 2963918774, 2963846996, 2413794162, 2963969878, 2962809918]}, {"id": 3104033643, "title": "SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation", "abstract": "Semantic Textual Similarity (STS) measures the meaning similarity of sentences. Applications include machine translation (MT), summarization, generation, question answering (QA), short answer grading, semantic search, dialog and conversational systems. The STS shared task is a venue for assessing the current state-of-the-art. The 2017 task focuses on multilingual and cross-lingual pairs with one sub-track exploring MT quality estimation (MTQE) data. The task obtained strong participation from 31 teams, with 17 participating in all language tracks. We summarize performance and review a selection of well performing methods. Analysis highlights common errors, providing insight into the limitations of existing models. To support ongoing work on semantic representations, the STS Benchmark is introduced as a new shared training and evaluation set carefully selected from the corpus of English STS shared task data (2012-2017).", "date": "2017", "authors": ["Daniel M. Cer 1, Mona T. Diab 2, Eneko Agirre 3, I\u00f1igo Lopez-Gazpio 3, Lucia Specia 4"], "references": [2153579005, 1614298861, 2250539671, 2064675550, 2131744502, 2038721957, 2123442489, 1840435438, 1486649854, 2963626623]}, {"id": 2130158090, "title": "The Third PASCAL Recognizing Textual Entailment Challenge", "abstract": "This paper presents the Third PASCAL Recognising Textual Entailment Challenge (RTE-3), providing an overview of the dataset creating methodology and the submitted systems. In creating this year's dataset, a number of longer texts were introduced to make the challenge more oriented to realistic scenarios. Additionally, a pool of resources was offered so that the participants could share common tools. A pilot task was also set up, aimed at differentiating unknown entailments from identified contradictions and providing justifications for overall system decisions. 26 participants submitted 44 runs, using different approaches and generally presenting new entailment models and achieving higher scores than in the previous challenges.", "date": "2007", "authors": ["Danilo Giampiccolo 1, Bernardo Magnini 2, Ido Dagan 3, Bill Dolan 4"], "references": [2912565176, 2525127255, 2115792525, 2102065370, 2396767181, 2002664886, 1990524510, 137514618, 2182572585, 2134061542]}, {"id": 2964350391, "title": "Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning", "abstract": "Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question: Are there any benefits to combining Inception architectures with residual connections? Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4 networks, we achieve 3.08% top-5 error on the test set of the ImageNet classification (CLS) challenge.", "date": "2016", "authors": ["Christian Szegedy , Sergey Ioffe , Vincent Vanhoucke , Alexander A Alemi"], "references": [2604319603, 2955425717, 2996428491, 2965658867, 2886335102, 2942841021, 2963918968, 2774644650, 2963402313]}, {"id": 179875071, "title": "Recurrent neural network based language model", "abstract": "A new recurrent neural network based language model (RNN LM) with applications to speech recognition is presented. Results indicate that it is possible to obtain around 50% reduction of perplexity by using mixture of several RNN LMs, compared to a state of the art backoff language model. Speech recognition experiments show around 18% reduction of word error rate on the Wall Street Journal task when comparing models trained on the same amount of data, and around 5% on the much harder NIST RT05 task, even when the backoff model is trained on much more data than the RNN LM. We provide ample empirical evidence to suggest that connectionist language models are superior to standard n-gram techniques, except their high computational (training) complexity. Index Terms: language modeling, recurrent neural networks, speech recognition", "date": "2009", "authors": ["Tomas Mikolov 1, Martin Karafi\u00e1t 1, Luk\u00e1s Burget 1, Jan Cernock\u00fd , Sanjeev Khudanpur 2"], "references": [2132339004, 2110485445, 2107878631, 36903255, 2096072088, 2468573742, 2152808281, 2027499299, 2292896937, 2437096199]}, {"id": 2963374479, "title": "Neural Architecture Search with Reinforcement Learning", "abstract": "", "date": "2016", "authors": ["Barret Zoph , Quoc V. Le"], "references": [2963163009, 2963420686, 2964081807, 2810075754, 2955425717, 2962851801, 2965658867, 2942841021]}, {"id": 2584341106, "title": "Memory Networks", "abstract": "Abstract: We describe a new class of learning models called memory networks. Memory networks reason with inference components combined with a long-term memory component; they learn how to use these jointly. The long-term memory can be read and written to, with the goal of using it for prediction. We investigate these models in the context of question answering (QA) where the long-term memory effectively acts as a (dynamic) knowledge base, and the output is a textual response. We evaluate them on a large-scale QA task, and a smaller, but more complex, toy task generated from a simulated world. In the latter, we show the reasoning power of such models by chaining multiple supporting sentences to answer questions that require understanding the intension of verbs.", "date": "2014", "authors": ["Jason Weston , Sumit Chopra , Antoine Bordes"], "references": [1840435438, 1793121960, 2551396370, 2507756961, 2962809918, 2963907629, 2964210218]}, {"id": 2135046866, "title": "Regression Shrinkage and Selection via the Lasso", "abstract": "SUMMARY We propose a new method for estimation in linear models. The 'lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.", "date": "1995", "authors": ["Robert Tibshirani"], "references": [2331432542, 3085162807, 2158940042, 1594031697, 2797583072, 2106706098, 2102201073, 2117897510, 2075665712, 2954064014]}, {"id": 2145094598, "title": "Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion", "abstract": "We explore an original strategy for building deep networks, based on stacking layers of denoising autoencoders which are trained locally to denoise corrupted versions of their inputs. The resulting algorithm is a straightforward variation on the stacking of ordinary autoencoders. It is however shown on a benchmark of classification problems to yield significantly lower classification error, thus bridging the performance gap with deep belief networks (DBN), and in several cases surpassing it. Higher level representations learnt in this purely unsupervised fashion also help boost the performance of subsequent SVM classifiers. Qualitative experiments show that, contrary to ordinary autoencoders, denoising autoencoders are able to learn Gabor-like edge detectors from natural image patches and larger stroke detectors from digit images. This work clearly establishes the value of using a denoising criterion as a tractable unsupervised objective to guide the learning of useful higher level representations.", "date": "2010", "authors": ["Pascal Vincent , Hugo Larochelle , Isabelle Lajoie , Yoshua Bengio , Pierre-Antoine Manzagol"], "references": [2136922672, 2100495367, 2072128103, 2116064496, 2025768430, 2110798204, 1652505363, 3110653090, 1479807131, 1994197834]}, {"id": 2131241448, "title": "Practical Bayesian Optimization of Machine Learning Algorithms", "abstract": "The use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters. Unfortunately, this tuning is often a \"black art\" requiring expert experience, rules of thumb, or sometimes brute-force search. There is therefore great appeal for automatic approaches that can optimize the performance of any given learning algorithm to the problem at hand. In this work, we consider this problem through the framework of Bayesian optimization, in which a learning algorithm's generalization performance is modeled as a sample from a Gaussian process (GP). We show that certain choices for the nature of the GP, such as the type of kernel and the treatment of its hyperparameters, can play a crucial role in obtaining a good optimizer that can achieve expertlevel performance. We describe new algorithms that take into account the variable cost (duration) of learning algorithm experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.", "date": "2012", "authors": ["Jasper Snoek 1, Hugo Larochelle 2, Ryan P Adams 3"], "references": [3118608800, 1746819321, 2141125852, 2097998348, 2106411961, 2951665052, 60686164, 2165599843, 2099201756, 1973333099]}, {"id": 2296319761, "title": "Convex Optimization", "abstract": "Convex optimization problems arise frequently in many different fields. A comprehensive introduction to the subject, this book shows in detail how such problems can be solved numerically with great efficiency. The focus is on recognizing convex optimization problems and then finding the most appropriate technique for solving them. The text contains many worked examples and homework exercises and will appeal to students, researchers and practitioners in fields such as engineering, computer science, mathematics, statistics, finance, and economics.", "date": "2004", "authors": ["Stephen Boyd 1, Lieven Vandenberghe 2"], "references": [2030723843, 2000296233, 2020830442, 2006980285, 82689443, 2611147814]}, {"id": 3120740533, "title": "UCI Machine Learning Repository", "abstract": "", "date": "2006", "authors": ["A. Asuncion"], "references": []}, {"id": 2798766386, "title": "Nonlinear Programming", "abstract": "", "date": "1994", "authors": ["Dimitri Bertsekas"], "references": [2164278908, 2146502635, 3029645440, 2100556411, 2067191022, 1964357740, 2202343345, 2109449402, 1601740268, 2132870739]}, {"id": 2610857016, "title": "Matrix Analysis", "abstract": "Linear algebra and matrix theory are fundamental tools in mathematical and physical science, as well as fertile fields for research. This new edition of the acclaimed text presents results of both classic and recent matrix analyses using canonical forms as a unifying theme, and demonstrates their importance in a variety of applications. The authors have thoroughly revised, updated, and expanded on the first edition. The book opens with an extended summary of useful concepts and facts and includes numerous new topics and features, such as: - New sections on the singular value and CS decompositions - New applications of the Jordan canonical form - A new section on the Weyr canonical form - Expanded treatments of inverse problems and of block matrices - A central role for the Von Neumann trace theorem - A new appendix with a modern list of canonical forms for a pair of Hermitian matrices and for a symmetric-skew symmetric pair - Expanded index with more than 3,500 entries for easy reference - More than 1,100 problems and exercises, many with hints, to reinforce understanding and develop auxiliary themes such as finite-dimensional quantum systems, the compound and adjugate matrices, and the Loewner ellipsoid - A new appendix provides a collection of problem-solving hints.", "date": "1984", "authors": ["Roger A. Horn 1, Charles R. Johnson 2"], "references": [2146502635, 3029645440, 2107396783, 2118040894, 2139212933, 2053186076, 2160643434, 2165744313, 2151795416]}, {"id": 2167732364, "title": "Smooth minimization of non-smooth functions", "abstract": "In this paper we propose a new approach for constructing efficient schemes for non-smooth convex optimization. It is based on a special smoothing technique, which can be applied to functions with explicit max-structure. Our approach can be considered as an alternative to black-box minimization. From the viewpoint of efficiency estimates, we manage to improve the traditional bounds on the number of iterations of the gradient schemes from ** keeping basically the complexity of each iteration unchanged.", "date": "2005", "authors": ["Yu Nesterov"], "references": [2124541940, 1568307856, 1553702074, 1568288633, 1669104078, 2015263936, 2150126561, 2969945254, 2008164266]}, {"id": 2150102617, "title": "RCV1: A New Benchmark Collection for Text Categorization Research", "abstract": "Reuters Corpus Volume I (RCV1) is an archive of over 800,000 manually categorized newswire stories recently made available by Reuters, Ltd. for research purposes. Use of this data for research on text categorization requires a detailed understanding of the real world constraints under which the data was produced. Drawing on interviews with Reuters personnel and access to Reuters documentation, we describe the coding policy and quality control procedures used in producing the RCV1 data, the intended semantics of the hierarchical category taxonomies, and the corrections necessary to remove errorful data. We refer to the original data as RCV1-v1, and the corrected data as RCV1-v2. We benchmark several widely used supervised learning methods on RCV1-v2, illustrating the collection's properties, suggesting new directions for research, and providing baseline results for future studies. We make available detailed, per-category experimental results, as well as corrected versions of the category assignments and taxonomy structures, via online appendices.", "date": "2004", "authors": ["David D. Lewis , Yiming Yang , Tony G. Rose , Fan Li"], "references": [2118020653, 2149684865, 2098162425, 2118202495, 2435251607, 2114535528, 2005422315, 2107008379, 2768149277, 2000672666]}, {"id": 2124541940, "title": "Introductory Lectures on Convex Optimization", "abstract": "It was in the middle of the 1980s, when the seminal paper by Kar markar opened a new epoch in nonlinear optimization. The importance of this paper, containing a new polynomial-time algorithm for linear op timization problems, was not only in its complexity bound. At that time, the most surprising feature of this algorithm was that the theoretical pre diction of its high efficiency was supported by excellent computational results. This unusual fact dramatically changed the style and direc tions of the research in nonlinear optimization. Thereafter it became more and more common that the new methods were provided with a complexity analysis, which was considered a better justification of their efficiency than computational experiments. In a new rapidly develop ing field, which got the name \"polynomial-time interior-point methods\", such a justification was obligatory. Afteralmost fifteen years of intensive research, the main results of this development started to appear in monographs [12, 14, 16, 17, 18, 19]. Approximately at that time the author was asked to prepare a new course on nonlinear optimization for graduate students. The idea was to create a course which would reflect the new developments in the field. Actually, this was a major challenge. At the time only the theory of interior-point methods for linear optimization was polished enough to be explained to students. The general theory of self-concordant functions had appeared in print only once in the form of research monograph [12].", "date": "2002", "authors": ["Yurii Nesterov"], "references": [2146502635, 3029645440, 104184427, 2092663520, 2473418344, 2107438106, 2167732364, 607505555, 2913535645]}, {"id": 1992208280, "title": "Robust Stochastic Approximation Approach to Stochastic Programming", "abstract": "In this paper we consider optimization problems where the objective function is given in a form of the expectation. A basic difficulty of solving such stochastic optimization problems is that the involved multidimensional integrals (expectations) cannot be computed with high accuracy. The aim of this paper is to compare two computational approaches based on Monte Carlo sampling techniques, namely, the stochastic approximation (SA) and the sample average approximation (SAA) methods. Both approaches, the SA and SAA methods, have a long history. Current opinion is that the SAA method can efficiently use a specific (say, linear) structure of the considered problem, while the SA approach is a crude subgradient method, which often performs poorly in practice. We intend to demonstrate that a properly modified SA approach can be competitive and even significantly outperform the SAA method for a certain class of convex stochastic problems. We extend the analysis to the case of convex-concave stochastic saddle point problems and present (in our opinion highly encouraging) results of numerical experiments.", "date": "2008", "authors": ["A. Nemirovski 1, A. Juditsky 2, G. Lan 1, A. Shapiro 1"], "references": [2038669746, 2169713291, 203276351, 2064076655, 1983916623, 2090359754, 2000257769, 1490324987, 2086161653, 2000953623]}, {"id": 2160218441, "title": "Online Passive-Aggressive Algorithms", "abstract": "We present a family of margin based online learning algorithms for various prediction tasks. In particular we derive and analyze algorithms for binary and multiclass categorization, regression, uniclass prediction and sequence prediction. The update steps of our different algorithms are all based on analytical solutions to simple constrained optimization problems. This unified view allows us to prove worst-case loss bounds for the different algorithms and for the various decision problems based on a single lemma. Our bounds on the cumulative loss of the algorithms are relative to the smallest loss that can be attained by any fixed hypothesis, and as such are applicable to both realizable and unrealizable settings. We demonstrate some of the merits of the proposed algorithms in a series of experiments with synthetic and real data sets.", "date": "2006", "authors": ["Koby Crammer 1, 2, Ofer Dekel 2, Joseph Keshet 2, Shai Shalev-Shwartz 2, Yoram Singer 2"], "references": [2296319761, 2148603752, 1563088657, 1560724230, 1601740268, 2053463056, 1978394996, 2032210760, 2101276256, 2157791002]}, {"id": 2116064496, "title": "Training products of experts by minimizing contrastive divergence", "abstract": "It is possible to combine multiple latent-variable models of the same data by multiplying their probability distributions together and then renormalizing. This way of combining individual \"expert\" models makes it hard to generate samples from the combined model but easy to infer the values of the latent variables of each expert, because the combination rule ensures that the latent variables of different experts are conditionally independent when given the data. A product of experts (PoE) is therefore an interesting candidate for a perceptual system in which rapid inference is vital and generation is unnecessary. Training a PoE by maximizing the likelihood of the data is difficult because it is hard even to approximate the derivatives of the renormalization term in the combination rule. Fortunately, a PoE can be trained using a different objective function called \"contrastive divergence\" whose derivatives with regard to the parameters can be approximated accurately and efficiently. Examples are presented of contrastive divergence learning using several types of expert on several types of data.", "date": "2002", "authors": ["Geoffrey E. Hinton"], "references": [1652505363, 1997063559, 2096175520, 1746680969, 1993845689, 2165225968, 2083380015, 1547224907, 2114153178, 2101706260]}, {"id": 2134557905, "title": "Learning methods for generic object recognition with invariance to pose and lighting", "abstract": "We assess the applicability of several popular learning methods for the problem of recognizing generic visual categories with invariance to pose, lighting, and surrounding clutter. A large dataset comprising stereo image pairs of 50 uniform-colored toys under 36 azimuths, 9 elevations, and 6 lighting conditions was collected (for a total of 194,400 individual images). The objects were 10 instances of 5 generic categories: four-legged animals, human figures, airplanes, trucks, and cars. Five instances of each category were used for training, and the other five for testing. Low-resolution grayscale images of the objects with various amounts of variability and surrounding clutter were used for training and testing. Nearest neighbor methods, support vector machines, and convolutional networks, operating on raw pixels or on PCA-derived features were tested. Test error rates for unseen object instances placed on uniform backgrounds were around 13% for SVM and 7% for convolutional nets. On a segmentation/recognition task with highly cluttered images, SVM proved impractical, while convolutional nets yielded 16/7% error. A real-time version of the system was implemented that can detect and classify objects in natural scenes at around 10 frames per second.", "date": "2004", "authors": ["Y. LeCun 1, Fu Jie Huang 1, L. Bottou 2"], "references": [2164598857, 2310919327, 2217896605, 2124087378, 2124351082, 2123977795, 2155511848, 2160225842, 2295106276, 2141376824]}, {"id": 2099866409, "title": "Restricted Boltzmann machines for collaborative filtering", "abstract": "Most of the existing approaches to collaborative filtering cannot handle very large data sets. In this paper we show how a class of two-layer undirected graphical models, called Restricted Boltzmann Machines (RBM's), can be used to model tabular data, such as user's ratings of movies. We present efficient learning and inference procedures for this class of models and demonstrate that RBM's can be successfully applied to the Netflix data set, containing over 100 million user/movie ratings. We also show that RBM's slightly outperform carefully-tuned SVD models. When the predictions of multiple RBM models and multiple SVD models are linearly combined, we achieve an error rate that is well over 6% better than the score of Netflix's own system.", "date": "2007", "authors": ["Ruslan Salakhutdinov , Andriy Mnih , Geoffrey Hinton"], "references": [2136922672, 2100495367, 2116064496, 2147152072, 1612003148, 2122090912, 2158164339, 2124914669, 205159212, 2165395308]}, {"id": 2536626143, "title": "Attribute and simile classifiers for face verification", "abstract": "We present two novel methods for face verification. Our first method - \u201cattribute\u201d classifiers - uses binary classifiers trained to recognize the presence or absence of describable aspects of visual appearance (e.g., gender, race, and age). Our second method - \u201csimile\u201d classifiers - removes the manual labeling required for attribute classification and instead learns the similarity of faces, or regions of faces, to specific reference people. Neither method requires costly, often brittle, alignment between image pairs; yet, both methods produce compact visual descriptions, and work on real-world images. Furthermore, both the attribute and simile classifiers improve on the current state-of-the-art for the LFW data set, reducing the error rates compared to the current best by 23.92% and 26.34%, respectively, and 31.68% when combined. For further testing across pose, illumination, and expression, we introduce a new data set - termed PubFig - of real-world images of public figures (celebrities and politicians) acquired from the internet. This data set is both larger (60,000 images) and deeper (300 images per individual) than existing data sets of its kind. Finally, we present an evaluation of human performance.", "date": "2009", "authors": ["Neeraj Kumar , Alexander C. Berg , Peter N. Belhumeur , Shree K. Nayar"], "references": [2151103935, 2119821739, 1782590233, 1989702938, 2112076978, 2033419168, 3111480503, 2098947662, 2905573712, 2155759509]}, {"id": 2173213060, "title": "MapReduce: simplified data processing on large clusters", "abstract": "MapReduce is a programming model and an associated implementation for processing and generating large datasets that is amenable to a broad variety of real-world tasks. Users specify the computation in terms of a map and a reduce function, and the underlying runtime system automatically parallelizes the computation across large-scale clusters of machines, handles machine failures, and schedules inter-machine communication to make efficient use of the network and disks. Programmers find the system easy to use: more than ten thousand distinct MapReduce programs have been implemented internally at Google over the past four years, and an average of one hundred thousand MapReduce jobs are executed on Google's clusters every day, processing a total of more than twenty petabytes of data per day.", "date": "2007", "authors": ["Jeffrey Dean , Sanjay Ghemawat"], "references": [2173213060, 2119565742, 2148317584, 2073965851, 2109722477, 2104644701, 1510543252, 2044534358, 1988243929, 2045271686]}, {"id": 2141125852, "title": "Multi-column deep neural networks for image classification", "abstract": "Traditional methods of computer vision and machine learning cannot match human performance on tasks such as the recognition of handwritten digits or traffic signs. Our biologically plausible, wide and deep artificial neural network architectures can. Small (often minimal) receptive fields of convolutional winner-take-all neurons yield large network depth, resulting in roughly as many sparsely connected neural layers as found in mammals between retina and visual cortex. Only winner neurons are trained. Several deep neural columns become experts on inputs preprocessed in different ways; their predictions are averaged. Graphics cards allow for fast training. On the very competitive MNIST handwriting benchmark, our method is the first to achieve near-human performance. On a traffic sign recognition benchmark it outperforms humans by a factor of two. We also improve the state-of-the-art on a plethora of common image classification benchmarks.", "date": "2012", "authors": ["Dan Cire\u015fan , Ueli Meier , Juergen Schmidhuber"], "references": [3118608800, 2310919327, 2110798204, 2154642048, 2134557905, 2156163116, 2138857742, 2148461049, 2144982973, 1991848143]}, {"id": 2184045248, "title": "Deep Neural Networks for Acoustic Modeling in Speech Recognition", "abstract": "Most current speech recognition systems use hidden Markov models (HMMs) to deal with the temporal variability of speech and Gaussian mixture models (GMMs) to determine how well each state of each HMM fits a frame or a short window of frames of coefficients that represents the acoustic input. An alternative way to evaluate the fit is to use a feed-forward neural network that takes several frames of coefficients as input and produces posterior probabilities over HMM states as output. Deep neural networks (DNNs) that have many hidden layers and are trained using new methods have been shown to outperform GMMs on a variety of speech recognition benchmarks, sometimes by a large margin. This article provides an overview of this progress and represents the shared views of four research groups that have had recent successes in using DNNs for acoustic modeling in speech recognition.", "date": "2012", "authors": ["Geoffrey Hinton , Li Deng , Dong Yu , George Dahl , Abdel-rahman Mohamed , Navdeep Jaitly , Andrew Senior , Vincent Vanhoucke , Patrick Nguyen , Tara Sainath , Brian Kingsbury"], "references": [2136922672, 2100495367, 1533861849, 2116064496, 2147768505, 2145094598, 1993882792, 2159080219, 44815768, 1498436455]}, {"id": 2118858186, "title": "An analysis of single-layer networks in unsupervised feature learning", "abstract": "A great deal of research has focused on algorithms for learning features from unlabeled data. Indeed, much progress has been made on benchmark datasets like NORB and CIFAR by employing increasingly complex unsupervised learning algorithms and deep models. In this paper, however, we show that several simple factors, such as the number of hidden nodes in the model, may be more important to achieving high performance than the learning algorithm or the depth of the model. Specifically, we will apply several othe-shelf feature learning algorithms (sparse auto-encoders, sparse RBMs, K-means clustering, and Gaussian mixtures) to CIFAR, NORB, and STL datasets using only singlelayer networks. We then present a detailed analysis of the eect of changes in the model setup: the receptive field size, number of hidden nodes (features), the step-size (\u201cstride\u201d) between extracted features, and the eect of whitening. Our results show that large numbers of hidden nodes and dense feature extraction are critical to achieving high performance\u2014so critical, in fact, that when these parameters are pushed to their limits, we achieve state-of-the-art performance on both CIFAR-10 and NORB using only a single layer of features. More surprisingly, our best performance is based on K-means clustering, which is extremely fast, has no hyperparameters to tune beyond the model structure itself, and is very easy to implement. Despite the simplicity of our system, we achieve accuracy beyond all previously published results on the CIFAR-10 and NORB datasets (79.6% and 97.2% respectively).", "date": "2011", "authors": ["Adam Coates 1, Andrew Y. Ng 2, Honglak Lee 1"], "references": [2136922672, 3118608800, 2162915993, 2116064496, 2546302380, 2025768430, 2130325614, 2097018403, 2107034620, 1625255723]}, {"id": 196761320, "title": "Deep learning via Hessian-free optimization", "abstract": "We develop a 2nd-order optimization method based on the \"Hessian-free\" approach, and apply it to training deep auto-encoders. Without using pre-training, we obtain results superior to those reported by Hinton & Salakhutdinov (2006) on the same tasks they considered. Our method is practical, easy to use, scales nicely to very large datasets, and isn't limited in applicability to auto-encoders, or any specific model class. We also discuss the issue of \"pathological curvature\" as a possible explanation for the difficulty of deep-learning and how 2nd-order optimization, and our method in particular, effectively deals with it.", "date": "2010", "authors": ["James Martens"], "references": [2100495367, 3029645440, 2110798204, 2138857742, 2914484425, 2606321545, 2006903949, 2130984546, 2166347285, 3023533631]}, {"id": 2912934387, "title": "Bagging predictors", "abstract": "", "date": "1996", "authors": ["Leo Breiman"], "references": [2331432542, 3085162807, 2102201073, 139959648, 2111814036, 1969557815, 2030748132, 1482451543, 1531648066, 1541145887]}, {"id": 2335728318, "title": "Reading Digits in Natural Images with Unsupervised Feature Learning", "abstract": "Detecting and reading text from natural images is a hard computer vision task that is central to a variety of emerging applications. Related problems like document character recognition have been widely studied by computer vision and machine learning researchers and are virtually solved for practical applications like reading handwritten digits. Reliably recognizing characters in more complex scenes like photographs, however, is far more difficult: the best existing methods lag well behind human performance on the same tasks. In this paper we attack the problem of recognizing digits in a real application using unsupervised feature learning methods: reading house numbers from street level photos. To this end, we introduce a new benchmark dataset for research use containing over 600,000 labeled digits cropped from Street View images. We then demonstrate the difficulty of recognizing these digits when the problem is approached with hand-designed features. Finally, we employ variants of two recently proposed unsupervised feature learning methods and find that they are convincingly superior on our benchmarks.", "date": "2010", "authors": ["Yuval Netzer 1, Tao Wang 1, Adam Coates 1, Alessandro Bissacco 2, Bo Wu 2, Andrew Y. Ng 2"], "references": []}, {"id": 189596042, "title": "Deep Boltzmann machines", "abstract": "We present a new learning algorithm for Boltzmann machines that contain many layers of hidden variables. Data-dependent expectations are estimated using a variational approximation that tends to focus on a single mode, and dataindependent expectations are approximated using persistent Markov chains. The use of two quite different techniques for estimating the two types of expectation that enter into the gradient of the log-likelihood makes it practical to learn Boltzmann machines with multiple hidden layers and millions of parameters. The learning can be made more efficient by using a layer-by-layer \u201cpre-training\u201d phase that allows variational inference to be initialized with a single bottomup pass. We present results on the MNIST and NORB datasets showing that deep Boltzmann machines learn good generative models and perform well on handwritten digit and visual object recognition tasks.", "date": "2009", "authors": ["Ruslan Salakhutdinov , Geoffrey E. Hinton"], "references": [2136922672, 2100495367, 2116064496, 2134557905, 2096192494, 2613634265, 2567948266, 2116825644, 2159737176, 2124914669]}, {"id": 2147152072, "title": "Indexing by Latent Semantic Analysis", "abstract": "A new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher-order structure in the association of terms with documents (\u201csemantic structure\u201d) in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular-value decomposition, in which a large term by document matrix is decomposed into a set of ca. 100 orthogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca. 100 item vectors of factor weights. Queries are represented as pseudo-document vectors formed from weighted combinations of terms, and documents with supra-threshold cosine values are returned. initial tests find this completely automatic method for retrieval to be promising.", "date": "1990", "authors": ["Scott Deerwester 1, Susan T. Dumais 2, George W. Furnas 2, Thomas K. Landauer 2, Richard Harshman 3"], "references": [1956559956, 1984565341, 1964262399, 2000215628, 2114804204, 2151561903, 3012395598, 2024683548, 1965061793, 2096411881]}, {"id": 1631260214, "title": "SRILM \u2013 An Extensible Language Modeling Toolkit", "abstract": "", "date": "2001", "authors": ["Andreas Stolcke"], "references": [2158195707, 2121227244, 1904457459, 2594610113, 1549285799, 2100506586, 1797288984, 2097978681, 1528470941, 2127836646]}, {"id": 2096175520, "title": "A maximum entropy approach to natural language processing", "abstract": "The concept of maximum entropy can be traced back along multiple threads to Biblical times. Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition. In this paper, we describe a method for statistical modeling based on maximum entropy. We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently, using as examples several problems in natural language processing.", "date": "1996", "authors": ["Adam L. Berger 1, Vincent J. Della Pietra 2, Stephen A. Della Pietra 2"], "references": [2099111195, 2049633694, 2006969979, 2121227244, 2160842254, 2097333193, 1597533204, 2099345940, 2167434254, 1976241232]}, {"id": 2110485445, "title": "Finding Structure in Time", "abstract": "Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves; the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands; indeed, in this approach the notion of memory is inextricably bound up with task processing. These representations reveal a rich structure, which allows them to be highly context-dependent while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction.", "date": "1990", "authors": ["Jeffrey L. Elman"], "references": [2154642048, 1652505363, 2173629880, 2016589492, 3036751298, 2118373646, 2046432185, 2122988375, 2170716495, 2094249282]}, {"id": 1575350781, "title": "MPI: A Message-Passing Interface Standard", "abstract": "The Message Passing Interface Forum (MPIF), with participation from over 40 organizations, has been meeting since November 1992 to discuss and define a set of library standards for message passing. MPIF is not sanctioned or supported by any official standards organization. The goal of the Message Passing Interface, simply stated, is to develop a widely used standard for writing message-passing programs. As such the interface should establish a practical, portable, efficient and flexible standard for message passing. , This is the final report, Version 1.0, of the Message Passing Interface Forum. This document contains all the technical features proposed for the interface. This copy of the draft was processed by LATEX on April 21, 1994. , Please send comments on MPI to mpi-comments@cs.utk.edu. Your comment will be forwarded to MPIF committee members who will attempt to respond.", "date": "1994", "authors": ["Message P Forum"], "references": [1480928214, 1521571223, 1964564149, 1978513924, 2575211436, 2083200599, 2090683636, 2010542899, 2992405598, 2294265735]}, {"id": 2158195707, "title": "An empirical study of smoothing techniques for language modeling", "abstract": "We survey the most widely-used algorithms for smoothing models for language n -gram modeling. We then present an extensive empirical comparison of several of these smoothing techniques, including those described by Jelinek and Mercer (1980); Katz (1987); Bell, Cleary and Witten (1990); Ney, Essen and Kneser (1994), and Kneser and Ney (1995). We investigate how factors such as training data size, training corpus (e.g. Brown vs. Wall Street Journal), count cutoffs, and n -gram order (bigram vs. trigram) affect the relative performance of these methods, which is measured through the cross-entropy of test data. We find that these factors can significantly affect the relative performance of models, with the most significant factor being training data size. Since no previous comparisons have examined these factors systematically, this is the first thorough characterization of the relative performance of various algorithms. In addition, we introduce methodologies for analyzing smoothing algorithm efficacy in detail, and using these techniques we motivate a novel variation of Kneser?Ney smoothing that consistently outperforms all other algorithms evaluated. Finally, results showing that improved language model smoothing leads to improved speech recognition performance are presented.", "date": "1999", "authors": ["Stanley F. Chen 1, Joshua Goodman 2"], "references": [2170120409, 2158195707, 2121227244, 2099247782, 2097333193, 1966812932, 2611071497, 2166637769, 2134237567, 2075201173]}, {"id": 2121227244, "title": "Class-based n -gram models of natural language", "abstract": "We address the problem of predicting a word from previous words in a sample of text. In particular, we discuss n-gram models based on classes of words. We also discuss several statistical algorithms for assigning words to classes based on the frequency of their co-occurrence with other words. We find that we are able to extract classes that have the flavor of either syntactically based groupings or semantically based groupings, depending on the nature of the underlying statistics.", "date": "1992", "authors": ["Peter F. Brown , Peter V. deSouza , Robert L. Mercer , Vincent J. Della Pietra , Jenifer C. Lai"], "references": [2049633694, 2097333193, 1966812932, 2142901448, 2751862591, 1597533204, 1575431606, 2007780422, 2016871293, 1628850721]}, {"id": 2914484425, "title": "Efficient BackProp", "abstract": "", "date": "1997", "authors": ["Yann LeCun , L\u00e9on Bottou , Genevieve B. Orr , Klaus-Robert M\u00fcller"], "references": [2156909104, 2148603752, 1554663460, 2798909945, 2077658674, 2171277043, 1970789124, 2173629880, 2076118331, 2051812123]}, {"id": 2120420045, "title": "No more pesky learning rates", "abstract": "The performance of stochastic gradient descent (SGD) depends critically on how learning rates are tuned and decreased over time. We propose a method to automatically adjust multiple learning rates so as to minimize the expected error at any one time. The method relies on local gradient variations across samples. In our approach, learning rates can increase as well as decrease, making it suitable for non-stationary problems. Using a number of convex and non-convex learning tasks, we show that the resulting algorithm matches the performance of SGD or other adaptive approaches with their best settings obtained through systematic search, and effectively removes the need for learning rate tuning.", "date": "2013", "authors": ["Tom Schaul , Sixin Zhang , Yann LeCun"], "references": [2146502635, 3118608800, 1533861849, 2113651538, 2914484425, 2156779765, 2137515395, 1568229137, 1598497354, 2130984546]}, {"id": 19621276, "title": "Improving the convergence of back-propagation learning with second-order methods", "abstract": "", "date": "1988", "authors": ["S. Becker , Yann Lecun"], "references": [2160699933, 1526055535]}, {"id": 1994616650, "title": "A Stochastic Approximation Method", "abstract": "Let M(x) denote the expected value at level x of the response to a certain experiment. M(x) is assumed to be a monotone function of x but is unknown to the experimenter, and it is desired to find the solution x = \u03b8 of the equation M(x) = \u03b1, where a is a given constant. We give a method for making successive experiments at levels x1, x2, \u00b7\u00b7\u00b7 in such a way that xn will tend to \u03b8 in probability.", "date": "1951", "authors": ["Herbert Robbins , Sutton Monro"], "references": []}, {"id": 1889268436, "title": "Semantic Compositionality through Recursive Matrix-Vector Spaces", "abstract": "Single-word vector space models have been very successful at learning lexical information. However, they cannot capture the compositional meaning of longer phrases, preventing them from a deeper understanding of language. We introduce a recursive neural network (RNN) model that learns compositional vector representations for phrases and sentences of arbitrary syntactic type and length. Our model assigns a vector and a matrix to every node in a parse tree: the vector captures the inherent meaning of the constituent, while the matrix captures how it changes the meaning of neighboring words or phrases. This matrix-vector RNN can learn the meaning of operators in propositional logic and natural language. The model obtains state of the art performance on three different experiments: predicting fine-grained sentiment distributions of adverb-adjective pairs; classifying sentiment labels of movie reviews and classifying semantic relationships such as cause-effect or topic-message between nouns using the syntactic path between them.", "date": "2012", "authors": ["Richard Socher , Brody Huval , Christopher D. Manning , Andrew Y. Ng"], "references": [2251939518, 2117130368, 1423339008, 71795751, 1662133657, 2097606805, 2103305545, 2163455955, 1984052055, 2151048449]}, {"id": 2006969979, "title": "The mathematics of statistical machine translation: parameter estimation", "abstract": "We describe a series of five statistical models of the translation process and give algorithms for estimating the parameters of these models given a set of pairs of sentences that are translations of one another. We define a concept of word-by-word alignment between such pairs of sentences. For any given pair of such sentences each of our models assigns a probability to each of the possible word-by-word alignments. We give an algorithm for seeking the most probable of these alignments. Although the algorithm is suboptimal, the alignment thus obtained accounts well for the word-by-word relationships in the pair of sentences. We have a great deal of data in French and English from the proceedings of the Canadian Parliament. Accordingly, we have restricted our work to these two languages; but we feel that because our algorithms have minimal linguistic content they would work well on other pairs of languages. We also feel, again because of the minimal linguistic content of our algorithms, that it is reasonable to argue that word-by-word alignments are inherent in any sufficiently large bilingual corpus.", "date": "1993", "authors": ["Peter F. Brown , Vincent J. Della Pietra , Stephen A. Della Pietra , Robert L. Mercer"], "references": [2049633694, 2121227244, 2097333193, 1489181569, 2117652747, 2129139611, 2154384676, 2138584836, 1575431606, 2048390999]}, {"id": 2171928131, "title": "Extensions of recurrent neural network language model", "abstract": "We present several modifications of the original recurrent neural network language model (RNN LM).While this model has been shown to significantly outperform many competitive language modeling techniques in terms of accuracy, the remaining problem is the computational complexity. In this work, we show approaches that lead to more than 15 times speedup for both training and testing phases. Next, we show importance of using a backpropagation through time algorithm. An empirical comparison with feedforward networks is also provided. In the end, we discuss possibilities how to reduce the amount of parameters in the model. The resulting RNN model can thus be smaller, faster both during training and testing, and more accurate than the basic one.", "date": "2011", "authors": ["Tomas Mikolov 1, Stefan Kombrink 1, Lukas Burget 1, Jan Cernocky 1, Sanjeev Khudanpur 2"], "references": [179875071, 2132339004, 1498436455, 2110485445, 2107878631, 2613634265, 36903255, 2096072088, 2111305191, 2152808281]}, {"id": 2103305545, "title": "Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection", "abstract": "Paraphrase detection is the task of examining two sentences and determining whether they have the same meaning. In order to obtain high accuracy on this task, thorough syntactic and semantic analysis of the two statements is needed. We introduce a method for paraphrase detection based on recursive autoencoders (RAE). Our unsupervised RAEs are based on a novel unfolding objective and learn feature vectors for phrases in syntactic trees. These features are used to measure the word- and phrase-wise similarity between two sentences. Since sentences may be of arbitrary length, the resulting matrix of similarity measures is of variable size. We introduce a novel dynamic pooling layer which computes a fixed-sized representation from the variable-sized matrices. The pooled representation is then used as input to a classifier. Our method outperforms other state-of-the-art approaches on the challenging MSRP paraphrase corpus.", "date": "2011", "authors": ["Richard Socher , Eric H. Huang , Jeffrey Pennin , Christopher D Manning , Andrew Y. Ng"], "references": [2117130368, 2132339004, 2158139315, 1423339008, 71795751, 2097606805, 2296073425, 2140833774, 1566018662, 2095739681]}, {"id": 2251222643, "title": "Continuous Space Translation Models for Phrase-Based Statistical Machine Translation", "abstract": "This paper presents a new approach to perform the estimation of the translation model probabilities of a phrase-based statistical machine translation system. We use neural networks to directly learn the translation probability of phrase pairs using continuous representations. The system can be easily trained on the same data used to build standard phrase-based systems. We provide experimental evidence that the approach seems to be able to infer meaningful translation probabilities for phrase pairs not seen in the training data, or even predict a list of the most likely translations given a source phrase. The approach can be used to rescore n-best lists, but we also discuss an integration into the Moses decoder. A preliminary evaluation on the English/French IWSLT task achieved improvements in the BLEU score and a human analysis showed that the new model often chooses semantically better translations. Several extensions of this work are discussed.", "date": "2012", "authors": ["Holger Schwenk"], "references": [2132339004, 2156985047, 2146574666, 1970689298, 2109664771, 2251098065, 2143719855, 2140679639, 2250379827, 2103078213]}, {"id": 196214544, "title": "Generating Text with Recurrent Neural Networks", "abstract": "Recurrent Neural Networks (RNNs) are very powerful sequence models that do not enjoy widespread use because it is extremely difficult to train them properly. Fortunately, recent advances in Hessian-free optimization have been able to overcome the difficulties associated with training RNNs, making it possible to apply them successfully to challenging sequence problems. In this paper we demonstrate the power of RNNs trained with the new Hessian-Free optimizer (HF) by applying them to character-level language modeling tasks. The standard RNN architecture, while effective, is not ideally suited for such tasks, so we introduce a new RNN variant that uses multiplicative (or \"gated\") connections which allow the current input character to determine the transition matrix from one hidden state vector to the next. After training the multiplicative RNN with the HF optimizer for five days on 8 high-end Graphics Processing Units, we were able to surpass the performance of the best previous single method for character-level language modeling \u2013 a hierarchical non-parametric sequence model. To our knowledge this represents the largest recurrent neural network application to date.", "date": "2011", "authors": ["Ilya Sutskever , James Martens , Geoffrey E. Hinton"], "references": [2064675550, 179875071, 1498436455, 196761320, 2131462252, 2118706537, 2110575115, 2107878631, 1408639475, 2170942820]}, {"id": 1554663460, "title": "Neural networks for pattern recognition", "abstract": "From the Publisher: This is the first comprehensive treatment of feed-forward neural networks from the perspective of statistical pattern recognition. After introducing the basic concepts, the book examines techniques for modelling probability density functions and the properties and merits of the multi-layer perceptron and radial basis function network models. Also covered are various forms of error functions, principal algorithms for error function minimalization, learning and generalization in neural networks, and Bayesian techniques and their applications. Designed as a text, with over 100 exercises, this fully up-to-date work will benefit anyone involved in the fields of neural computation and pattern recognition.", "date": "1994", "authors": ["Christopher M. Bishop"], "references": [2194775991, 2140190241, 1746819321, 1570448133, 2139212933, 1964357740, 2117812871, 1810943226, 2097998348]}, {"id": 2143612262, "title": "Speech recognition with deep recurrent neural networks", "abstract": "Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates deep recurrent neural networks, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score.", "date": "2013", "authors": ["Alex Graves , Abdel-rahman Mohamed , Geoffrey Hinton"], "references": [2064675550, 2160815625, 1993882792, 2184045248, 2127141656, 2144499799, 2108677974, 3023071679, 2155273149, 1828163288]}, {"id": 44815768, "title": "A Practical Guide to Training Restricted Boltzmann Machines", "abstract": "Restricted Boltzmann machines (RBMs) have been used as generative models of many different types of data. RBMs are usually trained using the contrastive divergence learning procedure. This requires a certain amount of practical experience to decide how to set the values of numerical meta-parameters. Over the last few years, the machine learning group at the University of Toronto has acquired considerable expertise at training RBMs and this guide is an attempt to share this expertise with other machine learning researchers.", "date": "2011", "authors": ["Geoffrey E. Hinton"], "references": [2136922672, 1665214252, 2116064496, 2099866409, 2096192494, 2293063825, 2029949252, 2116825644, 2158164339, 2124914669]}, {"id": 1632114991, "title": "Building a large annotated corpus of English: the penn treebank", "abstract": "Abstract : As a result of this grant, the researchers have now published oil CDROM a corpus of over 4 million words of running text annotated with part-of- speech (POS) tags, with over 3 million words of that material assigned skeletal grammatical structure. This material now includes a fully hand-parsed version of the classic Brown corpus. About one half of the papers at the ACL Workshop on Using Large Text Corpora this past summer were based on the materials generated by this grant.", "date": "1993", "authors": ["Mitchell P. Marcus 1, Mary Ann Marcinkiewicz 1, Beatrice Santorini 2"], "references": [2099247782, 1483126227, 2439178139, 2334801970, 900993354, 2110190189, 2012837062, 2121407024, 2076526090, 2034693287]}, {"id": 2108677974, "title": "Practical Variational Inference for Neural Networks", "abstract": "Variational methods have been previously explored as a tractable approximation to Bayesian inference for neural networks. However the approaches proposed so far have only been applicable to a few simple network architectures. This paper introduces an easy-to-implement stochastic variational method (or equivalently, minimum description length loss function) that can be applied to most neural networks. Along the way it revisits several common regularisers from a variational perspective. It also provides a simple pruning heuristic that can both drastically reduce the number of network weights and lead to improved generalisation. Experimental results are provided for a hierarchical multidimensional recurrent neural network applied to the TIMIT speech corpus.", "date": "2011", "authors": ["Alex Graves"], "references": [2064675550, 1498436455, 2127141656, 2129652681, 2143908786, 2170942820, 2103359087, 2114766824, 2150218618, 2054658115]}, {"id": 2120861206, "title": "A fast and simple algorithm for training neural probabilistic language models", "abstract": "In spite of their superior performance, neural probabilistic language models (NPLMs) remain far less widely used than n-gram models due to their notoriously long training times, which are measured in weeks even for moderately-sized datasets. Training NPLMs is computationally expensive because they are explicitly normalized, which leads to having to consider all words in the vocabulary when computing the log-likelihood gradients. We propose a fast and simple algorithm for training NPLMs based on noise-contrastive estimation, a newly introduced procedure for estimating unnormalized continuous distributions. We investigate the behaviour of the algorithm on the Penn Treebank corpus and show that it reduces the training times by more than an order of magnitude without affecting the quality of the resulting models. The algorithm is also more efficient and much more stable than importance sampling because it requires far fewer noise samples to perform well. We demonstrate the scalability of the proposed approach by training several neural language models on a 47M-word corpus with a 80K-word vocabulary, obtaining state-of-the-art results on the Microsoft Research Sentence Completion Challenge dataset.", "date": "2012", "authors": ["Andriy Mnih , Yee W. Teh"], "references": [2117130368, 179875071, 2132339004, 2158139315, 1423339008, 1631260214, 1521626219, 2131462252, 2138204974, 2096175520]}, {"id": 3023071679, "title": "Framewise phoneme classification with bidirectional LSTM and other neural network architectures", "abstract": "In this paper, we present bidirectional Long Short Term Memory (LSTM) networks, and a modified, full gradient version of the LSTM learning algorithm. We evaluate Bidirectional LSTM (BLSTM) and several other network architectures on the benchmark task of framewise phoneme classification, using the TIMIT database. Our main findings are that bidirectional networks outperform unidirectional ones, and Long Short Term Memory (LSTM) is much faster and also more accurate than both standard Recurrent Neural Nets (RNNs) and time-windowed Multilayer Perceptrons (MLPs). Our results support the view that contextual information is crucial to speech processing, and suggest that BLSTM is an effective architecture with which to exploit it'.", "date": "2004", "authors": ["Alex Graves , J\u00fcrgen Schmidhuber"], "references": [2076063813, 2143612262, 2963674932, 1810943226, 1689711448, 2963012544, 2102113734, 2285660444, 2293634267]}, {"id": 2130942839, "title": "Sequence to Sequence Learning with Neural Networks", "abstract": "Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.", "date": "2014", "authors": ["Ilya Sutskever , Oriol Vinyals , Quoc V. Le"], "references": [2618530766, 2964308564, 2157331557, 2310919327, 2064675550, 2101105183, 179875071, 2147768505, 2132339004, 1753482797]}, {"id": 2395935897, "title": "Audio Chord Recognition with Recurrent Neural Networks.", "abstract": "In this paper, we present an audio chord recognition system based on a recurrent neural network. The audio features are obtained from a deep neural network optimized with a combination of chromagram targets and chord information, and aggregated over different time scales. Contrarily to other existing approaches, our system incorporates acoustic and musicological models under a single training objective. We devise an efficient algorithm to search for the global mode of the output distribution while taking long-term dependencies into account. The resulting method is competitive with state-of-the-art approaches on the MIREX dataset in the major/minor prediction task.", "date": "2012", "authors": ["Nicolas Boulanger-Lewandowski , Yoshua Bengio , Pascal Vincent"], "references": [2136922672, 2072128103, 2147768505, 2154642048, 2184045248, 2107878631, 2962968839, 1828163288, 2108563286, 2162911105]}, {"id": 1905522558, "title": "Domain Adaptation via Pseudo In-Domain Data Selection", "abstract": "We explore efficient domain adaptation for the task of statistical machine translation based on extracting sentences from a large general-domain parallel corpus that are most relevant to the target domain. These sentences may be selected with simple cross-entropy based methods, of which we present three. As these sentences are not themselves identical to the in-domain data, we call them pseudo in-domain subcorpora. These subcorpora -- 1% the size of the original -- can then used to train small domain-adapted Statistical Machine Translation (SMT) systems which outperform systems trained on the entire corpus. Performance is further improved when we use these domain-adapted models in combination with a true in-domain model. The results show that more training data is not always better, and that best results are attained via proper domain-relevant data selection, as well as combining in- and general-domain systems during decoding.", "date": "2011", "authors": ["Amittai Axelrod 1, Xiaodong He 2, Jianfeng Gao 2"], "references": [2124807415, 2156985047, 1631260214, 2146574666, 2158195707, 2117278770, 2137387514, 2132001515, 2130450156, 2148861208]}, {"id": 1828163288, "title": "Sequence Transduction with Recurrent Neural Networks", "abstract": "Many machine learning tasks can be expressed as the transformation---or \\emph{transduction}---of input sequences into output sequences: speech recognition, machine translation, protein secondary structure prediction and text-to-speech to name but a few. One of the key challenges in sequence transduction is learning to represent both the input and output sequences in a way that is invariant to sequential distortions such as shrinking, stretching and translating. Recurrent neural networks (RNNs) are a powerful sequence learning architecture that has proven capable of learning such representations. However RNNs traditionally require a pre-defined alignment between the input and output sequences to perform transduction. This is a severe limitation since \\emph{finding} the alignment is the most difficult aspect of many sequence transduction problems. Indeed, even determining the length of the output sequence is often challenging. This paper introduces an end-to-end, probabilistic sequence transduction system, based entirely on RNNs, that is in principle able to transform any input sequence into any finite, discrete output sequence. Experimental results for phoneme recognition are provided on the TIMIT speech corpus.", "date": "2012", "authors": ["Alex Graves"], "references": [2310919327, 2064675550, 2147880316, 179875071, 2127141656, 196214544, 3023071679, 2131774270, 2170942820, 2103359087]}, {"id": 2341457423, "title": "BLEU Deconstructed: Designing a Better MT Evaluation Metric", "abstract": "BLEU is the de facto standard automatic evaluation metric in machine translation. While BLEU is undeniably useful, it has a number of limitations. Although it works well for large documents and multiple references, it is unreliable at the sentence or sub-sentence levels, and with a single reference. In this paper, we propose new variants of BLEU which address these limitations, resulting in a more flexible metric which is not only more reliable, but also allows for more accurate discriminative training. Our best metric has better correlation with human judgements than standard BLEU, despite using a simpler formulation. Moreover, these improvements carry over to a system tuned for our new metric.", "date": "2012", "authors": ["Xingyi Song , Trevor Cohn , Lucia Specia"], "references": [2101105183, 2124807415, 2146574666, 2123301721, 2078861931, 2087735403, 222053410, 1489525520, 2115081467, 2169279899]}, {"id": 2101105183, "title": "Bleu: a Method for Automatic Evaluation of Machine Translation", "abstract": "Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.", "date": "2002", "authors": ["Kishore Papineni , Salim Roukos , Todd Ward , Wei-Jing Zhu"], "references": [2001810881, 3037252522, 2732923061]}, {"id": 1508165687, "title": "Statistical methods for speech recognition", "abstract": "The speech recognition problem hidden Markov models the acoustic model basic language modelling the Viterbi search hypothesis search on a tree and the fast match elements of information theory the complexity of tasks - the quality of language models the expectation - maximization algorithm and its consequences decision trees and tree language models phonetics from orthography - spelling-to-base from mappings triphones and allophones maximum entropy probability estimation and language models three applications of maximum entropy estimation to language modelling estimation of probabilities from counts and the Back-Off method.", "date": "1996", "authors": ["Frederick Jelinek"], "references": [1560013842, 1980862600]}, {"id": 1973923101, "title": "Improved statistical alignment models", "abstract": "In this paper, we present and compare various single-word based alignment models for statistical machine translation. We discuss the five IBM alignment models, the Hidden-Markov alignment model, smoothing techniques and various modifications. We present different methods to combine alignments. As evaluation criterion we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment. We show that models with a first-order dependence and a fertility model lead to significantly better results than the simple models IBM-1 or IBM-2, which are not able to go beyond zero-order dependencies.", "date": "2000", "authors": ["Franz Josef Och , Hermann Ney"], "references": [2006969979, 2038698865, 1979102019, 1575431606, 3104029765, 1811404221, 2030750105, 1525706028, 136130055]}, {"id": 1986543644, "title": "Three Generative, Lexicalised Models for Statistical Parsing", "abstract": "In this paper we first propose a new statistical parsing model, which is a generative model of lexicalised context-free grammar. We then extend the model to include a probabilistic treatment of both subcategorisation and wh-movement. Results on Wall Street Journal text show that the parser performs at 88.1/87.5% constituent precision/recall, an average improvement of 2.3% over (Collins 96).", "date": "1997", "authors": ["Michael Collins"], "references": [1632114991, 1773803948, 2110882317, 2153439141, 2052449326, 2093647425, 1972573551, 2087165009, 2069912724, 2162455891]}, {"id": 2116316001, "title": "A Syntax-based Statistical Translation Model", "abstract": "We present a syntax-based statistical translation model. Our model transforms a source-language parse tree into a target-language string by applying stochastic operations at each node. These operations capture linguistic differences such as word order and case marking. Model parameters are estimated in polynomial time using an EM algorithm. The model produces word alignments that are better than those produced by IBM Model 5.", "date": "2001", "authors": ["Kenji Yamada , Kevin Knight"], "references": [2101105183, 2122410182, 1574901103, 2049633694, 2006969979, 2092654472, 2135843243, 2117400858, 1973923101, 2101210369]}, {"id": 1517947178, "title": "Improved Alignment Models for Statistical Machine Translation", "abstract": "", "date": "1998", "authors": ["Franz Josef Och , Christoph Tillmann , Hermann Ney"], "references": [2006969979, 2038698865, 2107551411, 2113106066, 2196555355, 2294072136, 2158164089]}, {"id": 2161792612, "title": "A Phrase-Based,Joint Probability Model for Statistical Machine Translation", "abstract": "We present a joint probability model for statistical machine translation, which automatically learns word and phrase equivalents from bilingual corpora. Translations produced with parameters estimated using the joint model are more accurate than translations produced using IBM Model 4.", "date": "2002", "authors": ["Daniel Marcu 1, Daniel Wong 2"], "references": [2049633694, 2006969979, 1916559533, 2001810881, 2116316001, 1517947178, 2129765547, 1549285799, 2139403546, 133045130]}, {"id": 1549285799, "title": "Statistical Language Modeling using the CMU-Cambridge Toolkit", "abstract": "", "date": "1996", "authors": ["Philip Clarkson , Ronald Rosenfeld"], "references": [2153653739, 1631260214, 1916559533, 2143017621, 2142069714, 2158195707, 2134800885, 2159981908, 2116316001, 2161792612]}, {"id": 2158388102, "title": "Stochastic inversion transduction grammars and bilingual parsing of parallel corpora", "abstract": "We introduce (1) a novel stochastic inversion transduction grammar formalism for bilingual language modeling of sentence-pairs, and (2) the concept of bilingual parsing with a variety of parallel corpus analysis applications. Aside from the bilingual orientation, three major features distinguish the formalism from the finite-state transducers more traditionally found in computational linguistics: it skips directly to a context-free rather than finite-state base, it permits a minimal extra degree of ordering flexibility, and its probabilistic formulation admits an efficient maximum-likelihood bilingual parsing algorithm. A convenient normal form is shown to exist. Analysis of the formalism's expressiveness suggests that it is particularly well suited to modeling ordering shifts between languages, balancing needed flexibility against complexity constraints. We discuss a number of examples of how stochastic inversion transduction grammars bring bilingual constraints to bear upon problematic corpus analysis tasks such as segmentation, bracketing, phrasal alignment, and parsing.", "date": "1997", "authors": ["Dekai Wu"], "references": [2006969979, 2097333193, 1489181569, 2117652747, 1513168562, 2439178139, 1991133427, 2138584836, 201288405, 2110190189]}, {"id": 1606347560, "title": "Theano: new features and speed improvements", "abstract": "Theano is a linear algebra compiler that optimizes a user's symbolically-specified mathematical computations to produce efficient low-level implementations. In this paper, we present new features and efficiency improvements to Theano, and benchmarks demonstrating Theano's performance relative to Torch7, a recently introduced machine learning library, and to RNNLM, a C++ library targeted at recurrent neural networks.", "date": "2012", "authors": ["Fr\u00e9d\u00e9ric Bastien , Pascal Lamblin , Razvan Pascanu , James Bergstra , Ian J. Goodfellow , Arnaud Bergeron , Nicolas Bouchard , David Warde-Farley , Yoshua Bengio"], "references": [2011301426, 2154642048, 753012316, 2061939373, 3005347330, 2110114082, 1408639475, 2185726469, 2254715784, 2006903949]}, {"id": 2171865010, "title": "Survey: Reservoir computing approaches to recurrent neural network training", "abstract": "Echo State Networks and Liquid State Machines introduced a new paradigm in artificial recurrent neural network (RNN) training, where an RNN (the reservoir) is generated randomly and only a readout is trained. The paradigm, becoming known as reservoir computing, greatly facilitated the practical application of RNNs and outperformed classical fully trained RNNs in many tasks. It has lately become a vivid research field with numerous extensions of the basic idea, including reservoir adaptation, thus broadening the initial paradigm to using different methods for training the reservoir and the readout. This review systematically surveys both current ways of generating/adapting the reservoirs and training different types of readouts. It offers a natural conceptual classification of the techniques, which transcends boundaries of the current ''brand-names'' of reservoir methods, and thus aims to help in unifying the field and providing the reader with a detailed ''map'' of it.", "date": "2009", "authors": ["Mantas Luko\u0161evi\u010dius , Herbert Jaeger"], "references": [2112090702, 2008620264, 2100495367, 1497256448, 2064675550, 2154642048, 3110653090, 2293063825, 1659842140, 2118706537]}, {"id": 2118706537, "title": "Harnessing Nonlinearity: Predicting Chaotic Systems and Saving Energy in Wireless Communication", "abstract": "We present a method for learning nonlinear systems, echo state networks (ESNs). ESNs employ artificial recurrent neural networks in a way that has recently been proposed independently as a learning mechanism in biological brains. The learning method is computationally efficient and easy to use. On a benchmark task of predicting a chaotic time series, accuracy is improved by a factor of 2400 over previous techniques. The potential for engineering applications is illustrated by equalizing a communication channel, where the signal error rate is improved by two orders of magnitude.", "date": "2004", "authors": ["Herbert Jaeger , Harald Haas"], "references": [2103179919, 2016589492, 1543237449, 2166322089, 2134514463, 2094631910, 2058580716, 2143879519, 2045182040, 1943433854]}, {"id": 2122585011, "title": "A Novel Connectionist System for Unconstrained Handwriting Recognition", "abstract": "Recognizing lines of unconstrained handwritten text is a challenging task. The difficulty of segmenting cursive or overlapping characters, combined with the need to exploit surrounding context, has led to low recognition rates for even the best current recognizers. Most recent progress in the field has been made either through improved preprocessing or through advances in language modeling. Relatively little work has been done on the basic recognition algorithms. Indeed, most systems rely on the same hidden Markov models that have been used for decades in speech and handwriting recognition, despite their well-known shortcomings. This paper proposes an alternative approach based on a novel type of recurrent neural network, specifically designed for sequence labeling tasks where the data is hard to segment and contains long-range bidirectional interdependencies. In experiments on two large unconstrained handwriting databases, our approach achieves word recognition accuracies of 79.7 percent on online data and 74.1 percent on offline data, significantly outperforming a state-of-the-art HMM-based system. In addition, we demonstrate the network's robustness to lexicon size, measure the individual influence of its hidden layers, and analyze its use of context. Last, we provide an in-depth discussion of the differences between the network and HMMs, suggesting reasons for the network's superior performance.", "date": "2009", "authors": ["A. Graves 1, M. Liwicki 2, S. Fernandez 3, R. Bertolami 4, H. Bunke 4, J. Schmidhuber 1"], "references": [2064675550, 2125838338, 2127141656, 3023071679, 2107878631, 2142069714, 2131774270, 1578856370, 2079735306, 2147568880]}, {"id": 2096733369, "title": "FaceNet: A unified embedding for face recognition and clustering", "abstract": "Despite significant recent advances in the field of face recognition [10, 14, 15, 17], implementing face verification and recognition efficiently at scale presents serious challenges to current approaches. In this paper we present a system, called FaceNet, that directly learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure of face similarity. Once this space has been produced, tasks such as face recognition, verification and clustering can be easily implemented using standard techniques with FaceNet embeddings as feature vectors.", "date": "2015", "authors": ["Florian Schroff , Dmitry Kalenichenko , James Philbin"], "references": [2097117768, 1849277567, 2146502635, 2963911037, 2168231600, 2145287260, 2294059674, 1782590233, 1498436455, 2296073425]}, {"id": 2016053056, "title": "Large-Scale Video Classification with Convolutional Neural Networks", "abstract": "Convolutional Neural Networks (CNNs) have been established as a powerful class of models for image recognition problems. Encouraged by these results, we provide an extensive empirical evaluation of CNNs on large-scale video classification using a new dataset of 1 million YouTube videos belonging to 487 classes. We study multiple approaches for extending the connectivity of a CNN in time domain to take advantage of local spatio-temporal information and suggest a multiresolution, foveated architecture as a promising way of speeding up the training. Our best spatio-temporal networks display significant performance improvements compared to strong feature-based baselines (55.3% to 63.9%), but only a surprisingly modest improvement compared to single-frame models (59.3% to 60.9%). We further study the generalization performance of our best model by retraining the top layers on the UCF-101 Action Recognition dataset and observe significant performance improvements compared to the UCF-101 baseline model (63.3% up from 43.9%).", "date": "2014", "authors": ["Andrej Karpathy , George Toderici , Sanketh Shetty , Thomas Leung , Rahul Sukthankar , Li Fei-Fei"], "references": [2618530766, 2102605133, 2161969291, 2108598243, 2963542991, 2310919327, 2168231600, 2022508996, 2131846894, 2062118960]}, {"id": 2141599568, "title": "Linguistic Regularities in Continuous Space Word Representations", "abstract": "Continuous space language models have recently demonstrated outstanding results across a variety of tasks. In this paper, we examine the vector-space word representations that are implicitly learned by the input-layer weights. We find that these representations are surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relation-specific vector offset. This allows vector-oriented reasoning based on the offsets between words. For example, the male/female relationship is automatically learned, and with the induced vector representations, \u201cKing Man + Woman\u201d results in a vector very close to \u201cQueen.\u201d We demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions (provided with this paper), and are able to correctly answer almost 40% of the questions. We demonstrate that the word vectors capture semantic regularities by using the vector offset method to answer SemEval-2012 Task 2 questions. Remarkably, this method outperforms the best previous systems.", "date": "2013", "authors": ["Tomas Mikolov 1, Wen-tau Yih 2, Geoffrey Zweig 2"], "references": [1614298861, 2100495367, 2117130368, 179875071, 2132339004, 2158139315, 2147152072, 1632114991, 2131462252, 1970689298]}, {"id": 2158139315, "title": "Word Representations: A Simple and General Method for Semi-Supervised Learning", "abstract": "If we take an existing supervised NLP system, a simple and general way to improve accuracy is to use unsupervised word representations as extra word features. We evaluate Brown clusters, Collobert and Weston (2008) embeddings, and HLBL (Mnih & Hinton, 2009) embeddings of words on both NER and chunking. We use near state-of-the-art supervised baselines, and find that each of the three word representations improves the accuracy of these baselines. We find further improvements by combining different word representations. You can download our word features, for off-the-shelf use in existing NLP systems, as well as our code, here: http://metaoptimize.com/projects/wordreprs/", "date": "2010", "authors": ["Joseph Turian 1, Lev-Arie Ratinov 2, Yoshua Bengio 1"], "references": [1880262756, 2117130368, 2132339004, 1662133657, 2131462252, 2296073425, 168564468, 2158997610, 2156515921, 2004763266]}, {"id": 1423339008, "title": "Parsing Natural Scenes and Natural Language with Recursive Neural Networks", "abstract": "Recursive structure is commonly found in the inputs of different modalities such as natural scene images or natural language sentences. Discovering this recursive structure helps us to not only identify the units that an image or sentence contains but also how they interact to form a whole. We introduce a max-margin structure prediction architecture based on recursive neural networks that can successfully recover such structure both in complex scene images as well as sentences. The same algorithm can be used both to provide a competitive syntactic parser for natural language sentences from the Penn Treebank and to outperform alternative approaches for semantic scene segmentation, annotation and classification. For segmentation and annotation our algorithm obtains a new level of state-of-the-art performance on the Stanford background dataset (78.1%). The features from the image parse tree outperform Gist descriptors for scene classification by 4%.", "date": "2011", "authors": ["Richard Socher , Cliff C. Lin , Chris Manning , Andrew Y. Ng"], "references": [2100495367, 2162915993, 2067191022, 2117130368, 1574901103, 2132339004, 2130325614, 1566135517, 1528789833, 2536208356]}, {"id": 1662133657, "title": "From frequency to meaning: vector space models of semantics", "abstract": "Computers understand very little of the meaning of human language. This profoundly limits our ability to give instructions to computers, the ability of computers to explain their actions to us, and the ability of computers to analyse and process text. Vector space models (VSMs) of semantics are beginning to address these limits. This paper surveys the use of VSMs for semantic processing of text. We organize the literature on VSMs according to the structure of the matrix in a VSM. There are currently three broad classes of VSMs, based on term-document, word-context, and pair-pattern matrices, yielding three classes of applications. We survey a broad range of applications in these three categories and we take a detailed look at a specific open source project in each category. Our goal in this survey is to show the breadth of applications of VSMs for semantics, to provide a new perspective on VSMs for those who are already familiar with the area, and to provide pointers into the literature for those who are less familiar with the field.", "date": "2009", "authors": ["Peter D. Turney 1, Patrick Pantel 2"], "references": [2173213060, 1880262756, 3013264884, 1532325895, 2038721957, 1660390307, 2024165284, 2117130368, 2166706824, 1992419399]}, {"id": 2129131372, "title": "Decoding by linear programming", "abstract": "This paper considers a natural error correcting problem with real valued input/output. We wish to recover an input vector f/spl isin/R/sup n/ from corrupted measurements y=Af+e. Here, A is an m by n (coding) matrix and e is an arbitrary and unknown vector of errors. Is it possible to recover f exactly from the data y? We prove that under suitable conditions on the coding matrix A, the input f is the unique solution to the /spl lscr//sub 1/-minimization problem (/spl par/x/spl par//sub /spl lscr/1/:=/spl Sigma//sub i/|x/sub i/|) min(g/spl isin/R/sup n/) /spl par/y - Ag/spl par//sub /spl lscr/1/ provided that the support of the vector of errors is not too large, /spl par/e/spl par//sub /spl lscr/0/:=|{i:e/sub i/ /spl ne/ 0}|/spl les//spl rho//spl middot/m for some /spl rho/>0. In short, f can be recovered exactly by solving a simple convex optimization problem (which one can recast as a linear program). In addition, numerical experiments suggest that this recovery procedure works unreasonably well; f is recovered exactly even in situations where a significant fraction of the output is corrupted. This work is related to the problem of finding sparse solutions to vastly underdetermined systems of linear equations. There are also significant connections with the problem of recovering signals from highly incomplete measurements. In fact, the results introduced in this paper improve on our earlier work. Finally, underlying the success of /spl lscr//sub 1/ is a crucial property we call the uniform uncertainty principle that we shall describe in detail.", "date": "2005", "authors": ["E.J. Candes 1, T. Tao 2"], "references": [2296319761, 2296616510, 2145096794, 2129638195, 2078204800, 2116148865, 2099641086, 2050834445, 2154332973, 1573820523]}, {"id": 2097726431, "title": "Opinion Mining and Sentiment Analysis", "abstract": "An important part of our information-gathering behavior has always been to find out what other people think. With the growing availability and popularity of opinion-rich resources such as online review sites and personal blogs, new opportunities and challenges arise as people now can, and do, actively use information technologies to seek out and understand the opinions of others. The sudden eruption of activity in the area of opinion mining and sentiment analysis, which deals with the computational treatment of opinion, sentiment, and subjectivity in text, has thus occurred at least in part as a direct response to the surge of interest in new systems that deal directly with opinions as a first-class object. This survey covers techniques and approaches that promise to directly enable opinion-oriented information-seeking systems. Our focus is on methods that seek to address the new challenges raised by sentiment-aware applications, as compared to those that are already present in more traditional fact-based analysis. We include material on summarization of evaluative text and on broader issues regarding privacy, manipulation, and economic impact that the development of opinion-oriented information-access services gives rise to. To facilitate future work, a discussion of available resources, benchmark datasets, and evaluation campaigns is also provided.", "date": "2008", "authors": ["Bo Pang 1, Lillian Lee 2"], "references": [1880262756, 3013264884, 2147880316, 2038721957, 2138621811, 2166706824, 2118020653, 2160660844, 2155328222, 2114524997]}, {"id": 1988790447, "title": "A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting", "abstract": "In the first part of the paper we consider the problem of dynamically apportioning resources among a set of options in a worst-case on-line framework. The model we study can be interpreted as a broad, abstract extension of the well-studied on-line prediction model to a general decision-theoretic setting. We show that the multiplicative weight-update Littlestone?Warmuth rule can be adapted to this model, yielding bounds that are slightly weaker in some cases, but applicable to a considerably more general class of learning problems. We show how the resulting learning algorithm can be applied to a variety of problems, including gambling, multiple-outcome prediction, repeated games, and prediction of points in Rn. In the second part of the paper we apply the multiplicative weight-update technique to derive a new boosting algorithm. This boosting algorithm does not require any prior knowledge about the performance of the weak learning algorithm. We also study generalizations of the new boosting algorithm to the problem of learning functions whose range, rather than being binary, is an arbitrary finite set or a bounded segment of the real line.", "date": "1997", "authors": ["Yoav Freund , Robert E Schapire"], "references": [2112076978, 1966280301, 1676820704, 1530699444, 2165758113, 2093717447, 2093825590, 2070534370, 1520252399, 2104364170]}, {"id": 1574901103, "title": "Foundations of Statistical Natural Language Processing", "abstract": "Statistical approaches to processing natural language text have become dominant in recent years. This foundational text is the first comprehensive introduction to statistical natural language processing (NLP) to appear. The book contains all the theory and algorithms needed for building NLP tools. It provides broad but rigorous coverage of mathematical and linguistic foundations, as well as detailed discussion of statistical methods, allowing students and researchers to construct their own implementations. The book covers collocation finding, word sense disambiguation, probabilistic parsing, information retrieval, and other applications.", "date": "1999", "authors": ["Christopher D. Manning 1, Hinrich Sch\u00fctze 2"], "references": [182831726, 1508165687, 1994851566, 1549026077, 2108321481, 2949237929, 1795234945, 1736036918, 1746620543]}, {"id": 2009570821, "title": "Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids", "abstract": "Probablistic models are becoming increasingly important in analyzing the huge amount of data being produced by large-scale DNA-sequencing efforts such as the Human Genome Project. For example, hidden Markov models are used for analyzing biological sequences, linguistic-grammar-based probabilistic models for identifying RNA secondary structure, and probabilistic evolutionary models for inferring phylogenies of sequences from different organisms. This book gives a unified, up-to-date and self-contained account, with a Bayesian slant, of such methods, and more generally to probabilistic methods of sequence analysis. Written by an interdisciplinary team of authors, it is accessible to molecular biologists, computer scientists, and mathematicians with no formal knowledge of the other fields, and at the same time presents the state of the art in this new and important field.", "date": "1998", "authors": ["Richard Durbin 1, Sean R. Eddy 2, Anders Krogh 3, Graeme J. Mitchison"], "references": [2140190241, 2147880316, 2168133698, 2158266063, 2153233077, 2045843097, 2138122982, 2145191876, 2110575115, 2162315106]}, {"id": 1934019294, "title": "Maximum Entropy Markov Models for Information Extraction and Segmentation", "abstract": "Hidden Markov models (HMMs) are a powerful probabilistic tool for modeling sequential data, and have been applied with success to many text-related tasks, such as part-of-speech tagging, text segmentation and information extraction. In these cases, the observations are usually modeled as multinomial distributions over a discrete vocabulary, and the HMM parameters are set to maximize the likelihood of the observations. This paper presents a new Markovian sequence model, closely related to HMMs, that allows observations to be represented as arbitrary overlapping features (such as word, capitalization, formatting, part-of-speech), and defines the conditional probability of state sequences given observation sequences. It does this by using the maximum entropy framework to fit a set of exponential models that represent the probability of a state given an observation and the previous state. We present positive experimental results on the segmentation of FAQ\u2019s.", "date": "2000", "authors": ["Andrew McCallum , Dayne Freitag 1, Fernando C. N. Pereira 2"], "references": [2125838338, 2049633694, 2160842254, 2117400858, 1520377376, 3113258433, 1557074680, 2158873310, 1597379537, 2100796029]}, {"id": 1773803948, "title": "A Maximum Entropy Model for Part-Of-Speech Tagging", "abstract": "This paper presents a statistical model which trains from a corpus annotated with Part Of Speech tags and assigns them to previously unseen text with state of the art accuracy The model can be classi ed as a Maximum Entropy model and simultaneously uses many contextual features to predict the POS tag Furthermore this paper demonstrates the use of specialized fea tures to model di cult tagging decisions discusses the corpus consistency problems discovered during the implementation of these features and proposes a training strategy that mitigates these problems", "date": "1995", "authors": ["Adwait Ratnaparkhi"], "references": [1632114991, 2096175520, 2121227244, 2160842254, 2170381724, 2153439141, 1718065290, 2112861996, 2015042937, 2069912724]}, {"id": 2160842254, "title": "Inducing features of random fields", "abstract": "We present a technique for constructing random fields from a set of training samples. The learning paradigm builds increasingly complex fields by allowing potential functions, or features, that are supported by increasingly large subgraphs. Each feature has a weight that is trained by minimizing the Kullback-Leibler divergence between the model and the empirical distribution of the training data. A greedy algorithm determines how features are incrementally added to the field and an iterative scaling algorithm is used to estimate the optimal values of the weights. The random field models and techniques introduced in this paper differ from those common to much of the computer vision literature in that the underlying random fields are non-Markovian and have a large number of parameters that must be estimated. Relations to other learning approaches, including decision trees, are given. As a demonstration of the method, we describe its application to the problem of automatic word classification in natural language processing.", "date": "1997", "authors": ["S. Della Pietra 1, V. Della Pietra , J. Lafferty 2"], "references": [3085162807, 1997063559, 2049633694, 1594031697, 2096175520, 2121227244, 2097333193, 107938046, 2167837909, 2089969354]}, {"id": 2117400858, "title": "Transformation-based error-driven learning and natural language processing: a case study in part-of-speech tagging", "abstract": "Recently, there has been a rebirth of empiricism in the field of natural language processing. Manual encoding of linguistic information is being challenged by automated corpus-based learning as a method of providing a natural language processing system with linguistic knowledge. Although corpus-based approaches have been successful in many different areas of natural language processing, it is often the case that these methods capture the linguistic information they are modelling indirectly in large opaque tables of statistics. This can make it difficult to analyze, understand and improve the ability of these approaches to model underlying linguistic behavior. In this paper, we will describe a simple rule-based approach to automated learning of linguistic knowledge. This approach has been shown for a number of tasks to capture information in a clearer and more direct fashion without a compromise in performance. We present a detailed case study of this learning method applied to part-of-speech tagging.", "date": "1995", "authors": ["Eric Brill"], "references": [3085162807, 2149706766, 1632114991, 1594031697, 2102381086, 2099247782, 2097333193, 2081687495, 1489181569, 2170381724]}, {"id": 3021452258, "title": "Discriminative Reranking for Natural Language Parsing", "abstract": "", "date": "2000", "authors": ["Michael Collins"], "references": [2147880316, 607505555, 2105842272, 2158847908, 2160218441, 2092654472, 2107890099, 2168020168, 2168029744]}, {"id": 2159737176, "title": "Training Invariant Support Vector Machines", "abstract": "Practical experience has shown that in order to obtain the best possible performance, prior knowledge about invariances of a classification problem at hand ought to be incorporated into the training procedure. We describe and review all known methods for doing so in support vector machines, provide experimental results, and discuss their respective merits. One of the significant new results reported in this work is our recent achievement of the lowest reported test error on the well-known MNIST digit recognition benchmark task, with SVM training times that are also significantly faster than previous SVM methods.", "date": "2002", "authors": ["Dennis Decoste 1, Bernhard Sch\u00f6lkopf 2"], "references": [2156909104, 2148603752, 2310919327, 2119821739, 2140095548, 1512098439, 2087347434, 1604938182, 2147800946, 2151040995]}, {"id": 2027197837, "title": "Universal approximation of an unknown mapping and its derivatives using multilayer feedforward networks", "abstract": "Abstract We give conditions ensuring that multilayer feedforward networks with as few as a single hidden layer and an appropriately smooth hidden layer activation function are capable of arbitrarily accurate approximation to an arbitrary function and its derivatives. In fact, these networks can approximate functions that are not differentiable in the classical sense, but possess only a generalized derivative, as is the case for certain piecewise differentiable functions. The conditions imposed on the hidden layer activation function are relatively mild; the conditions imposed on the domain of the function to be approximated have practical implications. Our approximation results provide a previously missing theoretical justification for the use of multilayer feedforward networks in applications requiring simultaneous approximation of a function and its derivatives.", "date": "1990", "authors": ["Kurt Hornik , Maxwell Stinchcombe , Halbert White"], "references": [2137983211, 1971735090, 2056099894, 2416739038, 2090270852, 1613359937, 1490039160, 2090248140, 1537887709, 1571581645]}, {"id": 2068017609, "title": "Mitigating the paucity-of-data problem: exploring the effect of training corpus size on classifier performance for natural language processing", "abstract": "In this paper, we discuss experiments applying machine learning techniques to the task of confusion set disambiguation, using three orders of magnitude more training data than has previously been used for any disambiguation-in-string-context problem. In an attempt to determine when current learning methods will cease to benefit from additional training data, we analyze residual errors made by learners when issues of sparse data have been significantly mitigated. Finally, in the context of our results, we discuss possible directions for the empirical natural language research community.", "date": "2001", "authors": ["Michele Banko , Eric Brill"], "references": [2097089247, 1977182536, 2156202195, 2118996379, 1953828586, 2130851608, 1648417313, 1519443010, 2119966617, 1988842251]}, {"id": 2147345686, "title": "An offline cursive handwritten word recognition system", "abstract": "This paper describes an offline cursive handwritten word recognition system that combines hidden Markov models (HMM) and neural networks (NN). Using a fast left-right slicing method, we generate a segmentation graph that describes all possible ways to segment a word into letters. The NN computes the observation probabilities for each letter hypothesis in the segmentation graph. Then, the HMM compute the likelihood for each word in the lexicon by summing the probabilities over all possible paths through the graph. We present the preprocessing and the recognition process as well as the training procedure for the NN-HMM hybrid system. Another recognition system based on discrete HMM is also presented for performance comparison. The latter is also used for bootstrapping the NN-HMM hybrid system. Recognition performances of the two recognition systems using two image databases of French isolated words are presented. This paper is one of the first publications using the IRONOFF database, and thus can be used as a reference for future work on this database.", "date": "2001", "authors": ["Yong Haur Tay , P.M. Lallican , M. Khalid , C. Viard-Gaudin , S. Kneer"], "references": [2310919327, 2125838338, 2142069714, 183625566, 2149597185, 2077863651, 2148295954, 101240229, 2064838583, 1599953749]}, {"id": 51975515, "title": "An improved recognition module for the identification of handwritten digits", "abstract": "", "date": "1998", "authors": ["Anshu Sinha"], "references": [2124776405, 2227933188, 2126727781, 2256679588, 2122827492, 49742075, 2072181047, 2107600630, 1519534430, 2151513848]}, {"id": 2166469100, "title": "Effective Training of a Neural Network Character Classifier for Word Recognition", "abstract": "We have combined an artificial neural network (ANN) character classifier with context-driven search over character segmentation, word segmentation, and word recognition hypotheses to provide robust recognition of hand-printed English text in new models of Apple Computer's Newton Message Pad. We present some innovations in the training and use of ANNs as character classifiers for word recognition, including normalized output error, frequency balancing, error emphasis, negative training, and stroke warping. A recurring theme of reducing a priori biases emerges and is discussed.", "date": "1996", "authors": ["Larry S. Yaeger 1, Richard F. Lyon 1, Brandyn J. Webb 2"], "references": [2150884987, 2137291015, 2055075080, 1980501707, 2099070536, 2111494971, 2140539590, 2124229187, 2170541567, 2607313294]}, {"id": 2964274690, "title": "Joint Optimization Framework for Learning with Noisy Labels", "abstract": "Deep neural networks (DNNs) trained on large-scale datasets have exhibited significant performance in image classification. Many large-scale datasets are collected from websites, however they tend to contain inaccurate labels that are termed as noisy labels. Training on such noisy labeled datasets causes performance degradation because DNNs easily overfit to noisy labels. To overcome this problem, we propose a joint optimization framework of learning DNN parameters and estimating true labels. Our framework can correct labels during training by alternating update of network parameters and labels. We conduct experiments on the noisy CIFAR-10 datasets and the Clothing1M dataset. The results indicate that our approach significantly outperforms other state-of-the-art methods.", "date": "2018", "authors": ["Daiki Tanaka , Daiki Ikami , Toshihiko Yamasaki , Kiyoharu Aizawa"], "references": []}, {"id": 2966415767, "title": "Interpolation Consistency Training for Semi-supervised Learning", "abstract": "", "date": "2019", "authors": ["Vikas Verma 1, Alex Lamb 2, Juho Kannala 1, Yoshua Bengio 2, David Lopez-Paz 3"], "references": [3035160371, 2987875759, 3035687950, 3092206109, 3099306795, 2965556524, 3034672614, 2994710732]}, {"id": 2963855133, "title": "Bag of Tricks for Image Classification with Convolutional Neural Networks", "abstract": "Much of the recent progress made in image classification research can be credited to training procedure refinements, such as changes in data augmentations and optimization methods. In the literature, however, most refinements are either briefly mentioned as implementation details or only visible in source code. In this paper, we will examine a collection of such refinements and empirically evaluate their impact on the final model accuracy through ablation study. We will show that, by combining these refinements together, we are able to improve various CNN models significantly. For example, we raise ResNet-50's top-1 validation accuracy from 75.3% to 79.29% on ImageNet. We will also demonstrate that improvement on image classification accuracy leads to better transfer learning performance in other application domains such as object detection and semantic segmentation.", "date": "2019", "authors": ["Tong He , Zhi Zhang , Hang Zhang , Zhongyue Zhang , Junyuan Xie , Mu Li"], "references": [2194775991, 2618530766, 2962835968, 639708223, 1903029394, 2183341477, 2963911037, 1533861849, 2963420686, 2560023338]}, {"id": 2970902013, "title": "On Adversarial Mixup Resynthesis", "abstract": "In this paper, we explore new approaches to combining information encoded within the learned representations of auto-encoders. We explore models that are capable of combining the attributes of multiple inputs such that a resynthesised output is trained to fool an adversarial discriminator for real versus synthesised data. Furthermore, we explore the use of such an architecture in the context of semi-supervised learning, where we learn a mixing function whose objective is to produce interpolations of hidden states, or masked combinations of latent representations that are consistent with a conditioned class label. We show quantitative and qualitative evidence that such a formulation is an interesting avenue of research.", "date": "2018", "authors": ["Christopher Beckham 1, Sina Honari 2, Vikas Verma 3, Alex M. Lamb 2, Farnoosh Ghadiri 4, R Devon Hjelm 5, Yoshua Bengio 2, Chris Pal 1"], "references": [3107669106, 3118146262, 3099306795, 3108796939, 3096851030, 3093382707, 3033844565, 3034543211, 3111882316, 3111791812]}, {"id": 2987875759, "title": "Interpolated Adversarial Training: Achieving Robust Neural Networks Without Sacrificing Too Much Accuracy", "abstract": "Adversarial robustness has become a central goal in deep learning, both in theory and in practice. However, successful methods to improve the adversarial robustness (such as adversarial training) greatly hurt generalization performance on the unperturbed data. This could have a major impact on how achieving adversarial robustness affects real world systems (i.e. many may opt to forego robustness if it can improve accuracy on the unperturbed data). We propose Interpolated Adversarial Training, which employs recently proposed interpolation based training methods in the framework of adversarial training. On CIFAR-10, adversarial training increases the standard test error (when there is no adversary) from 4.43% to 12.32%, whereas with our Interpolated adversarial training we retain adversarial robustness while achieving a standard test error of only 6.45%. With our technique, the relative increase in the standard error for the robust model is reduced from 178.1% to just 45.5%.", "date": "2019", "authors": ["Alex Lamb 1, Vikas Verma 2, Juho Kannala 2, Yoshua Bengio 3"], "references": [2194775991, 2919115771, 2964153729, 2964253222, 2964137095, 2963374479, 2963542245, 2963399829, 2962729158, 2978426779]}, {"id": 2971149989, "title": "How to Initialize your Network? Robust Initialization for WeightNorm & ResNets", "abstract": "Residual networks (ResNet) and weight normalization play an important role in various deep learning applications. However, parameter initialization strategies have not been studied previously for weight normalized networks and, in practice, initialization methods designed for un-normalized networks are used as a proxy. Similarly, initialization for ResNets have also been studied for un-normalized networks and often under simplified settings ignoring the shortcut connection. To address these issues, we propose a novel parameter initialization strategy that avoids explosion/vanishment of information across layers for weight normalized networks with and without residual connections. The proposed strategy is based on a theoretical analysis using mean field approximation. We run over 2,500 experiments and evaluate our proposal on image datasets showing that the proposed initialization outperforms existing initialization methods in terms of generalization performance, robustness to hyper-parameter values and variance between seeds, especially when networks get deeper in which case existing methods fail to even start training. Finally, we show that using our initialization in conjunction with learning rate warmup is able to reduce the gap between the performance of weight normalized and batch normalized networks.", "date": "2019", "authors": ["Devansh Arpit 1, V\u00edctor Campos 2, Yoshua Bengio 1"], "references": [3117548433, 3100971012, 3061525482, 3019371514]}, {"id": 3098350627, "title": "Fastai: A Layered API for Deep Learning", "abstract": "fastai is a deep learning library which provides practitioners with high-level components that can quickly and easily provide state-of-the-art results in standard deep learning domains, and provides researchers with low-level components that can be mixed and matched to build new approaches. It aims to do both things without substantial compromises in ease of use, flexibility, or performance. This is possible thanks to a carefully layered architecture, which expresses common underlying patterns of many deep learning and data processing techniques in terms of decoupled abstractions. These abstractions can be expressed concisely and clearly by leveraging the dynamism of the underlying Python language and the flexibility of the PyTorch library. fastai includes: a new type dispatch system for Python along with a semantic type hierarchy for tensors; a GPU-optimized computer vision library which can be extended in pure Python; an optimizer which refactors out the common functionality of modern optimizers into two basic pieces, allowing optimization algorithms to be implemented in 4\u20135 lines of code; a novel 2-way callback system that can access any part of the data, model, or optimizer and change it at any point during training; a new data block API; and much more. We used this library to successfully create a complete deep learning course, which we were able to write more quickly than using previous approaches, and the code was more clear. The library is already in wide use in research, industry, and teaching.", "date": "2020", "authors": ["Jeremy Howard , Sylvain Gugger"], "references": [2101234009, 2899771611, 2011301426, 2549139847, 2219888463, 2990427812, 1165382972, 2799462250, 2970193165]}, {"id": 3034351824, "title": "Deep Isometric Learning for Visual Recognition", "abstract": "", "date": "2020", "authors": ["Haozhi Qi , Chong You , Xiaolong Wang , Yi Ma , Jitendra Malik"], "references": [3109745609, 3089324491]}, {"id": 2963681653, "title": "The Visual Centrifuge: Model-Free Layered Video Representations", "abstract": "True video understanding requires making sense of non-lambertian scenes where the color of light arriving at the camera sensor encodes information about not just the last object it collided with, but about multiple mediums -- colored windows, dirty mirrors, smoke or rain. Layered video representations have the potential of accurately modelling realistic scenes but have so far required stringent assumptions on motion, lighting and shape. Here we propose a learning-based approach for multi-layered video representation: we introduce novel uncertainty-capturing 3D convolutional architectures and train them to separate blended videos. We show that these models then generalize to single videos, where they exhibit interesting abilities: color constancy, factoring out shadows and separating reflections. We present quantitative and qualitative results on real world videos.", "date": "2019", "authors": ["Jean-Baptiste Alayrac 1, Joao Carreira 1, Andrew Zisserman 2"], "references": [1901129140, 2963073614, 2963524571, 2307770531, 2326925005, 2098535678, 2963399829, 2963125871, 2141115311, 2099770336]}, {"id": 1554944419, "title": "The elements of statistical learning : data mining, inference,and prediction", "abstract": "During the past decade there has been an explosion in computation and information technology. With it have come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It is a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting---the first comprehensive treatment of this topic in any book. This major new edition features many topics not covered in the original, including graphical models, random forests, ensemble methods, least angle regression and path algorithms for the lasso, non-negative matrix factorization, and spectral clustering. There is also a chapter on methods for ``wide'' data (p bigger than n), including multiple testing and false discovery rates. Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at Stanford University. They are prominent researchers in this area: Hastie and Tibshirani developed generalized additive models and wrote a popular book of that title. Hastie co-developed much of the statistical modeling software and environment in R/S-PLUS and invented principal curves and surfaces. Tibshirani proposed the lasso and is co-author of the very successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, projection pursuit and gradient boosting.", "date": "2005", "authors": ["Trevor Hastie , Robert J. Tibshirani , Jerome Friedman"], "references": [2140190241, 2164278908, 2179438025, 2122825543, 2063978378, 2131975293, 2117756735, 2087681821, 2109574129]}, {"id": 2132914434, "title": "A tutorial on spectral clustering", "abstract": "In recent years, spectral clustering has become one of the most popular modern clustering algorithms. It is simple to implement, can be solved efficiently by standard linear algebra software, and very often outperforms traditional clustering algorithms such as the k-means algorithm. On the first glance spectral clustering appears slightly mysterious, and it is not obvious to see why it works at all and what it really does. The goal of this tutorial is to give some intuition on those questions. We describe different graph Laplacians and their basic properties, present the most common spectral clustering algorithms, and derive those algorithms from scratch by several different approaches. Advantages and disadvantages of the different spectral clustering algorithms are discussed.", "date": "2007", "authors": ["Ulrike Luxburg"], "references": [1480376833, 2121947440, 2798909945, 2097308346, 2165874743, 1578099820, 1479807131, 2798707604, 2011832962, 2071949631]}, {"id": 1578099820, "title": "Spectral Graph Theory", "abstract": "Eigenvalues and the Laplacian of a graph Isoperimetric problems Diameters and eigenvalues Paths, flows, and routing Eigenvalues and quasi-randomness Expanders and explicit constructions Eigenvalues of symmetrical graphs Eigenvalues of subgraphs with boundary conditions Harnack inequalities Heat kernels Sobolev inequalities Advanced techniques for random walks on graphs Bibliography Index.", "date": "1996", "authors": ["Fan R K Chung"], "references": [2901284226, 2053631808, 2027808858]}, {"id": 2156718197, "title": "Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering", "abstract": "Drawing on the correspondence between the graph Laplacian, the Laplace-Beltrami operator on a manifold, and the connections to the heat equation, we propose a geometrically motivated algorithm for constructing a representation for data sampled from a low dimensional manifold embedded in a higher dimensional space. The algorithm provides a computationally efficient approach to nonlinear dimensionality reduction that has locality preserving properties and a natural connection to clustering. Several applications are considered.", "date": "2001", "authors": ["Mikhail Belkin , Partha Niyogi"], "references": [2053186076, 2121947440, 2001141328, 1578099820, 108654854]}, {"id": 2962820688, "title": "Convolutional neural networks applied to house numbers digit classification", "abstract": "We classify digits of real-world house numbers using convolutional neural networks (ConvNets). Con-vNets are hierarchical feature learning neural networks whose structure is biologically inspired. Unlike many popular vision approaches that are hand-designed, ConvNets can automatically learn a unique set of features optimized for a given task. We augmented the traditional ConvNet architecture by learning multi-stage features and by using Lp pooling and establish a new state-of-the-art of 95.10% accuracy on the SVHN dataset (48% error improvement). Furthermore, we analyze the benefits of different pooling methods and multi-stage features in ConvNets. The source code and a tutorial are available at eblearn.sf.net.", "date": "2012", "authors": ["Pierre Sermanet , Soumith Chintala , Yann LeCun"], "references": [2095705004, 2963446712, 2016053056, 2607333215, 2152839228, 2964311892, 2308045930, 2162741153, 2963574257, 1872489089]}, {"id": 1999192586, "title": "Learning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis", "abstract": "Previous work on action recognition has focused on adapting hand-designed local features, such as SIFT or HOG, from static images to the video domain. In this paper, we propose using unsupervised feature learning as a way to learn features directly from video data. More specifically, we present an extension of the Independent Subspace Analysis algorithm to learn invariant spatio-temporal features from unlabeled video data. We discovered that, despite its simplicity, this method performs surprisingly well when combined with deep learning techniques such as stacking and convolution to learn hierarchical representations. By replacing hand-designed features with our learned features, we achieve classification results superior to all previous published results on the Hollywood2, UCF, KTH and YouTube action recognition datasets. On the challenging Hollywood2 and YouTube action datasets we obtain 53.3% and 75.8% respectively, which are approximately 5% better than the current best published results. Further benefits of this method, such as the ease of training and the efficiency of training and prediction, will also be discussed. You can download our code and learned spatio-temporal features here: http://ai.stanford.edu/\u223cwzou/", "date": "2011", "authors": ["Quoc V. Le , Will Y. Zou , Serena Y. Yeung , Andrew Y. Ng"], "references": [2151103935, 2161969291, 2136922672, 3118608800, 1677409904, 2119605622, 2124386111, 2177274842, 2130325614, 2142194269]}, {"id": 2147860648, "title": "Tiled convolutional neural networks", "abstract": "Convolutional neural networks (CNNs) have been successfully applied to many tasks such as digit and object recognition. Using convolutional (tied) weights significantly reduces the number of parameters that have to be learned, and also allows translational invariance to be hard-coded into the architecture. In this paper, we consider the problem of learning invariances, rather than relying on hard-coding. We propose tiled convolution neural networks (Tiled CNNs), which use a regular \"tiled\" pattern of tied weights that does not require that adjacent hidden units share identical weights, but instead requires only that hidden units k steps away from each other to have tied weights. By pooling over neighboring units, this architecture is able to learn complex invariances (such as scale and rotational invariance) beyond translational invariance. Further, it also enjoys much of CNNs' advantage of having a relatively small number of learned parameters (such as ease of learning and greater scalability). We provide an efficient learning algorithm for Tiled CNNs based on Topographic ICA, and show that learning complex invariant features allows us to achieve highly competitive results for both the NORB and CIFAR-10 datasets.", "date": "2010", "authors": ["Jiquan Ngiam , Zhenghao Chen , Daniel Chia , Pang W. Koh , Quoc V. Le , Andrew Y. Ng"], "references": [2136922672, 3118608800, 2100495367, 2310919327, 2072128103, 1548802052, 2118585731, 2117130368, 2546302380, 2110798204]}, {"id": 2053186076, "title": "Nonlinear dimensionality reduction by locally linear embedding.", "abstract": "Many areas of science depend on exploratory data analysis and visualization. The need to analyze large amounts of multivariate data raises the fundamental problem of dimensionality reduction: how to discover compact representations of high-dimensional data. Here, we introduce locally linear embedding (LLE), an unsupervised learning algorithm that computes low-dimensional, neighborhood-preserving embeddings of high-dimensional inputs. Unlike clustering methods for local dimensionality reduction, LLE maps its inputs into a single global coordinate system of lower dimensionality, and its optimizations do not involve local minima. By exploiting the local symmetries of linear reconstructions, LLE is able to learn the global structure of nonlinear manifolds, such as those generated by images of faces or documents of text.", "date": "2000", "authors": ["Sam T. Roweis 1, Lawrence K. Saul 2"], "references": [1902027874, 2610857016, 1991848143, 2107636931, 2121122425, 2122538988, 2047870719, 2070320140, 1513400187, 2019020850]}, {"id": 2138451337, "title": "Eigenfaces for recognition", "abstract": "We have developed a near-real-time computer system that can locate and track a subject's head, and then recognize the person by comparing characteristics of the face to those of known individuals. The computational approach taken in this system is motivated by both physiology and information theory, as well as by the practical requirements of near-real-time performance and accuracy. Our approach treats the face recognition problem as an intrinsically two-dimensional (2-D) recognition problem rather than requiring recovery of three-dimensional geometry, taking advantage of the fact that faces are normally upright and thus may be described by a small set of 2-D characteristic views. The system functions by projecting face images onto a feature space that spans the significant variations among known face images. The significant features are known as \"eigenfaces,\" because they are the eigenvectors (principal components) of the set of faces; they do not necessarily correspond to features such as eyes, ears, and noses. The projection operation characterizes an individual face by a weighted sum of the eigenface features, and so to recognize a particular face it is necessary only to compare these weights to those of known individuals. Some particular advantages of our approach are that it provides for the ability to learn and later recognize new faces in an unsupervised manner, and that it is easy to implement using a neural network architecture.", "date": "1990", "authors": ["Matthew Turk , Alex Pentland"], "references": [2135463994, 2130259898, 2125848778, 2055712799, 2125999363, 1509703770, 1526492552, 1507699566, 2032361618, 1986450498]}, {"id": 2121647436, "title": "Eigenfaces vs. Fisherfaces: recognition using class specific linear projection", "abstract": "We develop a face recognition algorithm which is insensitive to large variation in lighting direction and facial expression. Taking a pattern classification approach, we consider each pixel in an image as a coordinate in a high-dimensional space. We take advantage of the observation that the images of a particular face, under varying illumination but fixed pose, lie in a 3D linear subspace of the high dimensional image space-if the face is a Lambertian surface without shadowing. However, since faces are not truly Lambertian surfaces and do indeed produce self-shadowing, images will deviate from this linear subspace. Rather than explicitly modeling this deviation, we linearly project the image into a subspace in a manner which discounts those regions of the face with large deviation. Our projection method is based on Fisher's linear discriminant and produces well separated classes in a low-dimensional subspace, even under severe variation in lighting and facial expressions. The eigenface technique, another method based on linearly projecting the image space to a low dimensional subspace, has similar computational requirements. Yet, extensive experimental results demonstrate that the proposed \"Fisherface\" method has error rates that are lower than those of the eigenface technique for tests on the Harvard and Yale face databases.", "date": "1997", "authors": ["P.N. Belhumeur , J.P. Hespanha , D.J. Kriegman"], "references": [2138451337, 2098693229, 2123977795, 2115689562, 3017143921, 2113341759, 2098947662, 2740373864, 2130259898, 2159173611]}, {"id": 2994340921, "title": "The AR face databasae", "abstract": "", "date": "1997", "authors": ["A. M. Martinez"], "references": [2129812935, 1782590233, 2102544846, 3111038685, 2132467081, 2157364932, 2134262590, 1963932623, 2912990735, 3102431071]}, {"id": 2095757522, "title": "Distortion invariant object recognition in the dynamic link architecture", "abstract": "An object recognition system based on the dynamic link architecture, an extension to classical artificial neural networks (ANNs), is presented. The dynamic link architecture exploits correlations in the fine-scale temporal structure of cellular signals to group neurons dynamically into higher-order entities. These entities represent a rich structure and can code for high-level objects. To demonstrate the capabilities of the dynamic link architecture, a program was implemented that can recognize human faces and other objects from video images. Memorized objects are represented by sparse graphs, whose vertices are labeled by a multiresolution description in terms of a local power spectrum, and whose edges are labeled by geometrical distance vectors. Object recognition can be formulated as elastic graph matching, which is performed here by stochastic optimization of a matching cost function. The implementation on a transputer network achieved recognition of human faces and office objects from gray-level camera images. The performance of the program is evaluated by a statistical analysis of recognition results from a portrait gallery comprising images of 87 persons. >", "date": "1993", "authors": ["M. Lades 1, J.C. Vorbruggen 1, J. Buhmann 2, J. Lange 1, C. von der Malsburg 1, R.P. Wurtz , W. Konen 1"], "references": [2011039300, 2176300081, 1991848143, 2135463994, 2130259898, 2079948225, 2167034998, 23758216, 2051719061, 1914401667]}, {"id": 2144354855, "title": "Face recognition: a convolutional neural-network approach", "abstract": "We present a hybrid neural-network for human face recognition which compares favourably with other methods. The system combines local image sampling, a self-organizing map (SOM) neural network, and a convolutional neural network. The SOM provides a quantization of the image samples into a topological space where inputs that are nearby in the original space are also nearby in the output space, thereby providing dimensionality reduction and invariance to minor changes in the image sample, and the convolutional neural network provides partial invariance to translation, rotation, scale, and deformation. The convolutional network extracts successively larger features in a hierarchical set of layers. We present results using the Karhunen-Loeve transform in place of the SOM, and a multilayer perceptron (MLP) in place of the convolutional network for comparison. We use a database of 400 images of 40 individuals which contains quite a high degree of variability in expression, pose, and facial details. We analyze the computational complexity and discuss how new classes could be added to the trained recognizer.", "date": "1996", "authors": ["S. Lawrence 1, C.L. Giles 2, Ah Chung Tsoi 2, A.D. Back 3"], "references": [2124776405, 1679913846, 2138451337, 2046079134, 2115689562, 2113341759, 2098947662, 1770825568, 2095757522, 2012352340]}, {"id": 2107369107, "title": "Recognizing imprecisely localized, partially occluded, and expression variant faces from a single sample per class", "abstract": "The classical way of attempting to solve the face (or object) recognition problem is by using large and representative data sets. In many applications, though, only one sample per class is available to the system. In this contribution, we describe a probabilistic approach that is able to compensate for imprecisely localized, partially occluded, and expression-variant faces even when only one single training sample per class is available to the system. To solve the localization problem, we find the subspace (within the feature space, e.g., eigenspace) that represents this error for each of the training images. To resolve the occlusion problem, each face is divided into k local regions which are analyzed in isolation. In contrast with other approaches where a simple voting space is used, we present a probabilistic method that analyzes how \"good\" a local match is. To make the recognition system less sensitive to the differences between the facial expression displayed on the training and the testing images, we weight the results obtained on each local area on the basis of how much of this local area is affected by the expression displayed on the current test image.", "date": "2002", "authors": ["A.M. Martinez"], "references": [2156909104, 2148603752, 2138451337, 2217896605, 2121647436, 2033419168, 2132549764, 3111038685, 2049633694, 2914885528]}, {"id": 1802356529, "title": "Energy-based models for sparse overcomplete representations", "abstract": "We present a new way of extending independent components analysis (ICA) to overcomplete representations. In contrast to the causal generative extensions of ICA which maintain marginal independence of sources, we define features as deterministic (linear) functions of the inputs. This assumption results in marginal dependencies among the features, but conditional independence of the features given the inputs. By assigning energies to the features a probability distribution over the input states is defined through the Boltzmann distribution. Free parameters of this model are trained using the contrastive divergence objective (Hinton, 2002). When the number of features is equal to the number of input dimensions this energy-based model reduces to noiseless ICA and we show experimentally that the proposed learning algorithm is able to perform blind source separation on speech data. In additional experiments we train overcomplete energy-based models to extract features from various standard data-sets containing speech, natural images, hand-written digits and faces.", "date": "2003", "authors": ["Yee Whye Teh 1, Max Welling 1, Simon Osindero 2, Geoffrey E. Hinton 1"], "references": [2116064496, 2078204800, 2154642048, 1652505363, 2099741732, 2151693816, 3110653090, 2145889472, 2146474141, 2105464873]}, {"id": 10021998, "title": "Loss Functions for Discriminative Training of Energy-Based Models.", "abstract": "", "date": "2004", "authors": ["Yann LeCun , Fu Jie Huang"], "references": [2310919327, 2147880316, 2132339004, 2137813581, 2134557905, 2008652694, 2105644991, 1802356529, 2148099973, 2175582831]}, {"id": 2153635508, "title": "LIBSVM: A library for support vector machines", "abstract": "LIBSVM is a library for Support Vector Machines (SVMs). We have been actively developing this package since the year 2000. The goal is to help users to easily apply SVM to their applications. LIBSVM has gained wide popularity in machine learning and many other areas. In this article, we present all implementation details of LIBSVM. Issues such as solving SVM optimization problems theoretical convergence multiclass classification probability estimates and parameter selection are discussed in detail.", "date": "2011", "authors": ["Chih-Chung Chang , Chih-Jen Lin"], "references": [2148603752, 2119821739, 2109943925, 2172000360, 3021469268, 1512098439, 2104978738, 2087347434, 2132870739, 2124351082]}, {"id": 2119821739, "title": "Support-Vector Networks", "abstract": "The support-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data. High generalization ability of support-vector networks utilizing polynomial input transformations is demonstrated. We also compare the performance of the support-vector network to various classical learning algorithms that all took part in a benchmark study of Optical Character Recognition.", "date": "1995", "authors": ["Corinna Cortes , Vladimir Vapnik"], "references": [2154642048, 1498436455, 2087347434, 1530699444, 2154579312, 2168228682, 2504871398, 1568787085, 5594912, 2322002063]}, {"id": 2084812512, "title": "UCI Repository of machine learning databases", "abstract": "", "date": "1997", "authors": ["C. L. Blake"], "references": [2111072639, 2026131661, 1565746575, 2148143831, 2011430131, 2172000360, 3100785508, 2128906841, 2017337590]}, {"id": 2087347434, "title": "A training algorithm for optimal margin classifiers", "abstract": "A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented. The technique is applicable to a wide variety of the classification functions, including Perceptrons, polynomials, and Radial Basis Functions. The effective number of parameters is adjusted automatically to match the complexity of the problem. The solution is expressed as a linear combination of supporting patterns. These are the subset of training patterns that are closest to the decision boundary. Bounds on the generalization performance based on the leave-one-out method and the VC-dimension are given. Experimental results on optical character recognition problems demonstrate the good generalization obtained when compared with other learning algorithms.", "date": "1992", "authors": ["Bernhard E. Boser 1, Isabelle M. Guyon 2, Vladimir N. Vapnik 2"], "references": [3017143921, 2171277043, 1530699444, 2165758113, 2154579312, 2076118331, 3023695801, 2086472796, 1965770722, 2111494971]}, {"id": 1618905105, "title": "Probabilistic Outputs for Support vector Machines and Comparisons to Regularized Likelihood Methods", "abstract": "", "date": "1998", "authors": ["John C. Platt"], "references": [2153635508, 2122646361, 1648445109, 2964212410, 1825604117, 2115150266, 1999954155, 1510526001, 9657784]}, {"id": 2101276256, "title": "Reducing multiclass to binary: a unifying approach for margin classifiers", "abstract": "We present a unifying framework for studying the solution of multiclass categorization problems by reducing them to multiple binary problems that are then solved using a margin-based binary learning algorithm. The proposed framework unifies some of the most popular approaches in which each class is compared against all others, or in which all pairs of classes are compared to each other, or in which output codes with error-correcting properties are used. We propose a general method for combining the classifiers generated on the binary problems, and we prove a general empirical multiclass loss bound given the empirical loss of the individual binary learning algorithms. The scheme and the corresponding bounds apply to many popular classification learning algorithms including support-vector machines, AdaBoost, regression, logistic regression and decision-tree algorithms. We also give a multiclass generalization error analysis for general output codes with AdaBoost as the binary learner. Experimental results with SVM and AdaBoost show that our scheme provides a viable alternative to the most commonly used multiclass algorithms.", "date": "2001", "authors": ["Erin L. Allwein 1, Robert E. Schapire 2, Yoram Singer 3"], "references": [2156909104, 2119821739, 1988790447, 2125055259, 2154642048, 3085162807, 2024046085, 1975846642, 1594031697, 2161920802]}, {"id": 2019575783, "title": "Classification by pairwise coupling", "abstract": "We discuss a strategy for polychotomous classification that involves estimating class probabilities for each pair of classes, and then coupling the estimates together. The coupling model is similar to the Bradley-Terry method for paired comparisons. We study the nature of the class probability estimates that arise, and examine the performance of the procedure in real and simulated data sets. Classifiers used include linear discriminants, nearest neighbors, adaptive nonlinear methods and the support vector machine.", "date": "1998", "authors": ["Trevor Hastie , Robert Tibshirani"], "references": [2140190241, 1570448133, 2143331230, 1510526001, 2101276256, 2133864802, 2104269704, 2159680539, 1999533590]}, {"id": 1988195734, "title": "Random forest: a classification and regression tool for compound classification and QSAR modeling.", "abstract": "A new classification and regression tool, Random Forest, is introduced and investigated for predicting a compound's quantitative or categorical biological activity based on a quantitative description of the compound's molecular structure. Random Forest is an ensemble of unpruned classification or regression trees created by using bootstrap samples of the training data and random feature selection in tree induction. Prediction is made by aggregating (majority vote or averaging) the predictions of the ensemble. We built predictive models for six cheminformatics data sets. Our analysis demonstrates that Random Forest is a powerful tool capable of delivering performance that is among the most accurate methods to date. We also present three additional features of Random Forest:\u2009 built-in performance assessment, a measure of relative importance of descriptors, and a measure of compound similarity that is weighted by the relative importance of descriptors. It is the combination of relatively high prediction accu...", "date": "2003", "authors": ["Vladimir Svetnik , Andy Liaw , Christopher Tong , J. Christopher Culberson , Robert P. Sheridan , Bradley P. Feuston"], "references": [2911964244, 1988790447, 2119479037, 1678356000, 1975846642, 2067885219, 1840338487, 2133199783, 2096729078, 2005685204]}, {"id": 1677409904, "title": "SURF: speeded up robust features", "abstract": "In this paper, we present a novel scale- and rotation-invariant interest point detector and descriptor, coined SURF (Speeded Up Robust Features). It approximates or even outperforms previously proposed schemes with respect to repeatability, distinctiveness, and robustness, yet can be computed and compared much faster. This is achieved by relying on integral images for image convolutions; by building on the strengths of the leading existing detectors and descriptors (in casu, using a Hessian matrix-based measure for the detector, and a distribution-based descriptor); and by simplifying these methods to the essential. This leads to a combination of novel detection, description, and matching steps. The paper presents experimental results on a standard evaluation set, as well as on imagery obtained in the context of a real-life object recognition application. Both show SURF's strong performance.", "date": "2006", "authors": ["Herbert Bay 1, Tinne Tuytelaars 2, Luc Van Gool 1"], "references": [2151103935, 2164598857, 2124386111, 2177274842, 2128017662, 2012778485, 1980911747, 2145072179, 2172188317, 2124404372]}, {"id": 2104095591, "title": "Snakes : Active Contour Models", "abstract": "A snake is an energy-minimizing spline guided by external constraint forces and influenced by image forces that pull it toward features such as lines and edges. Snakes are active contour models: they lock onto nearby edges, localizing them accurately. Scale-space continuation can be used to enlarge the capture region surrounding a feature. Snakes provide a unified account of a number of visual problems, including detection of edges, lines, and subjective contours; motion tracking; and stereo matching. We have used snakes successfully for interactive interpretation, in which user-imposed constraint forces guide the snake near features of interest.", "date": "1987", "authors": ["Michael Kass , Andrew P. Witkin , Demetri Terzopoulos"], "references": [2109863423, 2003370853, 1995756857, 1531060698, 2139762693, 2107198582, 2582614493, 2045798786, 1631253743, 1977699267]}, {"id": 1647075334, "title": "Potent and specific genetic interference by double-stranded RNA in Caenorhabditis elegans", "abstract": "Experimental introduction of RNA into cells can be used in certain biological systems to interfere with the function of an endogenous gene. Such effects have been proposed to result from a simple antisense mechanism that depends on hybridization between the injected RNA and endogenous messenger RNA transcripts. RNA interference has been used in the nematode Caenorhabditis elegans to manipulate gene expression. Here we investigate the requirements for structure and delivery of the interfering RNA. To our surprise, we found that double-stranded RNA was substantially more effective at producing interference than was either strand individually. After injection into adult animals, purified single strands had at most a modest effect, whereas double-stranded mixtures caused potent and specific interference. The effects of this interference were evident in both the injected animals and their progeny. Only a few molecules of injected double-stranded RNA were required per affected cell, arguing against stochiometric interference with endogenous mRNA and suggesting that there could be a catalytic or amplification component in the interference process.", "date": "1998", "authors": ["Andrew Fire 1, SiQun Xu 1, Mary K. Montgomery 1, Steven A. Kostas 1, 2, Samuel E. Driver 3, Craig C. Mello 3"], "references": [2032579724, 2034831897, 2149871164, 1944127002, 1992194142, 1989952069, 1594381468, 1581412826, 2151068726, 2022846624]}, {"id": 2121927366, "title": "A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics", "abstract": "This paper presents a database containing 'ground truth' segmentations produced by humans for images of a wide variety of natural scenes. We define an error measure which quantifies the consistency between segmentations of differing granularities and find that different human segmentations of the same image are highly consistent. Use of this dataset is demonstrated in two applications: (1) evaluating the performance of segmentation algorithms and (2) measuring probability distributions associated with Gestalt grouping factors as well as statistics of image region properties.", "date": "2001", "authors": ["D. Martin , C. Fowlkes , D. Tal , J. Malik"], "references": [2121947440, 2126326837, 2124731682, 1524408959, 2101933716, 2120838001, 2150117517, 2139643804, 2124592837, 1537519384]}, {"id": 2119823327, "title": "Learning to detect natural image boundaries using local brightness, color, and texture cues", "abstract": "The goal of this work is to accurately detect and localize boundaries in natural scenes using local image measurements. We formulate features that respond to characteristic changes in brightness, color, and texture associated with natural boundaries. In order to combine the information from these features in an optimal way, we train a classifier using human labeled images as ground truth. The output of this classifier provides the posterior probability of a boundary at each image location and orientation. We present precision-recall curves showing that the resulting detector significantly outperforms existing approaches. Our two main results are 1) that cue combination can be performed adequately with a simple linear model and 2) that a proper, explicit treatment of texture is required to detect boundaries in natural images.", "date": "2004", "authors": ["D.R. Martin 1, C.C. Fowlkes 2, J. Malik 2"], "references": [2153635508, 2145023731, 2121927366, 2111308925, 2032210760, 2135705692, 2093191240, 2141376824, 3003716168, 2025653905]}, {"id": 1991848143, "title": "Self Organization And Associative Memory", "abstract": "1. Various Aspects of Memory.- 1.1 On the Purpose and Nature of Biological Memory.- 1.1.1 Some Fundamental Concepts.- 1.1.2 The Classical Laws of Association.- 1.1.3 On Different Levels of Modelling.- 1.2 Questions Concerning the Fundamental Mechanisms of Memory.- 1.2.1 Where Do the Signals Relating to Memory Act Upon?.- 1.2.2 What Kind of Encoding is Used for Neural Signals?.- 1.2.3 What are the Variable Memory Elements?.- 1.2.4 How are Neural Signals Addressed in Memory?.- 1.3 Elementary Operations Implemented by Associative Memory.- 1.3.1 Associative Recall.- 1.3.2 Production of Sequences from the Associative Memory.- 1.3.3 On the Meaning of Background and Context.- 1.4 More Abstract Aspects of Memory.- 1.4.1 The Problem of Infinite-State Memory.- 1.4.2 Invariant Representations.- 1.4.3 Symbolic Representations.- 1.4.4 Virtual Images.- 1.4.5 The Logic of Stored Knowledge.- 2. Pattern Mathematics.- 2.1 Mathematical Notations and Methods.- 2.1.1 Vector Space Concepts.- 2.1.2 Matrix Notations.- 2.1.3 Further Properties of Matrices.- 2.1.4 Matrix Equations.- 2.1.5 Projection Operators.- 2.1.6 On Matrix Differential Calculus.- 2.2 Distance Measures for Patterns.- 2.2.1 Measures of Similarity and Distance in Vector Spaces.- 2.2.2 Measures of Similarity and Distance Between Symbol Strings.- 2.2.3 More Accurate Distance Measures for Text.- 3. Classical Learning Systems.- 3.1 The Adaptive Linear Element (Adaline).- 3.1.1 Description of Adaptation by the Stochastic Approximation.- 3.2 The Perceptron.- 3.3 The Learning Matrix.- 3.4 Physical Realization of Adaptive Weights.- 3.4.1 Perceptron and Adaline.- 3.4.2 Classical Conditioning.- 3.4.3 Conjunction Learning Switches.- 3.4.4 Digital Representation of Adaptive Circuits.- 3.4.5 Biological Components.- 4. A New Approach to Adaptive Filters.- 4.1 Survey of Some Necessary Functions.- 4.2 On the \"Transfer Function\" of the Neuron.- 4.3 Models for Basic Adaptive Units.- 4.3.1 On the Linearization of the Basic Unit.- 4.3.2 Various Cases of Adaptation Laws.- 4.3.3 Two Limit Theorems.- 4.3.4 The Novelty Detector.- 4.4 Adaptive Feedback Networks.- 4.4.1 The Autocorrelation Matrix Memory.- 4.4.2 The Novelty Filter.- 5. Self-Organizing Feature Maps.- 5.1 On the Feature Maps of the Brain.- 5.2 Formation of Localized Responses by Lateral Feedback.- 5.3 Computational Simplification of the Process.- 5.3.1 Definition of the Topology-Preserving Mapping.- 5.3.2 A Simple Two-Dimensional Self-Organizing System.- 5.4 Demonstrations of Simple Topology-Preserving Mappings.- 5.4.1 Images of Various Distributions of Input Vectors.- 5.4.2 \"The Magic TV\".- 5.4.3 Mapping by a Feeler Mechanism.- 5.5 Tonotopic Map.- 5.6 Formation of Hierarchical Representations.- 5.6.1 Taxonomy Example.- 5.6.2 Phoneme Map.- 5.7 Mathematical Treatment of Self-Organization.- 5.7.1 Ordering of Weights.- 5.7.2 Convergence Phase.- 5.8 Automatic Selection of Feature Dimensions.- 6. Optimal Associative Mappings.- 6.1 Transfer Function of an Associative Network.- 6.2 Autoassociative Recall as an Orthogonal Projection.- 6.2.1 Orthogonal Projections.- 6.2.2 Error-Correcting Properties of Projections.- 6.3 The Novelty Filter.- 6.3.1 Two Examples of Novelty Filter.- 6.3.2 Novelty Filter as an Autoassociative Memory.- 6.4 Autoassociative Encoding.- 6.4.1 An Example of Autoassociative Encoding.- 6.5 Optimal Associative Mappings.- 6.5.1 The Optimal Linear Associative Mapping.- 6.5.2 Optimal Nonlinear Associative Mappings.- 6.6 Relationship Between Associative Mapping, Linear Regression, and Linear Estimation.- 6.6.1 Relationship of the Associative Mapping to Linear Regression.- 6.6.2 Relationship of the Regression Solution to the Linear Estimator.- 6.7 Recursive Computation of the Optimal Associative Mapping.- 6.7.1 Linear Corrective Algorithms.- 6.7.2 Best Exact Solution (Gradient Projection).- 6.7.3 Best Approximate Solution (Regression).- 6.7.4 Recursive Solution in the General Case.- 6.8 Special Cases.- 6.8.1 The Correlation Matrix Memory.- 6.8.2 Relationship Between Conditional Averages and Optimal Estimator.- 7. Pattern Recognition.- 7.1 Discriminant Functions.- 7.2 Statistical Formulation of Pattern Classification.- 7.3 Comparison Methods.- 7.4 The Subspace Methods of Classification.- 7.4.1 The Basic Subspace Method.- 7.4.2 The Learning Subspace Method (LSM).- 7.5 Learning Vector Quantization.- 7.6 Feature Extraction.- 7.7 Clustering.- 7.7.1 Simple Clustering (Optimization Approach).- 7.7.2 Hierarchical Clustering (Taxonomy Approach).- 7.8 Structural Pattern Recognition Methods.- 8. More About Biological Memory.- 8.1 Physiological Foundations of Memory.- 8.1.1 On the Mechanisms of Memory in Biological Systems.- 8.1.2 Structural Features of Some Neural Networks.- 8.1.3 Functional Features of Neurons.- 8.1.4 Modelling of the Synaptic Plasticity.- 8.1.5 Can the Memory Capacity Ensue from Synaptic Changes?.- 8.2 The Unified Cortical Memory Model.- 8.2.1 The Laminar Network Organization.- 8.2.2 On the Roles of Interneurons.- 8.2.3 Representation of Knowledge Over Memory Fields.- 8.2.4 Self-Controlled Operation of Memory.- 8.3 Collateral Reading.- 8.3.1 Physiological Results Relevant to Modelling.- 8.3.2 Related Modelling.- 9. Notes on Neural Computing.- 9.1 First Theoretical Views of Neural Networks.- 9.2 Motives for the Neural Computing Research.- 9.3 What Could the Purpose of the Neural Networks be?.- 9.4 Definitions of Artificial \"Neural Computing\" and General Notes on Neural Modelling.- 9.5 Are the Biological Neural Functions Localized or Distributed?.- 9.6 Is Nonlinearity Essential to Neural Computing?.- 9.7 Characteristic Differences Between Neural and Digital Computers.- 9.7.1 The Degree of Parallelism of the Neural Networks is Still Higher than that of any \"Massively Parallel\" Digital Computer.- 9.7.2 Why the Neural Signals Cannot be Approximated by Boolean Variables.- 9.7.3 The Neural Circuits do not Implement Finite Automata.- 9.7.4 Undue Views of the Logic Equivalence of the Brain and Computers on a High Level.- 9.8 \"Connectionist Models\".- 9.9 How can the Neural Computers be Programmed?.- 10. Optical Associative Memories.- 10.1 Nonholographic Methods.- 10.2 General Aspects of Holographic Memories.- 10.3 A Simple Principle of Holographic Associative Memory.- 10.4 Addressing in Holographic Memories.- 10.5 Recent Advances of Optical Associative Memories.- Bibliography on Pattern Recognition.- References.", "date": "1983", "authors": ["Teuvo Kohonen"], "references": []}, {"id": 2581275558, "title": "Optimization by simulated annealing", "abstract": "There is a deep and useful connection between statistical mechanics (the behavior of systems with many degrees of freedom in thermal equilibrium at a finite temperature) and multivariate or combinatorial optimization (finding the minimum of a given function depending on many parameters). A detailed analogy with annealing in solids provides a framework for optimization of the properties of very large and complex systems. This connection to statistical mechanics exposes new information and provides an unfamiliar perspective on traditional optimization problems and methods.", "date": "1986", "authors": ["S. Kirkpatrick 1, C. D. Gelatt 1, M. P. Vecchi 2"], "references": [2042986967, 2022820481, 2114552889, 2022494241, 2143037347, 2056760934, 2148673189, 86906884, 2014952973, 2014068360]}, {"id": 2154642048, "title": "Learning internal representations by error propagation", "abstract": "This chapter contains sections titled: The Problem, The Generalized Delta Rule, Simulation Results, Some Further Generalizations, Conclusion", "date": "1987", "authors": ["D. E. Rumelhart , G. E. Hinton , R. J. Williams"], "references": []}, {"id": 2016589492, "title": "A learning algorithm for continually running fully recurrent neural networks", "abstract": "The exact form of a gradient-following learning algorithm for completely recurrent networks running in continually sampled time is derived and used as the basis for practical algorithms for temporal supervised learning tasks. These algorithms have (1) the advantage that they do not require a precisely defined training interval, operating while the network runs; and (2) the disadvantage that they require nonlocal communication in the network being trained and are computationally expensive. These algorithms allow networks having recurrent connections to learn complex tasks that require the retention of information over time periods having either fixed or indefinite length.", "date": "1989", "authors": ["Ronald J. Williams 1, David Zipser 2"], "references": [2154642048, 2293063825, 2110485445, 2143503258, 1959983357, 1881179843, 1984205520, 1984375561, 2119796132, 1527772862]}, {"id": 2088978850, "title": "Minimizing multimodal functions of continuous variables with the \u201csimulated annealing\u201d algorithm Corrigenda for this article is available here", "abstract": "A new global optimization algorithm for functions of continuous variables is presented, derived from the \u201cSimulated Annealing\u201d algorithm recently introduced in combinatorial optimization. The algorithm is essentially an iterative random search procedure with adaptive moves along the coordinate directions. It permits uphill moves under the control of a probabilistic criterion, thus tending to avoid the first local minima encountered. The algorithm has been tested against the Nelder and Mead simplex method and against a version of Adaptive Random Search. The test functions were Rosenbrock valleys and multiminima functions in 2,4, and 10 dimensions. The new method proved to be more reliable than the others, being always able to find the optimum, or at least a point very close to it. It is quite costly in term of function evaluations, but its cost can be predicted in advance, depending only slightly on the starting point.", "date": "1987", "authors": ["A. Corana , M. Marchesi , C. Martini , S. Ridella"], "references": [2581275558, 2171074980, 2022772618, 2056760934, 2026258334, 2012231377, 2152710595, 2029673686, 1652464192, 2065606540]}, {"id": 2148099973, "title": "Global optimization of a neural network-hidden Markov model hybrid", "abstract": "The integration of multilayered and recurrent artificial neural networks (ANNs) with hidden Markov models (HMMs) is addressed. ANNs are suitable for approximating functions that compute new acoustic parameters, whereas HMMs have been proven successful at modeling the temporal structure of the speech signal. In the approach described, the ANN outputs constitute the sequence of observation vectors for the HMM. An algorithm is proposed for global optimization of all the parameters. Results on speaker-independent recognition experiments using this integrated ANN-HMM system on the TIMIT continuous speech database are reported. >", "date": "1992", "authors": ["Y. Bengio , R. De Mori , G. Flammia , R. Kompe"], "references": [2125838338, 2154642048, 169539560, 2077804127, 2135622428, 2086699924, 2140539590, 2169415433, 2140766383, 2125610452]}, {"id": 1996741810, "title": "Local feedback multilayered networks", "abstract": "In this paper, we investigate the capabilities of local feedback multilayered networks, a particular class of recurrent networks, in which feedback connections are only allowed from neurons to themselves. In this class, learning can be accomplished by an algorithm that is local in both space and time. We describe the limits and properties of these networks and give some insights on their use for solving practical problems.", "date": "1992", "authors": ["Paolo Frasconi , Marco Gori , Giovanni Soda"], "references": [2110485445, 2016589492, 2007431958, 2143503258, 2121553911, 2140539590, 2133656308, 2047515372, 2032164462, 2094096029]}, {"id": 2125329357, "title": "Using random weights to train multilayer networks of hard-limiting units", "abstract": "A gradient descent algorithm suitable for training multilayer feedforward networks of processing units with hard-limiting output functions is presented. The conventional backpropagation algorithm cannot be applied in this case because the required derivatives are not available. However, if the network weights are random variables with smooth distribution functions, the probability of a hard-limiting unit taking one of its two possible values is a continuously differentiable function. In the paper, this is used to develop an algorithm similar to backpropagation, but for the hard-limiting case. It is shown that the computational framework of this algorithm is similar to standard backpropagation, but there is an additional computational expense involved in the estimation of gradients. Upper bounds on this estimation penalty are given. Two examples which indicate that, when this algorithm is used to train networks of hard-limiting units, its performance is similar to that of conventional backpropagation applied to networks of units with sigmoidal characteristics are presented. >", "date": "1992", "authors": ["P.L. Barlett , T. Downs"], "references": [2154642048, 2165758113, 2016589492, 1507849272, 2176028050, 2010029425, 2084544490, 2181111061, 1529808766, 2003357516]}, {"id": 1527772862, "title": "A focused backpropagation algorithm for temporal pattern recognition", "abstract": "", "date": "1994", "authors": ["Michael C. Mozer"], "references": [2124592697, 2783190965, 2293185259, 2754791538, 2083703071, 2106725254, 2569140349, 2982966244, 2033955478]}, {"id": 1959983357, "title": "Attractor dynamics and parallelism in a connectionist sequential machine", "abstract": "", "date": "1990", "authors": ["Michael I. Jordan"], "references": [1546771929, 2144499799, 2136848157, 1934184906, 2016589492, 1553004968, 1597190145, 2128499899, 2986382673, 2964138630]}, {"id": 2028629011, "title": "Learning state space trajectories in recurrent neural networks", "abstract": "A number of procedures are described for finding delta E/ delta W/sub ij/ where E is an error functional of the temporal trajectory of the states of a continuous recurrent network and w/sub ij/ are the weights of that network. Computing these quantities allows one to perform gradient descent in the weights to minimize E, so these procedures form the kernels of connectionist learning algorithms. Simulations in which networks are taught to move through limit cycles are shown, along with some empirical perturbation sensitivity tests. The author describes a number of elaborations of the basic idea, including mutable time delays and teacher forcing. He includes a complexity analysis of the various learning procedures discussed and analyzed. Temporally continuous recurrent networks seems particularly suited for temporally continuous domains, such as signal processing, control, and speech. >", "date": "1988", "authors": ["Pearlmutter"], "references": [2581275558, 2154642048, 1597286183, 2173629880, 2016589492, 1507849272, 2007431958, 1959983357, 1971129545, 1573503290]}, {"id": 2053127376, "title": "On the time relations of mental processes: An examination of systems of processes in cascade.", "abstract": "", "date": "1979", "authors": ["James L. McClelland"], "references": [2135255848, 1784695092, 2098205603, 2045597501, 2094493170, 1502139053, 1967670055, 2106654511, 2147311265, 1529340823]}, {"id": 2167607759, "title": "The \"Moving Targets\" Training Algorithm", "abstract": "A simple method for training the dynamical behavior of a neural network is derived. It is applicable to any training problem in discrete-time networks with arbitrary feedback. The algorithm resembles back-propagation in that an error function is minimized using a gradient-based method, but the optimization is carried out in the hidden part of state space either instead of, or in addition to weight space. Computational results are presented for some simple dynamical training problems, one of which requires response to a signal 100 time steps in the past.", "date": "1988", "authors": ["Richard Rohwer"], "references": [2341283081, 2154642048, 3004157836, 2077658674, 2016589492, 2796837256, 2143503258, 1984205520, 1984375561, 2028629011]}, {"id": 1652505363, "title": "Parallel distributed processing: explorations in the microstructure of cognition, vol. 1: foundations", "abstract": "The fundamental principles, basic mechanisms, and formal analyses involved in the development of parallel distributed processing (PDP) systems are presented in individual chapters contributed by leading experts. Topics examined include distributed representations, PDP models and general issues in cognitive science, feature discovery by competitive learning, the foundations of harmony theory, learning and relearning in Boltzmann machines, and learning internal representations by error propagation. Consideration is given to linear algebra in PDP, the logic of additive functions, resource requirements of standard and programmable nets, and the P3 parallel-network simulating system.", "date": "1986", "authors": ["David E. Rumelhart 1, James L. McClelland 2"], "references": [1614298861, 2145339207, 2076063813, 2072128103, 2116064496, 2145094598, 2025768430, 2107941094, 2154642048, 2137983211]}, {"id": 2177721432, "title": "Neurons with graded response have collective computational properties like those of two-state neurons", "abstract": "A model for a large network of \"neurons\" with a graded response (or sigmoid input--output relation) is studied. This deterministic system has collective properties in very close correspondence with the earlier stochastic model based on McCulloch--Pitts neurons. The content-addressable memory and other emergent collective properties of the original model also are present in the graded response model. The idea that such collective properties are used in biological systems is given added credence by the continued presence of such properties for more nearly biological \"neurons.\" Collective analog electrical circuits of the kind described will certainly function. The collective states of the two models have a simple correspondence. The original model will continue to be useful for simulations, because its connection to graded response systems is established. Equations that include the effect of action potentials in the graded response system are also developed.", "date": "1988", "authors": ["J. J. Hopfield"], "references": [1973108021, 582196039]}, {"id": 2075510082, "title": "Characteristics of Random Nets of Analog Neuron-Like Elements", "abstract": "The dynamic behavior of randomly connected analog neuron-like elements that process pulse-frequency modulated signals is investigated from the macroscopic point of view. By extracting two statistical parameters, the macroscopic state equations are derived in terms of these parameters under some hypotheses on the stochastics of microscopic states. It is shown that a random net of statistically symmetric structure is monostable or bistable, and the stability criteria are explicitly given. Random nets consisting of many different classes of elements are also analyzed. Special attention is paid to nets of randomly connected excitatory and inhibitory elements. It is shown that a stable oscillation exists in such a net?in contrast with the fact that no stable oscillations exist in a net of statistically symmetric structure even if negative as well as positive synaptic weights are permitted at a time. The results are checked by computer-simulated experiments.", "date": "1972", "authors": ["Shun-Ichi Amarimber"], "references": [2159116087, 2060666586, 1968014724, 1995030605, 2159187100, 2072972096, 2082628174]}, {"id": 2138484437, "title": "Identification and control of dynamical systems using neural networks", "abstract": "It is demonstrated that neural networks can be used effectively for the identification and control of nonlinear dynamical systems. The emphasis is on models for both identification and control. Static and dynamic backpropagation methods for the adjustment of parameters are discussed. In the models that are introduced, multilayer and recurrent networks are interconnected in novel configurations, and hence there is a real need to study them in a unified fashion. Simulation results reveal that the identification and adaptive control schemes suggested are practically feasible. Basic concepts and definitions are introduced throughout, and theoretical questions that have to be addressed are also described. >", "date": "1990", "authors": ["K.S. Narendra , K. Parthasarathy"], "references": [2137983211, 2293063825, 1597286183, 1572161815, 3036751298, 2174984063, 2122136962, 2095425517, 2007431958, 2116528865]}, {"id": 2150355110, "title": "Backpropagation through time: what it does and how to do it", "abstract": "Basic backpropagation, which is a simple method now being widely used in areas like pattern recognition and fault diagnosis, is reviewed. The basic equations for backpropagation through time, and applications to areas like pattern recognition involving dynamic systems, systems identification, and control are discussed. Further extensions of this method, to deal with systems other than neural networks, systems involving simultaneous equations, or true recurrent networks, and other practical issues arising with the method are described. Pseudocode is provided to clarify the algorithms. The chain rule for ordered derivatives-the theorem which underlies backpropagation-is briefly discussed. The focus is on designing a simpler version of backpropagation which can be translated into computer code and applied directly by neutral network users. >", "date": "1990", "authors": ["P.J. Werbos"], "references": [2154642048, 2068484625, 2123036476, 1971129545, 1573503290, 1881179843, 2028629011, 2090248140, 2153795254, 1487148666]}, {"id": 1583833196, "title": "Neuronlike adaptive elements that can solve difficult learning control problems", "abstract": "", "date": "1990", "authors": ["Andrew G. Barto , Richard S. Sutton , Charles W. Anderson"], "references": [2170227253, 1602079996, 212314082, 1481405077, 1504806788, 2038584310, 1971728746, 2001185203, 2834852173, 2293846015]}, {"id": 2143787696, "title": "Gradient methods for the optimization of dynamical systems containing neural networks", "abstract": "An extension of the backpropagation method, termed dynamic backpropagation, which can be applied in a straightforward manner for the optimization of the weights (parameters) of multilayer neural networks is discussed. The method is based on the fact that gradient methods used in linear dynamical systems can be combined with backpropagation methods for neural networks to obtain the gradient of a performance index of nonlinear dynamical systems. The method can be applied to any complex system which can be expressed as the interconnection of linear dynamical systems and multilayer neural networks. To facilitate the practical implementation of the proposed method, emphasis is placed on the diagrammatic representation of the system which generates the gradient of the performance function. >", "date": "1991", "authors": ["K.S. Narendra , K. Parthasarathy"], "references": [2138484437, 2016589492, 2150355110, 2076086013, 2016261381]}, {"id": 2057653135, "title": "An efficient gradient-based algorithm for on-line training of recurrent network trajectories", "abstract": "A novel variant of the familiar backpropagation-through-time approach to training recurrent networks is described. This algorithm is intended to be used on arbitrary recurrent networks that run continually without ever being reset to an initial state, and it is specifically designed for computationally efficient computer implementation. This algorithm can be viewed as a cross between epochwise backpropagation through time, which is not appropriate for continually running networks, and the widely used on-line gradient approximation technique of truncated backpropagation through time.", "date": "1990", "authors": ["Ronald J. Williams , Jing Peng"], "references": [2154642048, 2016589492, 2007431958, 2143503258, 1881179843, 2121553911, 1984205520, 2139273175, 2047515372, 2044422789]}, {"id": 2132152975, "title": "Decoupled extended Kalman filter training of feedforward layered networks", "abstract": "Presents a training algorithm for feedforward layered networks based on a decoupled extended Kalman filter (DEKF). The authors present an artificial process noise extension to DEKF that increases its convergence rate and assists in the avoidance of local minima. Computationally efficient formulations for two particularly natural and useful cases of DEKF are given. Through a series of pattern classification and function approximation experiments, three members of DEKF are compared with one another and with standard backpropagation (SBP). These studies demonstrate that the judicious grouping of weights along with the use of artificial process noise in DEKF result in input-output mapping performance that is comparable to the global extended Kalman algorithm, and is often superior to SBP, while requiring significantly fewer presentations of training data than SBP and less overall training time than either of these procedures. >", "date": "1991", "authors": ["G.V. Puskorius , L.A. Feldkamp"], "references": [2138484437, 1761621746, 2112462566, 2122568838, 2077493113, 107400462, 2102496649]}, {"id": 2112462566, "title": "Training Multilayer Perceptrons with the Extended Kalman Algorithm", "abstract": "A large fraction of recent work in artificial neural nets uses multilayer perceptrons trained with the back-propagation algorithm described by Rumelhart et. al. This algorithm converges slowly for large or complex problems such as speech recognition, where thousands of iterations may be needed for convergence even with small data sets. In this paper, we show that training multilayer perceptrons is an identification problem for a nonlinear dynamic system which can be solved using the Extended Kalman Algorithm. Although computationally complex, the Kalman algorithm usually converges in a few iterations. We describe the algorithm and compare it with back-propagation using two-dimensional examples.", "date": "1987", "authors": ["Sharad Singhal , Lance Wu"], "references": [2154642048, 2173629880, 2293807537, 304861154, 2105934661, 1613359937, 2051992922]}, {"id": 1529008516, "title": "Supervised learning and systems with excess degrees of freedom", "abstract": "WHEN DISTINCT OUTPUTS OF AN ADAPTIVE SYSTEM HAVE EQUIVALENT EFFECTS ON THE ENVIRONMENT, THE PROBLEM OF FINDING APPROPRIATE ACTIONS GIVEN DESIRED RESULTS IS ILL-POSED. FOR SUPERVISED LEARNING ALGORITHMS, THE ILL-POSEDNESS OF SUCH \"INVERSE LEARNING PROBLEMS\" IMPLIES A CERTAIN FLEXIBILITY---DURING TRAINING, THERE ARE IN GENERAL MANY POSSIBLE TARGET VECTORS CORRESPONDING TO EACH INPUT VECTOR. TO ALLOW SUPERVISED LEARNING ALGORITHMS TO MAKE USE OF THIS FLEXIBILITY, THE CURRENT PAPER CONSIDERS HOW TO SPECIFY TARGETS BY SETS OF CONSTRAINTS, RATHER THAN AS PARTICULAR VECTORS. TWO CLASSES OF CONSTRAINTS ARE DISTINGUISHED---`CONFIGURATIONAL'' CONSTRAINTS, WHICH DEFINE REGIONS OF OUTPUT SPACE IN WHICH AN OUTPUT VECTOR MUST LIE, AND `TEMPORAL'' CONSTRAINTS, WHICH DEFINE RELATIONSHIPS BETWEEN OUTPUTS PRODUCED AT DIFFER- ENT POINTS IN TIME. LEARNING ALGORITHMS MINIMIZE A COST FUNCTION THAT CON- TAINS TERMS FOR BOTH KINDS OF CONSTRAINTS. THIS APPROACH TO INVERSE LEARN- ING IS ILLUSTRATED BY A ROBOTICS APPLICATION IN WHICH A NETWORK FINDS TRA- JECTORIES OF INVERSE KINEMATIC SOLUTIONS FOR MANIPULATORS WITH EXCESS DEGREES OF FREEDOM.", "date": "1988", "authors": ["Michael I. Jordan"], "references": [2076063813, 2123716044, 2123036476, 2156562940, 2167224731, 2118415523, 2056655352, 2188233853, 2152503618, 2115072676]}, {"id": 2090248140, "title": "Generic constraints on underspecified target trajectories", "abstract": "Although general network learning rules are of undeniable interest, it is generally agreed that successful accounts of learning must incorporate domain-specific, a priori knowledge. Such knowledge might be used, for example, to determine the structure of a network or its initial weights. The author discusses a third possibility in which domain-specific knowledge is incorporated directly in a network learning rule via a set of constraints on activations. The approach uses the notion of a forward model to give constraints a domain-specific interpretation. This approach is demonstrated with several examples from the domain of motor learning. >", "date": "1988", "authors": ["Jordan"], "references": [2895674046, 2016589492, 1507849272, 1967377907, 1885639605, 1981297107, 2150367199, 1892385946, 1969166509, 2046329526]}, {"id": 1971129545, "title": "Generalization of backpropagation with application to a recurrent gas market model", "abstract": "Abstract Backpropagation is often viewed as a method for adapting artificial neural networks to classify patterns. Based on parts of the book by Rumelhart and colleagues, many authors equate backpropagation with the generalized delta rule applied to fully-connected feedforward networks. This paper will summarize a more general formulation of backpropagation, developed in 1974, which does more justice to the roots of the method in numerical analysis and statistics, and also does more justice to creative approaches expressed by neural modelers in the past year or two. It will discuss applications of backpropagation to forecasting over time (where errors have been halved by using methods other than least squares), to optimization, to sensitivity analysis, and to brain research. This paper will go on to derive a generalization of backpropagation to recurrent systems (which input their own output), such as hybrids of perceptron-style networks and Grossberg/Hopfield networks. Unlike the proposal of Rumelhart, Hinton, and Williams, this generalization does not require the storage of intermediate iterations to deal with continuous recurrence. This generalization was applied in 1981 to a model of natural gas markets, where it located sources of forecast uncertainty related to the use of least squares to estimate the model parameters in the first place.", "date": "1987", "authors": ["Paul J. Werbos"], "references": [22297218, 2068484625, 1583833196, 2176028050, 2010526455, 1573503290, 1969166509, 2153795254, 134309601, 3022423118]}, {"id": 1996647346, "title": "A Steepest-Ascent Method for Solving Optimum Programming Problems", "abstract": "", "date": "1962", "authors": ["A. E. Bryson 1, W. F. Denham 2"], "references": [2962727772, 2143503258, 2154890045, 2080132051, 2136209220, 567460027, 2118415523, 2028629011, 2161710599]}, {"id": 2895674046, "title": "Adaptive Signal Processing", "abstract": "GENERAL INTRODUCTION. Adaptive Systems. The Adaptive Linear Combiner. THEORY OF ADAPTATION WITH STATIONARY SIGNALS. Properties of the Quadratic Performance Surface. Searching the Performance Surface. Gradient Estimation and Its Effects on Adaptation. ADAPTIVE ALGORITHMS AND STRUCTURES. The LMS Algorithm. The Z-Transform in Adaptive Signal Processing. Other Adaptive Algorithms and Structures. Adaptive Lattice Filters. APPLICATIONS. Adaptive Modeling and System Identification. Inverse Adaptive Modeling, Deconvolution, and Equalization. Adaptive Control Systems. Adaptive Interference Cancelling. Introduction to Adaptive Arrays and Adaptive Beamforming. Analysis of Adaptive Beamformers.", "date": "1984", "authors": ["Bernard Widrow 1, Samuel D. Stearns 2"], "references": [2071707134, 2019207321, 1544329015, 2249133779, 2158518777, 2148138104, 2118776392, 2091881639, 2148982591, 2100677568]}, {"id": 2147800946, "title": "Backpropagation applied to handwritten zip code recognition", "abstract": "The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.", "date": "1989", "authors": ["Y. LeCun , B. Boser , J. S. Denker , D. Henderson , R. E. Howard , W. Hubbard , L. D. Jackel"], "references": [2154642048, 2165758113, 169539560, 19621276, 2101926813, 2157475639, 1965770722, 56903235, 2606594511, 2116360511]}, {"id": 1597286183, "title": "Neural computation of decisions in optimization problems", "abstract": "Highly-interconnected networks of nonlinear analog neurons are shown to be extremely effective in computing. The networks can rapidly provide a collectively-computed solution (a digital output) to a problem on the basis of analog input information. The problems to be solved must be formulated in terms of desired optima, often subject to constraints. The general principles involved in constructing networks to solve specific problems are discussed. Results of computer simulations of a network designed to solve a difficult but well-defined optimization problem-the Traveling-Salesman Problem-are presented and used to illustrate the computational power of the networks. Good solutions to this problem are collectively computed within an elapsed time of only a few neural time constants. The effectiveness of the computation involves both the nonlinear analog response of the neurons and the large connectivity among them. Dedicated networks of biological or microelectronic neurons could provide the computational capabilities described for a wide class of problems having combinatorial complexity. The power and speed naturally displayed by such collective networks may contribute to the effectiveness of biological information processing.", "date": "1985", "authors": ["J. J. Hopfield 1, D. W. Tank 2"], "references": [2581275558, 2011039300, 2293063825, 2177721432, 1666015432, 307896644, 2042986967, 2112325651, 2032533296, 1543738661]}, {"id": 1535810436, "title": "Adaptive switching circuits", "abstract": "", "date": "1987", "authors": ["Bernard Widrow , Marcian E. Hoff"], "references": [2117812871, 2154642048, 2121863487, 2042264548, 114517082, 2166851633, 2025605741, 2604319603, 1570963478]}, {"id": 2173629880, "title": "Phoneme recognition using time-delay neural networks", "abstract": "", "date": "1994", "authors": ["Alexander Waibel , Toshiyuki Hanazawa , Geoffrey Hinton , Kiyohiro Shikano , Kevin J. Lang"], "references": [2217896605, 2973127116, 1606209288, 2970350205, 3007502375, 2019202657, 2098694627, 3016138882, 3115898234]}, {"id": 2076063813, "title": "Deep learning in neural networks", "abstract": "In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning & evolutionary computation, and indirect search for short programs encoding deep and large networks.", "date": "2014", "authors": ["J\u00fcrgen Schmidhuber"], "references": [2618530766, 2151103935, 2097117768, 2102605133, 2130942839, 2156909104, 1663973292, 2136922672, 1849277567, 2963542991]}, {"id": 1924770834, "title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "abstract": "In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.", "date": "2014", "authors": ["Junyoung Chung , \u00c7aglar G\u00fcl\u00e7ehre , KyungHyun Cho , Yoshua Bengio 1, 2, 3"], "references": [2964308564, 2130942839, 2064675550, 2143612262, 1810943226, 2097998348, 2964199361, 1606347560, 2144499799, 2108677974]}, {"id": 1026270304, "title": "Training very deep networks", "abstract": "Theoretical and empirical evidence indicates that the depth of neural networks is crucial for their success. However, training becomes more difficult as depth increases, and training of very deep networks remains an open problem. Here we introduce a new architecture designed to overcome this. Our so-called highway networks allow unimpeded information flow across many layers on information highways. They are inspired by Long Short-Term Memory recurrent networks and use adaptive gating units to regulate the information flow. Even with hundreds of layers, highway networks can be trained directly through simple gradient descent. This enables the study of extremely deep and efficient architectures.", "date": "2015", "authors": ["Rupesh Kumar Srivastava , Klaus Greff , J\u00fcrgen Schmidhuber"], "references": [2618530766, 2962835968, 2097117768, 2155893237, 1677182931, 2136922672, 2064675550, 1533861849, 2294059674, 104184427]}, {"id": 3017143921, "title": "Pattern classification and scene analysis", "abstract": "Provides a unified, comprehensive and up-to-date treatment of both statistical and descriptive methods for pattern recognition. The topics treated include Bayesian decision theory, supervised and unsupervised learning, nonparametric techniques, discriminant analysis, clustering, preprosessing of pictorial data, spatial filtering, shape description techniques, perspective transformations, projective invariants, linguistic procedures, and artificial intelligence techniques for scene analysis.", "date": "1972", "authors": ["Richard O. Duda , Peter E. Hart"], "references": [2310919327, 1746819321, 1570448133, 2163352848, 2067191022, 2110158442, 1992419399, 2117812871, 2121647436]}, {"id": 1966812932, "title": "A Maximum Likelihood Approach to Continuous Speech Recognition", "abstract": "Speech recognition is formulated as a problem of maximum likelihood decoding. This formulation requires statistical models of the speech production process. In this paper, we describe a number of statistical models for use in speech recognition. We give special attention to determining the parameters for such models from sparse data. We also describe two decoding methods, one appropriate for constrained artificial languages and one appropriate for more realistic decoding tasks. To illustrate the usefulness of the methods described, we review a number of decoding results that have been obtained with them.", "date": "1983", "authors": ["Lalit R. Bahl , Frederick Jelinek , Robert L. Mercer"], "references": [2142384583, 1597533204, 1575431606, 2341171179, 2163929346, 2157477135, 2029491572, 2035227369, 2137095888, 1989226853]}, {"id": 2176028050, "title": "Connectionist learning procedures", "abstract": "A major goal of research on networks of neuron-like processing units is to discover efficient learning procedures that allow these networks to construct complex internal representations of their environment. The learning procedures must be capable of modifying the connection strengths in such a way that internal units which are not part of the input or output come to represent important features of the task domain. Several interesting gradient-descent procedures have recently been discovered. Each connection computes the derivative, with respect to the connection strength, of a global measure of the error in the performance of the network. The strength is then adjusted in the direction that decreases the error. These relatively simple, gradient-descent learning procedures work well for small tasks and the new challenge is to find ways of improving their convergence rate and their generalization abilities so that they can be applied to larger, more realistic tasks.", "date": "1990", "authors": ["Geoffrey E. Hinton"], "references": [1497256448, 2581275558, 2154642048, 1498436455, 1997063559, 2049633694, 2293063825, 2895674046, 1597286183, 22297218]}, {"id": 2101926813, "title": "Neocognitron: A Self Organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position", "abstract": "A neural network model for a mechanism of visual pattern recognition is proposed in this paper. The network is self-organized by \u201clearning without a teacher\u201d, and acquires an ability to recognize stimulus patterns based on the geometrical similarity (Gestalt) of their shapes without affected by their positions. This network is given a nickname \u201cneocognitron\u201d. After completion of self-organization, the network has a structure similar to the hierarchy model of the visual nervous system proposed by Hubel and Wiesel. The network consits of an input layer (photoreceptor array) followed by a cascade connection of a number of modular structures, each of which is composed of two layers of cells connected in a cascade. The first layer of each module consists of \u201cS-cells\u201d, which show characteristics similar to simple cells or lower order hypercomplex cells, and the second layer consists of \u201cC-cells\u201d similar to complex cells or higher order hypercomplex cells. The afferent synapses to each S-cell have plasticity and are modifiable. The network has an ability of unsupervised learning: We do not need any \u201cteacher\u201d during the process of self-organization, and it is only needed to present a set of stimulus patterns repeatedly to the input layer of the network. The network has been simulated on a digital computer. After repetitive presentation of a set of stimulus patterns, each stimulus pattern has become to elicit an output only from one of the C-cell of the last layer, and conversely, this C-cell has become selectively responsive only to that stimulus pattern. That is, none of the C-cells of the last layer responds to more than one stimulus pattern. The response of the C-cells of the last layer is not affected by the pattern's position at all. Neither is it affected by a small change in shape nor in size of the stimulus pattern.", "date": "1980", "authors": ["Kunihiko Fukushima"], "references": [2116360511, 2322002063, 2053120614, 1588340522, 1594551768, 2010315761, 2272360941, 22889343, 2324189819, 2091546412]}, {"id": 1991133427, "title": "Error bounds for convolutional codes and an asymptotically optimum decoding algorithm", "abstract": "The probability of error in decoding an optimal convolutional code transmitted over a memoryless channel is bounded from above and below as a function of the constraint length of the code. For all but pathological channels the bounds are asymptotically (exponentially) tight for rates above R_{0} , the computational cutoff rate of sequential decoding. As a function of constraint length the performance of optimal convolutional codes is shown to be superior to that of block codes of the same length, the relative improvement increasing with rate. The upper bound is obtained for a specific probabilistic nonsequential decoding algorithm which is shown to be asymptotically optimum for rates above R_{0} and whose performance bears certain similarities to that of sequential decoding algorithms.", "date": "1967", "authors": ["A. Viterbi"], "references": [2034274945, 1993944611, 2087362480, 2005530146, 1976797517, 1527268325, 1527096151]}, {"id": 1573503290, "title": "Beyond Regression : \"New Tools for Prediction and Analysis in the Behavioral Sciences", "abstract": "", "date": "1973", "authors": ["P. Werbos"], "references": [2618530766, 2076063813, 2117812871, 2141125852, 2019207321, 2170973209, 2132424367, 1586335931]}, {"id": 2048330959, "title": "Cooperative computation of stereo disparity", "abstract": "Perhaps one of the most striking differences between a brain and today\u2019s computers is the amount of \u201cwiring.\u201d In a digital computer the ratio of connections to components is about 3, whereas for the mammalian cortex it lies between 10 and 10,000 (1).", "date": "1987", "authors": ["D. Marr 1, T. Poggio 2"], "references": [2170716495, 1997494543, 2052810501, 2087895317, 2089840306, 1975068880, 1992476998, 1981520343, 2001963156, 1989544735]}, {"id": 2798813531, "title": "Linear Systems", "abstract": "", "date": "1979", "authors": ["Thomas Kailath"], "references": [2134673975, 1506119886, 2099839128, 1521785144, 1588998206, 2065297540, 1986922155, 2012445782, 2912155302, 2130667916]}, {"id": 2098398123, "title": "Non-linear system identification using neural networks", "abstract": "Multi-layered neural networks offer an exciting alternative for modelling complex non-linear systems. This paper investigates the identification of discrete-time nonlinear systems using neural networks with a single hidden layer. New parameter estimation algorithms are derived for the neural network model based on a prediction error formulation and the application to both simulated and real data is included to demonstrate the effectiveness of the neural network approach.", "date": "1989", "authors": ["S. Chen 1, S. A. Billings 2, Peter Grant 1"], "references": [2154642048, 1652505363, 2103496339, 1971735090, 1540723801, 1603277681, 2102380305, 1650765400, 2125812768, 1535689967]}, {"id": 2144499799, "title": "Supervised Sequence Labelling with Recurrent Neural Networks", "abstract": "Recurrent neural networks are powerful sequence learners. They are able to incorporate context information in a flexible way, and are robust to localised distortions of the input data. These properties make them well suited to sequence labelling, where input sequences are transcribed with streams of labels. The aim of this thesis is to advance the state-of-the-art in supervised sequence labelling with recurrent networks. Its two main contributions are (1) a new type of output layer that allows recurrent networks to be trained directly for sequence labelling tasks where the alignment between the inputs and the labels is unknown, and (2) an extension of the long short-term memory network architecture to multidimensional data, such as images and video sequences.", "date": "2012", "authors": ["Alexander Graves"], "references": [2156909104, 1663973292, 2136922672, 2310919327, 2064675550, 1554663460, 2147880316, 2125838338, 2110798204, 1993882792]}, {"id": 2136848157, "title": "Learning to Forget: Continual Prediction with LSTM", "abstract": "Long short-term memory (LSTM; Hochreiter & Schmidhuber, 1997) can solve numerous tasks not solvable by previous learning algorithms for recurrent neural networks (RNNs). We identify a weakness of LSTM networks processing continual input streams that are not a priori segmented into subsequences with explicitly marked ends at which the network's internal state could be reset. Without resets, the state may grow indefinitely and eventually cause the network to break down. Our remedy is a novel, adaptive \"forget gate\" that enables an LSTM cell to learn to reset itself at appropriate times, thus releasing internal resources. We review illustrative benchmark problems on which standard LSTM outperforms other RNN algorithms. All algorithms (including LSTM) fail to solve continual versions of these problems. LSTM with forget gates, however, easily solves them, and in an elegant way.", "date": "2000", "authors": ["Felix A. Gers , J\u00fcrgen A. Schmidhuber , Fred A. Cummins"], "references": [2064675550, 2107878631, 2154890045, 1959983357, 194249466, 2103452139, 1674799117, 1971129545, 2121029939, 2057653135]}, {"id": 1735317348, "title": "Recurrent Network Models for Human Dynamics", "abstract": "We propose the Encoder-Recurrent-Decoder (ERD) model for recognition and prediction of human body pose in videos and motion capture. The ERD model is a recurrent neural network that incorporates nonlinear encoder and decoder networks before and after recurrent layers. We test instantiations of ERD architectures in the tasks of motion capture (mocap) generation, body pose labeling and body pose forecasting in videos. Our model handles mocap training data across multiple subjects and activity domains, and synthesizes novel motions while avoiding drifting for long periods of time. For human pose labeling, ERD outperforms a per frame body part detector by resolving left-right body part confusions. For video pose forecasting, ERD predicts body joint displacements across a temporal horizon of 400ms and outperforms a first order motion model based on optical flow. ERDs extend previous Long Short Term Memory (LSTM) models in the literature to jointly learn representations and their dynamics. Our experiments show such representation learning is crucial for both labeling and prediction in space-time. We find this is a distinguishing feature between the spatio-temporal visual domain in comparison to 1D text, speech or handwriting, where straightforward hard coded representations have shown excellent results when directly combined with recurrent units [31].", "date": "2015", "authors": ["Katerina Fragkiadaki , Sergey Levine , Panna Felsen , Jitendra Malik"], "references": [2618530766, 2130942839, 2100495367, 2064675550, 1895577753, 2145094598, 1810943226, 2113325037, 2136391815, 196214544]}, {"id": 2079735306, "title": "2005 Special Issue: Framewise phoneme classification with bidirectional LSTM and other neural network architectures", "abstract": "In this paper, we present bidirectional Long Short Term Memory (LSTM) networks, and a modified, full gradient version of the LSTM learning algorithm. We evaluate Bidirectional LSTM (BLSTM) and several other network architectures on the benchmark task of framewise phoneme classification, using the TIMIT database. Our main findings are that bidirectional networks outperform unidirectional ones, and Long Short Term Memory (LSTM) is much faster and also more accurate than both standard Recurrent Neural Nets (RNNs) and time-windowed Multilayer Perceptrons (MLPs). Our results support the view that contextual information is crucial to speech processing, and suggest that BLSTM is an effective architecture with which to exploit it.", "date": "2005", "authors": ["Alex Graves 1, J\u00fcrgen Schmidhuber 2"], "references": [2064675550, 1554663460, 2131774270, 2147568880, 1553004968, 1485231155, 2619993508, 1525783482, 2110871230, 1566256432]}, {"id": 2147568880, "title": "Learning precise timing with lstm recurrent networks", "abstract": "The temporal distance between events conveys information essential for numerous sequential tasks such as motor control and rhythm detection. While Hidden Markov Models tend to ignore this information, recurrent neural networks (RNNs) can in principle learn to make use of it. We focus on Long Short-Term Memory (LSTM) because it has been shown to outperform other RNNs on tasks involving long time lags. We find that LSTM augmented by \"peephole connections\" from its internal cells to its multiplicative gates can learn the fine distinction between sequences of spikes spaced either 50 or 49 time steps apart without the help of any short training exemplars. Without external resets or teacher forcing, our LSTM variant also learns to generate stable streams of precisely timed spikes and other highly nonlinear periodic patterns. This makes LSTM a promising approach for tasks that require the accurate measurement or generation of time intervals.", "date": "2003", "authors": ["Felix A. Gers 1, Nicol N. Schraudolph 2, J\u00fcrgen Schmidhuber 1"], "references": [2064675550, 2107878631, 2136848157, 2914484425, 2016589492, 1525783482, 2154890045, 194249466, 1674799117, 2121029939]}, {"id": 3099873379, "title": "Neural NILM: Deep Neural Networks Applied to Energy Disaggregation", "abstract": "Energy disaggregation estimates appliance-by-appliance electricity consumption from a single meter that measures the whole home's electricity demand. Recently, deep neural networks have driven remarkable improvements in classification performance in neighbouring machine learning fields such as image classification and automatic speech recognition. In this paper, we adapt three deep neural network architectures to energy disaggregation: 1) a form of recurrent neural network called `long short-term memory' (LSTM); 2) denoising autoencoders; and 3) a network which regresses the start time, end time and average power demand of each appliance activation. We use seven metrics to test the performance of these algorithms on real aggregate power data from five appliances. Tests are performed against a house not seen during training and against houses seen during training. We find that all three neural nets achieve better F1 scores (averaged over all five appliances) than either combinatorial optimisation or factorial hidden Markov models and that our neural net algorithms generalise well to an unseen house.", "date": "2015", "authors": ["Jack Kelly , William Knottenbelt"], "references": [2618530766, 2145339207, 2130942839, 2136922672, 2310919327, 2064675550, 2124386111, 2025768430, 1810943226, 2154642048]}, {"id": 2057175746, "title": "Shape matching and object recognition using shape contexts", "abstract": "We present a novel approach to measuring similarity between shapes and exploit it for object recognition. In our framework, the measurement of similarity is preceded by: (1) solving for correspondences between points on the two shapes; (2) using the correspondences to estimate an aligning transform. In order to solve the correspondence problem, we attach a descriptor, the shape context, to each point. The shape context at a reference point captures the distribution of the remaining points relative to it, thus offering a globally discriminative characterization. Corresponding points on two similar shapes will have similar shape contexts, enabling us to solve for correspondences as an optimal assignment problem. Given the point correspondences, we estimate the transformation that best aligns the two shapes; regularized thin-plate splines provide a flexible class of transformation maps for this purpose. The dissimilarity between the two shapes is computed as a sum of matching errors between corresponding points, together with a term measuring the magnitude of the aligning transform. We treat recognition in a nearest-neighbor classification framework as the problem of finding the stored prototype shape that is maximally similar to that in the image. Results are presented for silhouettes, trademarks, handwritten digits, and the COIL data set.", "date": "2002", "authors": ["S. Belongie 1, J. Malik 2, J. Puzicha 3"], "references": [2310919327, 2124386111, 2119821739, 2138451337, 2117812871, 2038952578, 2124087378, 2123977795, 2101522199, 2146766088]}, {"id": 2159080219, "title": "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference", "abstract": "From the Publisher: Probabilistic Reasoning in Intelligent Systems is a complete andaccessible account of the theoretical foundations and computational methods that underlie plausible reasoning under uncertainty. The author provides a coherent explication of probability as a language for reasoning with partial belief and offers a unifying perspective on other AI approaches to uncertainty, such as the Dempster-Shafer formalism, truth maintenance systems, and nonmonotonic logic. The author distinguishes syntactic and semantic approaches to uncertainty\u0097and offers techniques, based on belief networks, that provide a mechanism for making semantics-based systems operational. Specifically, network-propagation techniques serve as a mechanism for combining the theoretical coherence of probability theory with modern demands of reasoning-systems technology: modular declarative inputs, conceptually meaningful inferences, and parallel distributed computation. Application areas include diagnosis, forecasting, image interpretation, multi-sensor fusion, decision support systems, plan recognition, planning, speech recognition\u0097in short, almost every task requiring that conclusions be drawn from uncertain clues and incomplete information. Probabilistic Reasoning in Intelligent Systems will be of special interest to scholars and researchers in AI, decision theory, statistics, logic, philosophy, cognitive psychology, and the management sciences. Professionals in the areas of knowledge-based systems, operations research, engineering, and statistics will find theoretical and computational tools of immediate practical use. The book can also be used as an excellent text for graduate-level courses in AI, operations research, or applied probability.", "date": "1987", "authors": ["Judea Pearl"], "references": [2581275558, 1997063559, 1593793857, 2797148637, 2155322595, 158727920, 2138162238, 2108309071, 1986808060, 2142901448]}, {"id": 2131686571, "title": "Fields of Experts: a framework for learning image priors", "abstract": "We develop a framework for learning generic, expressive image priors that capture the statistics of natural scenes and can be used for a variety of machine vision tasks. The approach extends traditional Markov random field (MRF) models by learning potential functions over extended pixel neighborhoods. Field potentials are modeled using a Products-of-Experts framework that exploits nonlinear functions of many linear filter responses. In contrast to previous MRF approaches all parameters, including the linear filters themselves, are learned from training data. We demonstrate the capabilities of this Field of Experts model with two example applications, image denoising and image inpainting, which are implemented using a simple, approximate inference scheme. While the model is trained on a generic image database and is not tuned toward a specific application, we obtain results that compete with and even outperform specialized techniques.", "date": "2005", "authors": ["S. Roth , M.J. Black"], "references": [2133665775, 2116064496, 3110653090, 1997063559, 2121927366, 2113945798, 2295936755, 2116013899, 2149760002, 2105464873]}, {"id": 2567948266, "title": "A view of the EM algorithm that justifies incremental, sparse, and other variants", "abstract": "The EM algorithm performs maximum likelihood estimation for data in which some variables are unobserved. We present a function that resembles negative free energy and show that the M step maximizes this function with respect to the model parameters and the E step maximizes it with respect to the distribution over the unobserved variables. From this perspective, it is easy to justify an incremental variant of the EM algorithm in which the distribution for only one of the unobserved variables is recalculated in each E step. This variant is shown empirically to give faster convergence in a mixture estimation problem. A variant of the algorithm that exploits sparse conditional distributions is also described, and a wide range of other variant algorithms are also seen to be possible.", "date": "1998", "authors": ["Radford M. Neal , Geoffrey E. Hinton"], "references": [2049633694, 2117853077, 2024476015, 1580495158, 1991278573, 581152777]}, {"id": 2124914669, "title": "Exponential Family Harmoniums with an Application to Information Retrieval", "abstract": "Directed graphical models with one layer of observed random variables and one or more layers of hidden random variables have been the dominant modelling paradigm in many research fields. Although this approach has met with considerable success, the causal semantics of these models can make it difficult to infer the posterior distribution over the hidden variables. In this paper we propose an alternative two-layer model based on exponential family distributions and the semantics of undirected models. Inference in these \"exponential family harmoniums\" is fast while learning is performed by minimizing contrastive divergence. A member of this family is then studied as an alternative probabilistic model for latent semantic indexing. In experiments it is shown that they perform well on document retrieval tasks and provide an elegant solution to searching with keywords.", "date": "2004", "authors": ["Max Welling 1, Michal Rosen-zvi 1, Geoffrey E. Hinton 2"], "references": [1880262756, 2116064496, 2147152072, 1612003148, 2140124448, 1934021597, 2138448681, 145818128, 2109720450, 1813659000]}, {"id": 2081580037, "title": "WordNet: a lexical database for English", "abstract": "Because meaningful sentences are composed of meaningful words, any system that hopes to process natural languages as people do must have information about words and their meanings. This information is traditionally provided through dictionaries, and machine-readable dictionaries are now widely available. But dictionary entries evolved for the convenience of human readers, not for machines. WordNet 1 provides a more effective combination of traditional lexicographic information and modern computing. WordNet is an online lexical database designed for use under program control. English nouns, verbs, adjectives, and adverbs are organized into sets of synonyms, each representing a lexicalized concept. Semantic relations link the synonym sets [4].", "date": "1995", "authors": ["George A. Miller"], "references": [2102381086, 2103318667, 2065157922, 1483126227, 2017668967, 1518768680, 13823885]}, {"id": 2096192494, "title": "On the quantitative analysis of deep belief networks", "abstract": "Deep Belief Networks (DBN's) are generative models that contain many layers of hidden variables. Efficient greedy algorithms for learning and approximate inference have allowed these models to be applied successfully in many application domains. The main building block of a DBN is a bipartite undirected graphical model called a restricted Boltzmann machine (RBM). Due to the presence of the partition function, model selection, complexity control, and exact maximum likelihood learning in RBM's are intractable. We show that Annealed Importance Sampling (AIS) can be used to efficiently estimate the partition function of an RBM, and we present a novel AIS scheme for comparing RBM's with different architectures. We further show how an AIS estimator, along with approximate inference, can be used to estimate a lower bound on the log-probability that a DBN model with multiple hidden layers assigns to the test data. This is, to our knowledge, the first step towards obtaining quantitative results that would allow us to directly assess the performance of Deep Belief Networks as generative models of data.", "date": "2008", "authors": ["Ruslan Salakhutdinov , Iain Murray"], "references": [2136922672, 2100495367, 2116064496, 2099866409, 2169415915, 2158164339, 66838807, 2064630666, 1513873506, 2135094946]}, {"id": 2165225968, "title": "Unsupervised learning of distributions on binary vectors using two layer networks", "abstract": "We present a distribution model for binary vectors, called the influence combination model and show how this model can be used as the basis for unsupervised learning algorithms for feature selection. The model can be represented by a particular type of Boltzmann machine with a bipartite graph structure that we call the combination machine. This machine is closely related to the Harmonium model defined by Smolensky. In the first part of the paper we analyze properties of this distribution representation scheme. We show that arbitrary distributions of binary vectors can be approximated by the combination model. We show how the weight vectors in the model can be interpreted as high order correlation patterns among the input bits, and how the combination machine can be used as a mechanism for detecting these patterns. We compare the combination model with the mixture model and with principle component analysis. In the second part of the paper we present two algorithms for learning the combination model from examples. The first learning algorithm is the standard gradient ascent heuristic for computing maximum likelihood estimates for the parameters of the model. Here we give a closed form for this gradient that is significantly easier to compute than the corresponding gradient for the general Boltzmann machine. The second learning algorithm is a greedy method that creates the hidden units and computes their weights one at a time. This method is a variant of projection pursuit density estimation. In the third part of the paper we give experimental results for these learning methods on synthetic data and on natural data of handwritten digit images.", "date": "1991", "authors": ["Yoav Freund , David Haussler"], "references": [1652505363, 2159080219, 1997063559, 2049633694, 2293063825, 1507849272, 1964724001, 2121407732, 2725061391, 2315016682]}, {"id": 2156909104, "title": "The Nature of Statistical Learning Theory", "abstract": "Setting of the learning problem consistency of learning processes bounds on the rate of convergence of learning processes controlling the generalization ability of learning processes constructing learning algorithms what is important in learning theory?.", "date": "1994", "authors": ["Vladimir N. Vapnik"], "references": [2140190241, 2148603752, 2164278908, 2310919327, 2129812935, 2076063813, 1746819321, 1570448133, 2072128103, 2139212933]}, {"id": 2296616510, "title": "Compressed sensing", "abstract": "Suppose x is an unknown vector in Ropfm (a digital image or signal); we plan to measure n general linear functionals of x and then reconstruct. If x is known to be compressible by transform coding with a known transform, and we reconstruct via the nonlinear procedure defined here, the number of measurements n can be dramatically smaller than the size m. Thus, certain natural classes of images with m pixels need only n=O(m1/4log5/2(m)) nonadaptive nonpixel samples for faithful recovery, as opposed to the usual m pixel samples. More specifically, suppose x has a sparse representation in some orthonormal basis (e.g., wavelet, Fourier) or tight frame (e.g., curvelet, Gabor)-so the coefficients belong to an lscrp ball for 0<ples1. The N most important coefficients in that expansion allow reconstruction with lscr2 error O(N1/2-1p/). It is possible to design n=O(Nlog(m)) nonadaptive measurements allowing reconstruction with accuracy comparable to that attainable with direct knowledge of the N most important coefficients. Moreover, a good approximation to those N important coefficients is extracted from the n measurements by solving a linear program-Basis Pursuit in signal processing. The nonadaptive measurements have the character of \"random\" linear combinations of basis/frame elements. Our results use the notions of optimal recovery, of n-widths, and information-based complexity. We estimate the Gel'fand n-widths of lscrp balls in high-dimensional Euclidean space in the case 0<ples1, and give a criterion identifying near- optimal subspaces for Gel'fand n-widths. We show that \"most\" subspaces are near-optimal, and show that convex optimization (Basis Pursuit) is a near-optimal way to extract information derived from these near-optimal subspaces", "date": "2003", "authors": ["D.L. Donoho"], "references": [2296319761, 2145096794, 2115755118, 2129638195, 2129131372, 2135046866, 2062024414, 2078204800, 2798909945, 2138019504]}, {"id": 2187089797, "title": "Visualizing Data using t-SNE", "abstract": "We present a new technique called \u201ct-SNE\u201d that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images of objects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large datasets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of datasets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualizations produced by t-SNE are significantly better than those produced by the other techniques on almost all of the datasets.", "date": "2007", "authors": ["Laurens van der Maaten , Geoffrey Hinton"], "references": [2100495367, 2072128103, 2053186076, 2001141328, 2156718197, 2125637308, 2139823104, 2137570937, 2157444450, 1742512077]}, {"id": 2158847908, "title": "The Proposition Bank: An Annotated Corpus of Semantic Roles", "abstract": "The Proposition Bank project takes a practical approach to semantic representation, adding a layer of predicate-argument information, or semantic role labels, to the syntactic structures of the Penn Treebank. The resulting resource can be thought of as shallow, in that it does not represent coreference, quantification, and many other higher-order phenomena, but also broad, in that it covers every instance of every verb in the corpus and allows representative statistics to be calculated.We discuss the criteria used to define the sets of semantic roles used in the annotation process and to analyze the frequency of syntactic/semantic alternations in the corpus. We describe an automatic system for semantic role tagging trained on the corpus and discuss the effect on its performance of various types of information, including a comparison of full syntactic parsing with a flat representation and the contribution of the empty ''trace'' categories of the treebank.", "date": "2005", "authors": ["Martha Palmer 1, Daniel Gildea 2, Paul Kingsbury 1"], "references": [1632114991, 1535015163, 2092654472, 2151170651, 2115792525, 3021452258, 2039217078, 1567277581, 2126851059, 2154626406]}, {"id": 2130903752, "title": "A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data", "abstract": "One of the most important issues in machine learning is whether one can improve the performance of a supervised learning algorithm by including unlabeled data. Methods that use both labeled and unlabeled data are generally referred to as semi-supervised learning. Although a number of such methods are proposed, at the current stage, we still don't have a complete understanding of their effectiveness. This paper investigates a closely related problem, which leads to a novel approach to semi-supervised learning. Specifically we consider learning predictive structures on hypothesis spaces (that is, what kind of classifiers have good predictive power) from multiple learning tasks. We present a general framework in which the structural learning problem can be formulated and analyzed theoretically, and relate it to learning with unlabeled data. Under this framework, algorithms for structural learning will be proposed, and computational issues will be investigated. Experiments will be given to demonstrate the effectiveness of the proposed algorithms in the semi-supervised learning setting.", "date": "2005", "authors": ["Rie Kubota Ando , Tong Zhang"], "references": [2148603752, 1480376833, 2154455818, 2139823104, 2048679005, 2097089247, 2107008379, 2914746235, 2010353172, 2101210369]}, {"id": 2107008379, "title": "Transductive Inference for Text Classification using Support Vector Machines", "abstract": "", "date": "1999", "authors": ["Thorsten Joachims"], "references": [2158899491, 3112605745, 2964015378, 2117130368, 2118020653, 1995903777, 1479807131, 2104290444, 2150102617, 2136504847]}, {"id": 2914746235, "title": "Multitask learning", "abstract": "Multitask Learning is an approach to inductive transfer that improves learning for one task by using the information contained in the training signals of other related tasks. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better. In this thesis we demonstrate multitask learning for a dozen problems. We explain how multitask learning works and show that there are many opportunities for multitask learning in real domains. We show that in some cases features that would normally be used as inputs work better if used as multitask outputs instead. We present suggestions for how to get the most out of multitask learning in artificial neural nets, present an algorithm for multitask learning with case-based methods like k-nearest neighbor and kernel regression, and sketch an algorithm for multitask learning in decision trees. Multitask learning improves generalization performance, can be applied in many different kinds of domains, and can be used with different learning algorithms. We conjecture there will be many opportunities for its use on real-world problems.", "date": "1998", "authors": ["Rich Caruana"], "references": [1536680647, 2076063813, 3112605745, 2117130368, 2098411764, 2184188583, 2114315281, 2150341604, 1512387364, 2122922389]}, {"id": 2885050925, "title": "Shallow Semantic Parsing using Support Vector Machines.", "abstract": "", "date": "2003", "authors": ["Sameer S. Pradhan , Wayne H. Ward , Kadri Hacioglu , James H. Martin , Daniel Jurafsky"], "references": [2092654472, 2151170651, 2166776180, 1520377376, 1988995507, 2145310422, 2155693943, 2093647425, 2040909025, 2150203234]}, {"id": 2158823144, "title": "Dynamic conditional random fields: factorized probabilistic models for labeling and segmenting sequence data", "abstract": "In sequence modeling, we often wish to represent complex interaction between labels, such as when performing multiple, cascaded labeling tasks on the same sequence, or when long-range dependencies exist. We present dynamic conditional random fields (DCRFs), a generalization of linear-chain conditional random fields (CRFs) in which each time slice contains a set of state variables and edges---a distributed state representation as in dynamic Bayesian networks (DBNs)---and parameters are tied across slices. Since exact inference can be intractable in such models, we perform approximate inference using several schedules for belief propagation, including tree-based reparameterization (TRP). On a natural-language chunking task, we show that a DCRF performs better than a series of linear-chain CRFs, achieving comparable performance using only half the training data.", "date": "2004", "authors": ["Charles Sutton , Khashayar Rohanimanesh , Andrew McCallum"], "references": [2147880316, 2116064496, 2125838338, 1574901103, 1632114991, 2008652694, 2096175520, 2169415915, 1934019294, 2156515921]}, {"id": 2163568299, "title": "Effective Self-Training for Parsing", "abstract": "We present a simple, but surprisingly effective, method of self-training a two-phase parser-reranker system using readily available unlabeled data. We show that this type of bootstrapping is possible for parsing when the bootstrapped parses are processed by a discriminative reranker. Our improved model achieves an f-score of 92.1%, an absolute 1.1% improvement (12% error reduction) over the previous best result for Wall Street Journal parsing. Finally, we provide some analysis to better understand the phenomenon.", "date": "2006", "authors": ["David McClosky , Eugene Charniak , Mark Johnson"], "references": [1632114991, 2048679005, 1535015163, 2158195707, 3021452258, 1567570606, 2125712079, 2729906263, 2098379588, 1859173823]}, {"id": 2153663612, "title": "Image Denoising Via Sparse and Redundant Representations Over Learned Dictionaries", "abstract": "We address the image denoising problem, where zero-mean white and homogeneous Gaussian additive noise is to be removed from a given image. The approach taken is based on sparse and redundant representations over trained dictionaries. Using the K-SVD algorithm, we obtain a dictionary that describes the image content effectively. Two training options are considered: using the corrupted image itself, or training on a corpus of high-quality image database. Since the K-SVD is limited in handling small image patches, we extend its deployment to arbitrary image sizes by defining a global image prior that forces sparsity over patches in every location in the image. We show how such Bayesian treatment leads to a simple and effective denoising algorithm. This leads to a state-of-the-art denoising performance, equivalent and sometimes surpassing recently published leading alternative denoising methods", "date": "2006", "authors": ["M. Elad , M. Aharon"], "references": [2160547390, 2078204800, 2146842127, 2158940042, 2151693816, 2097323375, 2113945798, 2132680427, 2079724595, 2069912449]}, {"id": 2293063825, "title": "Neural networks and physical systems with emergent collective computational abilities", "abstract": "Computational properties of use to biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.", "date": "1999", "authors": ["John J. Hopfield"], "references": [1594524188, 2086789740, 2970228278, 2076870593]}, {"id": 2172174689, "title": "Efficient Learning of Sparse Representations with an Energy-Based Model", "abstract": "We describe a novel unsupervised method for learning sparse, overcomplete features. The model uses a linear encoder, and a linear decoder preceded by a sparsifying non-linearity that turns a code vector into a quasi-binary sparse code vector. Given an input, the optimal code minimizes the distance between the output of the decoder and the input patch while being as similar as possible to the encoder output. Learning proceeds in a two-phase EM-like fashion: (1) compute the minimum-energy code vector, (2) adjust the parameters of the encoder and decoder so as to decrease the energy. The model produces \"stroke detectors\" when trained on handwritten numerals, and Gabor-like filters when trained on natural image patches. Inference and learning are very fast, requiring no preprocessing, and no expensive sampling. Using the proposed unsupervised method to initialize the first layer of a convolutional network, we achieved an error rate slightly lower than the best reported result on the MNIST dataset. Finally, an extension of the method is described to learn topographical filter maps.", "date": "2006", "authors": ["Marc'aurelio Ranzato , Christopher Poultney , Sumit Chopra , Yann L. Cun"], "references": [2136922672, 2310919327, 2116064496, 1902027874, 2156163116, 2105464873, 1802356529, 2075187489, 2102409316, 11828546]}, {"id": 2613634265, "title": "Scaling learning algorithms towards AI", "abstract": "One long-term goal of machine learning research is to produce methods that are applicable to highly complex tasks, such as perception (vision, audition), reasoning, intelligent control, and other artificially intelligent behaviors. We argue that in order to progress toward this goal, the Machine Learning community must endeavor to discover algorithms that can learn highly complex functions, with minimal need for prior knowledge, and with minimal human intervention. We present mathematical and empirical evidence suggesting that many popular approaches to non-parametric learning, particularly kernel methods, are fundamentally limited in their ability to learn complex high-dimensional functions. Our analysis focuses on two problems. First, kernel machines are shallow architectures, in which one large layer of simple template matchers is followed by a single layer of trainable coefficients. We argue that shallow architectures can be very inefficient in terms of required number of computational elements and examples. Second, we analyze a limitation of kernel machines with a local kernel, linked to the curse of dimensionality, that applies to supervised, unsupervised (manifold learning) and semi-supervised kernel machines. Using empirical results on invariant image recognition tasks, kernel methods are compared with deep architectures, in which lower-level features or concepts are progressively combined into more abstract and higher-level representations. We argue that deep architectures have the potential to generalize in non-local ways, i.e., beyond immediate neighbors, and that this is crucial in order to make progress on the kind of complex tasks required for artificial intelligence.", "date": "2006", "authors": ["Yoshua Bengio 1, 2, 3, Yann Lecun"], "references": [2136922672, 2148603752, 2310919327, 2119821739, 2053186076, 2001141328, 2116064496, 2057175746, 2110798204, 2140095548]}, {"id": 1993845689, "title": "The \"Wake-Sleep\" Algorithm for Unsupervised Neural Networks", "abstract": "An unsupervised learning algorithm for a multilayer network of stochastic neurons is described. Bottom-up \"recognition\" connections convert the input into representations in successive hidden layers, and top-down \"generative\" connections reconstruct the representation in one layer from the representation in the layer above. In the \"wake\" phase, neurons are driven by recognition connections, and generative connections are adapted to increase the probability that they would reconstruct the correct activity vector in the layer below. In the \"sleep\" phase, neurons are driven by generative connections, and recognition connections are adapted to increase the probability that they would produce the correct activity vector in the layer above.", "date": "1995", "authors": ["Geoffrey E. Hinton , Peter Dayan , Brendan J. Frey , Radford M. Neal"], "references": [2740373864, 2177040213, 1533169541, 2044875682, 94647076]}, {"id": 2109779438, "title": "The Cascade-Correlation Learning Architecture", "abstract": "Cascade-Correlation is a new architecture and supervised learning algorithm for artificial neural networks. Instead of just adjusting the weights in a network of fixed topology. Cascade-Correlation begins with a minimal network, then automatically trains and adds new hidden units one by one, creating a multi-layer structure. Once a new hidden unit has been added to the network, its input-side weights are frozen. This unit then becomes a permanent feature-detector in the network, available for producing outputs or for creating other, more complex feature detectors. The Cascade-Correlation architecture has several advantages over existing algorithms: it learns very quickly, the network determines its own size and topology, it retains the structures it has built even if the training set changes, and it requires no back-propagation of error signals through the connections of the network.", "date": "1988", "authors": ["Scott E. Fahlman , Christian Lebiere"], "references": [2154642048, 1652505363, 2160208155, 19621276, 3121126077, 2160699933, 50076749, 2127385318, 2169163929, 2112957975]}, {"id": 2103626435, "title": "Practical Issues in Temporal Difference Learning", "abstract": "This paper examines whether temporal difference methods for training connectionist networks, such as Sutton's TD(\u03bb) algorithm, can be successfully applied to complex real-world problems. A number of important practical issues are identified and discussed from a general theoretical perspective. These practical issues are then examined in the context of a case study in which TD(\u03bb) is applied to learning the game of backgammon from the outcome of self-play. This is apparently the first application of this algorithm to a complex non-trivial task. It is found that, with zero knowledge built in, the network is able to learn from scratch to play the entire game at a fairly strong intermediate level of performance, which is clearly better than conventional commercial programs, and which in fact surpasses comparable networks trained on a massive human expert data set. This indicates that TD learning may work better in practice than one would expect based on current theory, and it suggests that further analysis of TD methods, as well as applications in other complex domains, may be worth investigating.", "date": "1992", "authors": ["Gerald Tesauro"], "references": [2154642048, 1652505363, 2137983211, 2100677568, 1583833196, 2178806388, 2154952480, 2159047538, 1569296262, 192142699]}, {"id": 2125569215, "title": "The Curse of Highly Variable Functions for Local Kernel Machines", "abstract": "We present a series of theoretical arguments supporting the claim that a large class of modern learning algorithms that rely solely on the smoothness prior - with similarity between examples expressed with a local kernel - are sensitive to the curse of dimensionality, or more precisely to the variability of the target. Our discussion covers supervised, semi-supervised and unsupervised learning algorithms. These algorithms are found to be local in the sense that crucial properties of the learned function at x depend mostly on the neighbors of x in the training set. This makes them sensitive to the curse of dimensionality, well studied for classical non-parametric statistical learning. We show in the case of the Gaussian kernel that when the function to be learned has many variations, these algorithms require a number of training examples proportional to the number of variations, which could be large even though there may exist short descriptions of the target function, i.e. their Kolmogorov complexity may be low. This suggests that there exist non-local learning algorithms that at least have the potential to learn about such structured but apparently complex functions (because locally they have many variations), while not using very specific prior domain knowledge.", "date": "2005", "authors": ["Yoshua Bengio , Olivier Delalleau , Nicolas L. Roux"], "references": [2119821739, 2053186076, 2001141328, 2140095548, 2087347434, 2154455818, 2139823104, 1604938182, 3017143921, 2160167256]}, {"id": 2130313186, "title": "Continuous restricted Boltzmann machine with an implementable training algorithm", "abstract": "The authors introduce a continuous stochastic generative model that can model continuous data, with a simple and reliable training algorithm. The architecture is a continuous restricted Boltzmann machine, with one step of Gibbs sampling, to minimise contrastive divergence, replacing a time-consuming relaxation search. With a small approximation, the training algorithm requires only addition and multiplication and is thus computationally inexpensive in both software and hardware. The capabilities of the model are demonstrated and explored with both artificial and real data.", "date": "2003", "authors": ["H. Chen , A.F. Murray"], "references": [2116064496, 2177721432, 145818128, 2079182758, 2103369448, 2121014581, 2158969192, 2085779969, 1538323098, 2105659727]}, {"id": 2322002063, "title": "Principles of neurodynamics", "abstract": "", "date": "1961", "authors": ["A. A. Mullin , Frank Rosenblatt"], "references": [2076063813, 2119821739, 1498436455, 1570963478, 2144499799, 1873332500, 2155653793, 2148138104, 2147169507]}, {"id": 2144081223, "title": "Coot: model-building tools for molecular graphics.", "abstract": "CCP4mg is a project that aims to provide a general-purpose tool for structural biologists, providing tools for X-ray structure solution, structure comparison and analysis, and publication-quality graphics. The map-fitting tools are available as a stand-alone package, distributed as `Coot'.", "date": "2004", "authors": ["Paul Emsley , Kevin Cowtan"], "references": [2130479394, 2013083986, 2135839939, 1986830449, 1969222787]}, {"id": 2111211467, "title": "New Algorithms and Methods to Estimate Maximum-Likelihood Phylogenies: Assessing the Performance of PhyML 3.0", "abstract": "PhyML is a phylogeny software based on the maximum-likelihood principle. Early PhyML versions used a fast algorithm performing nearest neighbor interchanges to improve a reasonable starting tree topology. Since the original publication (Guindon S., Gascuel O. 2003. A simple, fast and accurate algorithm to estimate large phylogenies by maximum likelihood. Syst. Biol. 52:696-704), PhyML has been widely used (>2500 citations in ISI Web of Science) because of its simplicity and a fair compromise between accuracy and speed. In the meantime, research around PhyML has continued, and this article describes the new algorithms and methods implemented in the program. First, we introduce a new algorithm to search the tree space with user-defined intensity using subtree pruning and regrafting topological moves. The parsimony criterion is used here to filter out the least promising topology modifications with respect to the likelihood function. The analysis of a large collection of real nucleotide and amino acid data sets of various sizes demonstrates the good performance of this method. Second, we describe a new test to assess the support of the data for internal branches of a phylogeny. This approach extends the recently proposed approximate likelihood-ratio test and relies on a nonparametric, Shimodaira-Hasegawa-like procedure. A detailed analysis of real alignments sheds light on the links between this new approach and the more classical nonparametric bootstrap method. Overall, our tests show that the last version (3.0) of PhyML is fast, accurate, stable, and ready to use. A Web server and binary files are available from http://www.atgc-montpellier.fr/phyml/.", "date": "2010", "authors": ["St\u00e9phane Guindon 1, Jean-Fran\u00e7ois Dufayard 2, Vincent Lefort 1, Maria Anisimova 2, Wim Hordijk 2, Olivier Gascuel 1"], "references": [2146058063, 2168696662, 2103546861, 2031611770, 2127847431, 2030966943, 2103088017, 2098448352, 2105926960, 2112956791]}, {"id": 2470646526, "title": "SARS and MERS: recent insights into emerging coronaviruses", "abstract": "The emergence of Middle East respiratory syndrome coronavirus (MERS-CoV) in 2012 marked the second introduction of a highly pathogenic coronavirus into the human population in the twenty-first century. The continuing introductions of MERS-CoV from dromedary camels, the subsequent travel-related viral spread, the unprecedented nosocomial outbreaks and the high case-fatality rates highlight the need for prophylactic and therapeutic measures. Scientific advancements since the 2002-2003 severe acute respiratory syndrome coronavirus (SARS-CoV) pandemic allowed for rapid progress in our understanding of the epidemiology and pathogenesis of MERS-CoV and the development of therapeutics. In this Review, we detail our present understanding of the transmission and pathogenesis of SARS-CoV and MERS-CoV, and discuss the current state of development of measures to combat emerging coronaviruses.", "date": "2016", "authors": ["Emmie de Wit 1, Neeltje van Doremalen 1, Darryl Falzarano 2, Vincent J. Munster 1"], "references": []}, {"id": 2306794997, "title": "Epidemiology, Genetic Recombination, and Pathogenesis of Coronaviruses", "abstract": "Human coronaviruses (HCoVs) were first described in the 1960s for patients with the common cold. Since then, more HCoVs have been discovered, including those that cause severe acute respiratory syndrome (SARS) and Middle East respiratory syndrome (MERS), two pathogens that, upon infection, can cause fatal respiratory disease in humans. It was recently discovered that dromedary camels in Saudi Arabia harbor three different HCoV species, including a dominant MERS HCoV lineage that was responsible for the outbreaks in the Middle East and South Korea during 2015. In this review we aim to compare and contrast the different HCoVs with regard to epidemiology and pathogenesis, in addition to the virus evolution and recombination events which have, on occasion, resulted in outbreaks amongst humans.", "date": "2016", "authors": ["Shuo Su 1, Gary Wong 2, Weifeng Shi 3, Jun Liu 2, 4, Alexander C.K. Lai 5, Jiyong Zhou 1, Wenjun Liu 2, Yuhai Bi 2, George F. Gao 6"], "references": [2166867592, 2025170735, 2006434809, 2129542667, 1993577573, 2138324310, 2125251240, 2160011624, 2115555188, 2134061616]}, {"id": 1993577573, "title": "Isolation and characterization of a bat SARS-like coronavirus that uses the ACE2 receptor", "abstract": "The 2002-3 pandemic caused by severe acute respiratory syndrome coronavirus (SARS-CoV) was one of the most significant public health events in recent history. An ongoing outbreak of Middle East respiratory syndrome coronavirus suggests that this group of viruses remains a key threat and that their distribution is wider than previously recognized. Although bats have been suggested to be the natural reservoirs of both viruses, attempts to isolate the progenitor virus of SARS-CoV from bats have been unsuccessful. Diverse SARS-like coronaviruses (SL-CoVs) have now been reported from bats in China, Europe and Africa, but none is considered a direct progenitor of SARS-CoV because of their phylogenetic disparity from this virus and the inability of their spike proteins to use the SARS-CoV cellular receptor molecule, the human angiotensin converting enzyme II (ACE2). Here we report whole-genome sequences of two novel bat coronaviruses from Chinese horseshoe bats (family: Rhinolophidae) in Yunnan, China: RsSHC014 and Rs3367. These viruses are far more closely related to SARS-CoV than any previously identified bat coronaviruses, particularly in the receptor binding domain of the spike protein. Most importantly, we report the first recorded isolation of a live SL-CoV (bat SL-CoV-WIV1) from bat faecal samples in Vero E6 cells, which has typical coronavirus morphology, 99.9% sequence identity to Rs3367 and uses ACE2 from humans, civets and Chinese horseshoe bats for cell entry. Preliminary in vitro testing indicates that WIV1 also has a broad species tropism. Our results provide the strongest evidence to date that Chinese horseshoe bats are natural reservoirs of SARS-CoV, and that intermediate hosts may not be necessary for direct human infection by some bat SL-CoVs. They also highlight the importance of pathogen-discovery programs targeting high-risk wildlife groups in emerging disease hotspots as a strategy for pandemic preparedness.", "date": "2013", "authors": ["Xing Yi Ge 1, Jia Lu Li 1, Xing Lou Yang 1, Aleksei A. Chmura 2, Guangjian Zhu 2, Jonathan H. Epstein 2, Jonna A Mazet 3, Ben Hu 1, Wei Zhang 1, Cheng Peng 1, Yu Ji Zhang 1, Chu Ming Luo 1, Bing Tan 1, Ning Wang 1, Yan Zhu 1, Gary Crameri 4, Shu Yi Zhang 5, Lin Fa Wang 4, 6, Peter Daszak 2, Zheng Li Shi 1"], "references": [2166867592, 2104548316, 2119111857, 1966238900, 2103503670, 2141008678, 2140338292, 2049975503, 2101063972, 1990059132]}, {"id": 2775086803, "title": "Discovery of a rich gene pool of bat SARS-related coronaviruses provides new insights into the origin of SARS coronavirus.", "abstract": "A large number of SARS-related coronaviruses (SARSr-CoV) have been detected in horseshoe bats since 2005 in different areas of China. However, these bat SARSr-CoVs show sequence differences from SARS coronavirus (SARS-CoV) in different genes (S, ORF8, ORF3, etc) and are considered unlikely to represent the direct progenitor of SARS-CoV. Herein, we report the findings of our 5-year surveillance of SARSr-CoVs in a cave inhabited by multiple species of horseshoe bats in Yunnan Province, China. The full-length genomes of 11 newly discovered SARSr-CoV strains, together with our previous findings, reveals that the SARSr-CoVs circulating in this single location are highly diverse in the S gene, ORF3 and ORF8. Importantly, strains with high genetic similarity to SARS-CoV in the hypervariable N-terminal domain (NTD) and receptor-binding domain (RBD) of the S1 gene, the ORF3 and ORF8 region, respectively, were all discovered in this cave. In addition, we report the first discovery of bat SARSr-CoVs highly similar to human SARS-CoV in ORF3b and in the split ORF8a and 8b. Moreover, SARSr-CoV strains from this cave were more closely related to SARS-CoV in the non-structural protein genes ORF1a and 1b compared with those detected elsewhere. Recombination analysis shows evidence of frequent recombination events within the S gene and around the ORF8 between these SARSr-CoVs. We hypothesize that the direct progenitor of SARS-CoV may have originated after sequential recombination events between the precursors of these SARSr-CoVs. Cell entry studies demonstrated that three newly identified SARSr-CoVs with different S protein sequences are all able to use human ACE2 as the receptor, further exhibiting the close relationship between strains in this cave and SARS-CoV. This work provides new insights into the origin and evolution of SARS-CoV and highlights the necessity of preparedness for future emergence of SARS-like diseases.", "date": "2017", "authors": ["Ben Hu 1, Lei Ping Zeng 1, Xing Lou Yang 1, Xing Yi Ge 1, Wei Zhang 1, Bei Li 1, Jia Zheng Xie 1, Xu Rui Shen 1, Yun Zhi Zhang 2, Ning Wang 1, Dong Sheng Luo 1, Xiao Shuang Zheng 1, Mei Niang Wang 1, Peter Daszak 3, Lin Fa Wang 4, Jie Cui 1, Zheng Li Shi 1"], "references": [2111211467, 1993577573, 2169198329, 2195009776, 2103503670, 2134061616, 2298153446, 2141008678, 2140338292, 2141877163]}, {"id": 2119111857, "title": "Dipeptidyl peptidase 4 is a functional receptor for the emerging human coronavirus-EMC", "abstract": "Most human coronaviruses cause mild upper respiratory tract disease but may be associated with more severe pulmonary disease in immunocompromised individuals. However, SARS coronavirus caused severe lower respiratory disease with nearly 10% mortality and evidence of systemic spread. Recently, another coronavirus (human coronavirus-Erasmus Medical Center (hCoV-EMC)) was identified in patients with severe and sometimes lethal lower respiratory tract infection. Viral genome analysis revealed close relatedness to coronaviruses found in bats. Here we identify dipeptidyl peptidase 4 (DPP4; also known as CD26) as a functional receptor for hCoV-EMC. DPP4 specifically co-purified with the receptor-binding S1 domain of the hCoV-EMC spike protein from lysates of susceptible Huh-7 cells. Antibodies directed against DPP4 inhibited hCoV-EMC infection of primary human bronchial epithelial cells and Huh-7 cells. Expression of human and bat (Pipistrellus pipistrellus) DPP4 in non-susceptible COS-7 cells enabled infection by hCoV-EMC. The use of the evolutionarily conserved DPP4 protein from different species as a functional receptor provides clues about the host range potential of hCoV-EMC. In addition, it will contribute critically to our understanding of the pathogenesis and epidemiology of this emerging human coronavirus, and may facilitate the development of intervention strategies.", "date": "2013", "authors": ["V. Stalin Raj 1, Huihui Mou 2, Saskia L. Smits 1, Dick H. W. Dekkers 1, Marcel A. M\u00fcller 3, Ronald Dijkman 4, Doreen Muth 3, Jeroen A. A. Demmers 1, Ali Zaki 5, Ron A. M. Fouchier 1, Volker Thiel 4, 6, Christian Drosten 3, Peter J. M. Rottier 2, Albert D. M. E. Osterhaus 1, Berend Jan Bosch 2, Bart L. Haagmans 1"], "references": [2166867592, 1966238900, 2103503670, 2113457186, 1690366459, 1982533785, 2091671824, 2126707939, 1964982019, 2101063972]}, {"id": 2025170735, "title": "Coronavirus as a possible cause of severe acute respiratory syndrome", "abstract": "Summary Background An outbreak of severe acute respiratory syndrome (SARS) has been reported in Hong Kong. We investigated the viral cause and clinical presentation among 50 patients. Methods We analysed case notes and microbiological findings for 50 patients with SARS, representing more than five separate epidemiologically linked transmission clusters. We defined the clinical presentation and risk factors associated with severe disease and investigated the causal agents by chest radiography and laboratory testing of nasopharyngeal aspirates and sera samples. We compared the laboratory findings with those submitted for microbiological investigation of other diseases from patients whose identity was masked. Findings Patients' age ranged from 23 to 74 years. Fever, chills, myalgia, and cough were the most frequent complaints. When compared with chest radiographic changes, respiratory symptoms and auscultatory findings were disproportionally mild. Patients who were household contacts of other infected people and had older age, lymphopenia, and liver dysfunction were associated with severe disease. A virus belonging to the family Coronaviridae was isolated from two patients. By use of serological and reverse-transcriptase PCR specific for this virus, 45 of 50 patients with SARS, but no controls, had evidence of infection with this virus. Interpretation A coronavirus was isolated from patients with SARS that might be the primary agent associated with this disease. Serological and molecular tests specific for the virus permitted a definitive laboratory diagnosis to be made and allowed further investigation to define whether other cofactors play a part in disease progression.", "date": "2003", "authors": ["Jsm Peiris 1, ST Lai 2, Llm Poon 1, Y Guan 1, Lyc Yam 3, W Lim 4, J Nicholls 1, Wks Yee 5, WW Yan 2, MT Cheung 3, Vcc Cheng 1, KH Chan 1, Dnc Tsang 6, Rwh Yung 3, TK Ng 2, KY Yuen 1"], "references": [2116682907, 2122399224, 2104730345, 2141291230, 2154664055, 1970720481, 576359727, 2239493136, 2081510963, 2169726092]}, {"id": 2129542667, "title": "Clinical progression and viral load in a community outbreak of coronavirus-associated SARS pneumonia: a prospective study.", "abstract": "Summary Background We investigated the temporal progression of the clinical, radiological, and virological changes in a community outbreak of severe acute respiratory syndrome (SARS). Methods We followed up 75 patients for 3 weeks managed with a standard treatment protocol of ribavirin and corticosteroids, and assessed the pattern of clinical disease, viral load, risk factors for poor clinical outcome, and the usefulness of virological diagnostic methods. Findings Fever and pneumonia initially improved but 64 (85%) patients developed recurrent fever after a mean of 8\u00b79 (SD 3\u00b71) days, 55 (73%) had watery diarrhoea after 7\u00b75 (2\u00b73) days, 60 (80%) had radiological worsening after 7\u00b74 (2\u00b72) days, and respiratory symptoms worsened in 34 (45%) after 8\u00b76 (3\u00b70) days. In 34 (45%) patients, improvement of initial pulmonary lesions was associated with appearance of new radiological lesions at other sites. Nine (12%) patients developed spontaneous pneumomediastinum and 15 (20%) developed acute respiratory distress syndrome (ARDS) in week 3. Quantitative reverse-transcriptase (RT) PCR of nasopharyngeal aspirates in 14 patients (four with ARDS) showed peak viral load at day 10, and at day 15 a load lower than at admission. Age and chronic hepatitis B virus infection treated with lamivudine were independent significant risk factors for progression to ARDS (p=0\u00b7001). SARS-associated coronavirus in faeces was seen on RT-PCR in 65 (97%) of 67 patients at day 14. The mean time to seroconversion was 20 days. Interpretation The consistent clinical progression, shifting radiological infiltrates, and an inverted V viral-load profile suggest that worsening in week 2 is unrelated to uncontrolled viral replication but may be related to immunopathological damage.", "date": "2003", "authors": ["J S M Peiris 1, C M Chu 2, V C C Cheng 1, K S Chan 2, I F N Hung 1, L L M Poon 1, K I Law 2, B S F Tang 1, T Y W Hon 2, C S Chan 2, K H Chan 1, J S C Ng 2, B J Zheng 1, W L Ng 2, R W M Lai 2, Y Guan 1, Kwok-Yung Yuen 1"], "references": [2104548316, 2025170735, 2131262274, 2100820722, 2125251240, 2107978811, 2161328469, 2155583106, 1675164605, 2061759246]}, {"id": 1703839189, "title": "Detection of a novel human coronavirus by real-time reverse-transcription polymerase chain reaction", "abstract": "We present two real-time reverse-transcription polymerase chain reaction assays for a novel human coronavirus (CoV), targeting regions upstream of the E gene (upE) or within open reading frame (ORF)1b, respectively. Sensitivity for upE is 3.4 copies per reaction (95% confidence interval (CI): 2.5-6.9 copies) or 291 copies/mL of sample. No cross-reactivity was observed with coronaviruses OC43, NL63, 229E, SARS-CoV, nor with 92 clinical specimens containing common human respiratory viruses. We recommend using upE for screening and ORF1b for confirmation. .", "date": "2012", "authors": ["V M Corman 1, I Eckerle 1, T Bleicker 1, A Zaki 2, O Landt 3, M Eschbach-Bludau 1, S van Boheemen 4, R Gopal 5, M Ballhause 3, T M Bestebroer 4, D Muth 1, M A M\u00fcller 1, J F Drexler 1, M Zambon 5, A D Osterhaus 4, R M Fouchier 4, C Drosten 1"], "references": [2132260239, 2129542667, 2167080692, 2069961370, 1593955729, 1975169783, 2145810580, 2105870155, 2100516702, 2161315652]}, {"id": 2116586125, "title": "Characterization of a novel coronavirus associated with severe acute respiratory syndrome", "abstract": "In March 2003, a novel coronavirus (SARS-CoV) was discovered in association with cases of severe acute respiratory syndrome (SARS). The sequence of the complete genome of SARS-CoV was determined, and the initial characterization of the viral genome is presented in this report. The genome of SARS-CoV is 29,727 nucleotides in length and has 11 open reading frames, and its genome organization is similar to that of other coronaviruses. Phylogenetic analyses and sequence comparisons showed that SARS-CoV is not closely related to any of the previously characterized coronaviruses.", "date": "2003", "authors": ["Paul A. Rota 1, M. Steven Oberste 1, Stephan S. Monroe 1, W. Allan Nix 1, Ray Campagnoli 1, Joseph P. Icenogle 1, Silvia Pe\u00f1aranda 1, Bettina Bankamp 1, Kaija Maher 1, Min hsin Chen 1, Suxiong Tong 1, Azaibi Tamin 1, Luis Lowe 1, Michael Frace 1, Joseph L. DeRisi 2, Qi Chen 1, David Wang 2, Dean D. Erdman 1, Teresa C.T. Peret 1, Cara Burns 1, Thomas G. Ksiazek 1, Pierre E. Rollin 1, Anthony Sanchez 1, Stephanie Liffick 1, Brian Holloway 1, Josef Limor 1, Karen McCaustland 1, Mellissa Olsen-Rasmussen 1, Ron Fouchier 3, Stephan G\u00fcnther 4, Albert D.H.E. Osterhaus 3, Christian Drosten 4, Mark A. Pallansch 1, Larry J. Anderson 1, William J. Bellini 1"], "references": [2025170735, 2169198329, 2163400707, 2103854602, 2048093932, 2152215476, 72835126, 2027659163, 2126980050, 2097622821]}, {"id": 1987783718, "title": "Extracorporeal membrane oxygenation for 2009 Influenza A (H1N1) Acute Respiratory Distress Syndrome", "abstract": "CONTEXT The novel influenza A(H1N1) pandemic affected Australia and New Zealand during the 2009 southern hemisphere winter. It caused an epidemic of critical illness and some patients developed severe acute respiratory distress syndrome (ARDS) and were treated with extracorporeal membrane oxygenation (ECMO). OBJECTIVES To describe the characteristics of all patients with 2009 influenza A(H1N1)-associated ARDS treated with ECMO and to report incidence, resource utilization, and patient outcomes. DESIGN, SETTING, AND PATIENTS An observational study of all patients (n = 68) with 2009 influenza A(H1N1)-associated ARDS treated with ECMO in 15 intensive care units (ICUs) in Australia and New Zealand between June 1 and August 31, 2009. MAIN OUTCOME MEASURES Incidence, clinical features, degree of pulmonary dysfunction, technical characteristics, duration of ECMO, complications, and survival. RESULTS Sixty-eight patients with severe influenza-associated ARDS were treated with ECMO, of whom 61 had either confirmed 2009 influenza A(H1N1) (n = 53) or influenza A not subtyped (n = 8), representing an incidence rate of 2.6 ECMO cases per million population. An additional 133 patients with influenza A received mechanical ventilation but no ECMO in the same ICUs. The 68 patients who received ECMO had a median (interquartile range [IQR]) age of 34.4 (26.6-43.1) years and 34 patients (50%) were men. Before ECMO, patients had severe respiratory failure despite advanced mechanical ventilatory support with a median (IQR) Pao(2)/fraction of inspired oxygen (Fio(2)) ratio of 56 (48-63), positive end-expiratory pressure of 18 (15-20) cm H(2)O, and an acute lung injury score of 3.8 (3.5-4.0). The median (IQR) duration of ECMO support was 10 (7-15) days. At the time of reporting, 48 of the 68 patients (71%; 95% confidence interval [CI], 60%-82%) had survived to ICU discharge, of whom 32 had survived to hospital discharge and 16 remained as hospital inpatients. Fourteen patients (21%; 95% CI, 11%-30%) had died and 6 remained in the ICU, 2 of whom were still receiving ECMO. CONCLUSIONS During June to August 2009 in Australia and New Zealand, the ICUs at regional referral centers provided mechanical ventilation for many patients with 2009 influenza A(H1N1)-associated respiratory failure, one-third of whom received ECMO. These ECMO-treated patients were often young adults with severe hypoxemia and had a 21% mortality rate at the end of the study period.", "date": "2009", "authors": ["Andrew Davies 1, Daryl Jones 1, Michael Bailey 1, John Beca 2, Rinaldo Bellomo 1, Nikki Blackwell 3, Paul Forrest 4, David Gattas 4, Emily Granger 5, Robert Herkes 4, Andrew Jackson 5, Shay McGuinness 2, Priya Nair 5, Vincent Pellegrino 1, Ville Yrjo Olavi Pettila 1, Brian Plunkett 4, Roger Pye 5, Paul Torzillo 4, Steven Webb 6, Michael Wilson 4, Marc Ziegenfuss 3"], "references": [2166867592, 3011611380, 2610582798, 2037145800, 3012186724, 2105123265, 2094826575, 2563391002, 2048608755, 2101328245]}, {"id": 1963953102, "title": "Fingerprinting genomes using PCR with arbitrary primers", "abstract": "Simple and reproducible fingerprints of complex genomes can be generated using single arbitrarily chosen primers and the polymerase chain reaction (PCR). No prior sequence information is required. The method, arbitrarily primed PCR (AP-PCR), involves two cycles of low stringency amplification followed by PCR at higher stringency. We show that strains can be distinguished by comparing polymorphisms in genomic fingerprints. The generality of the method is demonstrated by application to twenty four strains from five species of Staphylococcus, eleven strains of Streptococcus pyogenes and three varieties of Oryza sativa (rice).", "date": "1989", "authors": ["John Welsh , Michael McClelland"], "references": [2166867592, 2170881661, 2113457186, 589702940, 2029197798, 1986937402, 2078917493, 2141047744, 2145525392, 2037210253]}, {"id": 2111412754, "title": "Identification of a new human coronavirus", "abstract": "Three human coronaviruses are known to exist: human coronavirus 229E (HCoV-229E), HCoV-OC43 and severe acute respiratory syndrome (SARS)-associated coronavirus (SARS-CoV). Here we report the identification of a fourth human coronavirus, HCoV-NL63, using a new method of virus discovery. The virus was isolated from a 7-month-old child suffering from bronchiolitis and conjunctivitis. The complete genome sequence indicates that this virus is not a recombinant, but rather a new group 1 coronavirus. The in vitro host cell range of HCoV-NL63 is notable because it replicates on tertiary monkey kidney cells and the monkey kidney LLC-MK2 cell line. The viral genome contains distinctive features, including a unique N-terminal fragment within the spike protein. Screening of clinical specimens from individuals suffering from respiratory illness identified seven additional HCoV-NL63-infected individuals, indicating that the virus was widely spread within the human population.", "date": "2004", "authors": ["Lia van der Hoek 1, Krzysztof Pyrc 1, Maarten F Jebbink 1, Wilma Vermeulen-Oost 2, Ron J M Berkhout 2, Katja C Wolthers 1, Pauline M E Wertheim-van Dillen 1, Jos Kaandorp 3, Joke Spaargaren 2, Ben Berkhout 1"], "references": [2132260239, 2104548316, 2129542667, 2116586125, 2169198329, 2134061616, 2166229810, 2163400707, 2159857626, 2029293367]}, {"id": 2170933940, "title": "Characterization and Complete Genome Sequence of a Novel Coronavirus, Coronavirus HKU1, from Patients with Pneumonia", "abstract": "Despite extensive laboratory investigations in patients with respiratory tract infections, no microbiological cause can be identified in a significant proportion of patients. In the past 3 years, several novel respiratory viruses, including human metapneumovirus, severe acute respiratory syndrome (SARS) coronavirus (SARSCoV), and human coronavirus NL63, were discovered. Here we report the discovery of another novel coronavirus, coronavirus HKU1 (CoV-HKU1), from a 71-year-old man with pneumonia who had just returned from Shenzhen, China. Quantitative reverse transcription-PCR showed that the amount of CoV-HKU1 RNA was 8.5 to 9.6 10 6 copies per ml in his nasopharyngeal aspirates (NPAs) during the first week of the illness and dropped progressively to undetectable levels in subsequent weeks. He developed increasing serum levels of specific antibodies against the recombinant nucleocapsid protein of CoV-HKU1, with immunoglobulin M (IgM) titers of 1:20, 1:40, and 1:80 and IgG titers of <1:1,000, 1:2,000, and 1:8,000 in the first, second and fourth weeks of the illness, respectively. Isolation of the virus by using various cell lines, mixed neuron-glia culture, and intracerebral inoculation of suckling mice was unsuccessful. The complete genome sequence of CoV-HKU1 is a 29,926-nucleotide, polyadenylated RNA, with GC content of 32%, the lowest among all known coronaviruses with available genome sequence. Phylogenetic analysis reveals that CoV-HKU1 is a new group 2 coronavirus. Screening of 400 NPAs, negative for SARS-CoV, from patients with respiratory illness during the SARS period identified the presence of CoV-HKU1 RNA in an additional specimen, with a viral load of 1.13 10 6 copies per ml, from a 35-year-old woman with pneumonia. Our data support the existence of a novel group 2 coronavirus associated with pneumonia in humans.", "date": "2005", "authors": ["Patrick C. Y. Woo 1, Susanna K. P. Lau 1, Chung-ming Chu 2, Kwok-hung Chan 3, Hoi-wah Tsoi 3, Yi Huang 3, Beatrice H. L. Wong 3, Rosana W. S. Poon 3, James J. Cai 3, Wei-kwang Luk 4, Leo L. M. Poon 1, Samson S. Y. Wong 1, Yi Guan 1, J. S. Malik Peiris 1, Kwok-yung Yuen 1"], "references": [2141885858, 2025170735, 2129542667, 2116586125, 2169198329, 2171091522, 2170881661, 2134061616, 2111412754, 2141877163]}, {"id": 2181908191, "title": "Virus taxonomy: classification and nomenclature of viruses. Seventh report of the International Committee on Taxonomy of Viruses.", "abstract": "The practical need to partition the world of viruses into distinguishable, universally agreed upon entities is the ultimate justification for developing a virus classification system. Since 1971, the International Committee on Taxonomy of Viruses (ICTV) operating on behalf of the world community of virologists has taken on the task of developing a single, universal taxonomic scheme for all viruses infecting animals (vertebrate, invertebrates, and protozoa), plants (higher plants and algae), fungi, bacteria, and archaea. The current report builds on the accumulated taxonomic construction of the eight previous reports dating back to 1971 and records the proceedings of the Committee since publication of the last report in 2005. Representing the work of more than 500 virologists worldwide, this report is the authoritative reference for virus organization, distinction, and structure.", "date": "1999", "authors": ["M. H. V. Van Regenmortel"], "references": [2166867592, 2023092952, 2410066853, 1517000857, 1963749198, 1998643471, 2763547223, 2261246381, 2127081791, 2081524511]}, {"id": 2107277218, "title": "ANALYSIS OF RELATIVE GENE EXPRESSION DATA USING REAL-TIME QUANTITATIVE PCR AND THE 2(-DELTA DELTA C(T)) METHOD", "abstract": "The two most commonly used methods to analyze data from real-time, quantitative PCR experiments are absolute quantification and relative quantification. Absolute quantification determines the input copy number, usually by relating the PCR signal to a standard curve. Relative quantification relates the PCR signal of the target transcript in a treatment group to that of another sample such as an untreated control. The 2(-Delta Delta C(T)) method is a convenient way to analyze the relative changes in gene expression from real-time quantitative PCR experiments. The purpose of this report is to present the derivation, assumptions, and applications of the 2(-Delta Delta C(T)) method. In addition, we present the derivation and applications of two variations of the 2(-Delta Delta C(T)) method that may be useful in the analysis of real-time, quantitative PCR data.", "date": "2001", "authors": ["Kenneth J. Livak 1, Thomas D. Schmittgen 2"], "references": [2164578725, 2109970232, 2128088040, 2145879504, 1983241347, 2123325948, 2134343377, 1756433044, 1566892773, 2069943574]}, {"id": 2565805236, "title": "Middle East Respiratory Syndrome Coronavirus (MERS-CoV)", "abstract": "", "date": "2016", "authors": ["Geethamma Jolly"], "references": [3001118548, 3000413850, 2138324310, 2610356532, 3014604938, 3007866917, 1843578997, 3007537254, 2766931063, 2115555188]}, {"id": 2791599184, "title": "Treatment of Middle East Respiratory Syndrome with a combination of lopinavir-ritonavir and interferon-\u03b21b (MIRACLE trial): study protocol for a randomized controlled trial.", "abstract": "It had been more than 5 years since the first case of Middle East Respiratory Syndrome coronavirus infection (MERS-CoV) was recorded, but no specific treatment has been investigated in randomized clinical trials. Results from in vitro and animal studies suggest that a combination of lopinavir/ritonavir and interferon-\u03b21b (IFN-\u03b21b) may be effective against MERS-CoV. The aim of this study is to investigate the efficacy of treatment with a combination of lopinavir/ritonavir and recombinant IFN-\u03b21b provided with standard supportive care, compared to treatment with placebo provided with standard supportive care in patients with laboratory-confirmed MERS requiring hospital admission. The protocol is prepared in accordance with the SPIRIT (Standard Protocol Items: Recommendations for Interventional Trials) guidelines. Hospitalized adult patients with laboratory-confirmed MERS will be enrolled in this recursive, two-stage, group sequential, multicenter, placebo-controlled, double-blind randomized controlled trial. The trial is initially designed to include 2 two-stage components. The first two-stage component is designed to adjust sample size and determine futility stopping, but not efficacy stopping. The second two-stage component is designed to determine efficacy stopping and possibly readjustment of sample size. The primary outcome is 90-day mortality. This will be the first randomized controlled trial of a potential treatment for MERS. The study is sponsored by King Abdullah International Medical Research Center, Riyadh, Saudi Arabia. Enrollment for this study began in November 2016, and has enrolled thirteen patients as of Jan 24-2018. ClinicalTrials.gov, ID: NCT02845843. Registered on 27 July 2016.", "date": "2018", "authors": ["Yaseen M. Arabi 1, 2, 3, Adel Alothman 2, 3, Hanan H. Balkhy 2, 3, Abdulaziz Al-Dawood 2, 3, Sameera AlJohani 2, 3, Shmeylan Al Harbi 2, 3, Suleiman Kojan 2, 3, Majed Al Jeraisy 2, 3, Ahmad M. Deeb 2, 3, Abdullah M. Assiri 4, Fahad Al-Hameed 2, 3, Asim AlSaedi 2, 3, Yasser Mandourah 5, Ghaleb A. Almekhlafi 5, Nisreen Murad Sherbeeni 6, Fatehi Elnour Elzein 6, Javed Memon 7, Yusri Taha 8, Abdullah Almotairi 9, Khalid A. Maghrabi 10, Ismael Qushmaq 11, Ali Al Bshabshe 12, Ayman Kharaba 13, Sarah Shalhoub 14, Jesna Jose 2, Robert A. Fowler 15, 16, Frederick G. Hayden 17, Mohamed A. Hussein 2"], "references": [2166867592, 2145758369, 2139937737, 2591646177, 1703839189, 2586093485, 2034462612, 1977050884, 2150120685, 2345375456]}, {"id": 2255243349, "title": "Coronaviruses - drug discovery and therapeutic options.", "abstract": "Severe acute respiratory syndrome (SARS) and Middle East respiratory syndrome (MERS), which are caused by coronaviruses, have attracted substantial attention owing to their high mortality rates and potential to cause epidemics. Yuen and colleagues discuss progress with treatment options for these syndromes, including virus- and host-targeted drugs, and the challenges that need to be overcome in their further development. In humans, infections with the human coronavirus (HCoV) strains HCoV-229E, HCoV-OC43, HCoV-NL63 and HCoV-HKU1 usually result in mild, self-limiting upper respiratory tract infections, such as the common cold. By contrast, the CoVs responsible for severe acute respiratory syndrome (SARS) and Middle East respiratory syndrome (MERS), which were discovered in Hong Kong, China, in 2003, and in Saudi Arabia in 2012, respectively, have received global attention over the past 12 years owing to their ability to cause community and health-care-associated outbreaks of severe infections in human populations. These two viruses pose major challenges to clinical management because there are no specific antiviral drugs available. In this Review, we summarize the epidemiology, virology, clinical features and current treatment strategies of SARS and MERS, and discuss the discovery and development of new virus-based and host-based therapeutic options for CoV infections.", "date": "2016", "authors": ["Alimuddin Zumla 1, Jasper F. W. Chan 2, Esam I. Azhar 3, David S. C. Hui 4, Kwok-Yung Yuen 2"], "references": [2166867592, 2132260239, 2104548316, 2107053896, 2025170735, 2131262274, 2006434809, 2129542667, 1993577573, 2138324310]}, {"id": 2292021561, "title": "Therapeutic efficacy of the small molecule GS-5734 against Ebola virus in rhesus monkeys", "abstract": "The most recent Ebola virus outbreak in West Africa, which was unprecedented in the number of cases and fatalities, geographic distribution, and number of nations affected, highlights the need for safe, effective, and readily available antiviral agents for treatment and prevention of acute Ebola virus (EBOV) disease (EVD) or sequelae. No antiviral therapeutics have yet received regulatory approval or demonstrated clinical efficacy. Here we report the discovery of a novel small molecule GS-5734, a monophosphoramidate prodrug of an adenosine analogue, with antiviral activity against EBOV. GS-5734 exhibits antiviral activity against multiple variants of EBOV and other filoviruses in cell-based assays. The pharmacologically active nucleoside triphosphate (NTP) is efficiently formed in multiple human cell types incubated with GS-5734 in vitro, and the NTP acts as an alternative substrate and RNA-chain terminator in primer-extension assays using a surrogate respiratory syncytial virus RNA polymerase. Intravenous administration of GS-5734 to nonhuman primates resulted in persistent NTP levels in peripheral blood mononuclear cells (half-life, 14\u2009h) and distribution to sanctuary sites for viral replication including testes, eyes, and brain. In a rhesus monkey model of EVD, once-daily intravenous administration of 10\u2009mg\u2009kg(-1) GS-5734 for 12 days resulted in profound suppression of EBOV replication and protected 100% of EBOV-infected animals against lethal disease, ameliorating clinical disease signs and pathophysiological markers, even when treatments were initiated three days after virus exposure when systemic viral RNA was detected in two out of six treated animals. These results show the first substantive post-exposure protection by a small-molecule antiviral compound against EBOV in nonhuman primates. The broad-spectrum antiviral activity of GS-5734 in vitro against other pathogenic RNA viruses, including filoviruses, arenaviruses, and coronaviruses, suggests the potential for wider medical use. GS-5734 is amenable to large-scale manufacturing, and clinical studies investigating the drug safety and pharmacokinetics are ongoing.", "date": "2016", "authors": ["Travis K. Warren 1, Robert Jordan , Michael K. Lo 2, Adrian S. Ray , Richard L. Mackman , Veronica Soloveva 1, Dustin Siegel , Michel Perron , Roy Bannister , Hon C. Hui , Nate Larson , Robert Strickley , Jay Wells 1, Kelly S. Stuthman 1, Sean A. Van Tongeren 1, Nicole L. Garza 1, Ginger Donnelly 1, Amy C. Shurtleff 1, Cary J. Retterer 1, Dima Gharaibeh 1, Rouzbeh Zamani 1, Tara Kenny 1, Brett P. Eaton 1, Elizabeth Grimes 1, Lisa S. Welch 1, Laura Gomba 1, Catherine L. Wilhelmsen 1, Donald K. Nichols 1, Jonathan E. Nuss 1, Elyse R. Nagle 1, Jeffrey R. Kugelman 1, Gustavo Palacios 1, Edward Doerffler , Sean Neville , Ernest Carra , Michael O. Clarke , Lijun Zhang , Willard Lew , Bruce Ross , Queenie Wang , Kwon Chun , Lydia Wolfe , Darius Babusis , Yeojin Park , Kirsten M. Stray , Iva Trancheva , Joy Y. Feng , Ona Barauskas , Yili Xu , Pamela Wong +12"], "references": [2094993149, 2314681741, 1971292277, 2326558924, 277041599, 2072202424, 2117671399, 1975876487, 2030160453, 2138192885]}, {"id": 2290466312, "title": "A simple practice guide for dose conversion between animals and human.", "abstract": "Understanding the concept of extrapolation of dose between species is important for pharmaceutical researchers when initiating new animal or human experiments. Interspecies allometric scaling for dose conversion from animal to human studies is one of the most controversial areas in clinical pharmacology. Allometric approach considers the differences in body surface area, which is associated with animal weight while extrapolating the doses of therapeutic agents among the species. This review provides basic information about translation of doses between species and estimation of starting dose for clinical trials using allometric scaling. The method of calculation of injection volume for parenteral formulation based on human equivalent dose is also briefed.", "date": "2016", "authors": ["Anroop B Nair 1, Shery Jacob 2"], "references": [2024938615, 2142366069, 2100521244, 2065093669, 2080335269, 2034194552, 2160483062, 2057666951]}, {"id": 2034462612, "title": "Treatment with interferon-\u03b12b and ribavirin improves outcome in MERS-CoV\u2013infected rhesus macaques", "abstract": "The emergence of Middle East respiratory syndrome coronavirus (MERS-CoV) is of global concern: the virus has caused severe respiratory illness, with 111 confirmed cases and 52 deaths at the time of this article's publication. Therapeutic interventions have not been evaluated in vivo; thus, patient management relies exclusively on supportive care, which, given the high case-fatality rate, is not highly effective. The rhesus macaque is the only known model organism for MERS-CoV infection, developing an acute localized to widespread pneumonia with transient clinical disease that recapitulates mild to moderate human MERS-CoV cases. The combination of interferon-\u03b12b and ribavirin was effective in reducing MERS-CoV replication in vitro; therefore, we initiated this treatment 8 h after inoculation of rhesus macaques. In contrast to untreated, infected macaques, treated animals did not develop breathing abnormalities and showed no or very mild radiographic evidence of pneumonia. Moreover, treated animals showed lower levels of systemic (serum) and local (lung) proinflammatory markers, in addition to fewer viral genome copies, distinct gene expression and less severe histopathological changes in the lungs. Taken together, these data suggest that treatment of MERS-CoV infected rhesus macaques with IFN-\u03b12b and ribavirin reduces virus replication, moderates the host response and improves clinical outcome. As these two drugs are already used in combination in the clinic for other infections, IFN-\u03b12b and ribavirin should be considered for the management of MERS-CoV cases.", "date": "2013", "authors": ["Darryl Falzarano 1, Emmie de Wit 1, Angela L Rasmussen 2, Friederike Feldmann 1, Atsushi Okumura 2, Dana P Scott 1, Doug Brining 1, Trenton Bushmaker 1, Cynthia Martellaro 1, Laura Baseler 3, Arndt G Benecke 2, 4, Michael G Katze 2, Vincent J Munster 1, Heinz Feldmann 1, 5"], "references": [2166867592, 2129542667, 1703839189, 2113457186, 2163627712, 2150120685, 2054076340, 2073234751, 2038576929, 1984326550]}, {"id": 1967300023, "title": "Acute Kidney Injury Network: report of an initiative to improve outcomes in acute kidney injury.", "abstract": "Acute kidney injury (AKI) is a complex disorder for which currently there is no accepted definition. Having a uniform standard for diagnosing and classifying AKI would enhance our ability to manage these patients. Future clinical and translational research in AKI will require collaborative networks of investigators drawn from various disciplines, dissemination of information via multidisciplinary joint conferences and publications, and improved translation of knowledge from pre-clinical research. We describe an initiative to develop uniform standards for defining and classifying AKI and to establish a forum for multidisciplinary interaction to improve care for patients with or at risk for AKI. Members representing key societies in critical care and nephrology along with additional experts in adult and pediatric AKI participated in a two day conference in Amsterdam, The Netherlands, in September 2005 and were assigned to one of three workgroups. Each group's discussions formed the basis for draft recommendations that were later refined and improved during discussion with the larger group. Dissenting opinions were also noted. The final draft recommendations were circulated to all participants and subsequently agreed upon as the consensus recommendations for this report. Participating societies endorsed the recommendations and agreed to help disseminate the results. The term AKI is proposed to represent the entire spectrum of acute renal failure. Diagnostic criteria for AKI are proposed based on acute alterations in serum creatinine or urine output. A staging system for AKI which reflects quantitative changes in serum creatinine and urine output has been developed. We describe the formation of a multidisciplinary collaborative network focused on AKI. We have proposed uniform standards for diagnosing and classifying AKI which will need to be validated in future studies. The Acute Kidney Injury Network offers a mechanism for proceeding with efforts to improve patient outcomes.", "date": "2007", "authors": ["Ravindra L Mehta 1, John A Kellum 2, Sudhir V Shah 3, Bruce A Molitoris 4, Claudio Ronco 5, David G Warnock 6, Adeera Levin 7"], "references": []}, {"id": 2131419242, "title": "Acute Kidney Injury, Mortality, Length of Stay, and Costs in Hospitalized Patients", "abstract": "The marginal effects of acute kidney injury on in-hospital mortality, length of stay (LOS), and costs have not been well described. A consecutive sample of 19,982 adults who were admitted to an urban academic medical center, including 9210 who had two or more serum creatinine (SCr) determinations, was evaluated. The presence and degree of acute kidney injury were assessed using absolute and relative increases from baseline to peak SCr concentration during hospitalization. Large increases in SCr concentration were relatively rare (e.g., >or=2.0 mg/dl in 105 [1%] patients), whereas more modest increases in SCr were common (e.g., >or=0.5 mg/dl in 1237 [13%] patients). Modest changes in SCr were significantly associated with mortality, LOS, and costs, even after adjustment for age, gender, admission International Classification of Diseases, Ninth Revision, Clinical Modification diagnosis, severity of illness (diagnosis-related group weight), and chronic kidney disease. For example, an increase in SCr >or=0.5 mg/dl was associated with a 6.5-fold (95% confidence interval 5.0 to 8.5) increase in the odds of death, a 3.5-d increase in LOS, and nearly 7500 dollars in excess hospital costs. Acute kidney injury is associated with significantly increased mortality, LOS, and costs across a broad spectrum of conditions. Moreover, outcomes are related directly to the severity of acute kidney injury, whether characterized by nominal or percentage changes in serum creatinine.", "date": "2005", "authors": ["Glenn M. Chertow , Elisabeth Burdick , Melissa Honour , Joseph V. Bonventre , David W. Bates"], "references": [2157825442, 1992332433, 2133482423, 1996020381, 1550111394, 2072075701, 2029723446, 2153767046, 2154145185, 2023644538]}, {"id": 2143432233, "title": "A comparison of albumin and saline for fluid resuscitation in the intensive care unit", "abstract": "background It remains uncertain whether the choice of resuscitation fluid for patients in intensive care units (ICUs) affects survival. We conducted a multicenter, randomized, double-blind trial to compare the effect of fluid resuscitation with albumin or saline on mortality in a heterogeneous population of patients in the ICU. methods We randomly assigned patients who had been admitted to the ICU to receive either 4 percent albumin or normal saline for intravascular-fluid resuscitation during the next 28 days. The primary outcome measure was death from any cause during the 28-day period after randomization. results Of the 6997 patients who underwent randomization, 3497 were assigned to receive albumin and 3500 to receive saline; the two groups had similar baseline characteristics. There were 726 deaths in the albumin group, as compared with 729 deaths in the saline group (relative risk of death, 0.99; 95 percent confidence interval, 0.91 to 1.09; P=0.87). The proportion of patients with new single-organ and multiple-organ failure was similar in the two groups (P=0.85). There were no significant differences between the groups in the mean (\u00b1SD) numbers of days spent in the ICU (6.5\u00b16.6 in the albumin group and 6.2\u00b16.2 in the saline group, P=0.44), days spent in the hospital (15.3\u00b19.6 and 15.6\u00b19.6, respectively; P = 0.30), days of mechanical ventilation (4.5\u00b16.1 and 4.3\u00b15.7, respectively; P = 0.74), or days of renal-replacement therapy (0.5\u00b12.3 and 0.4\u00b12.0, respectively; P = 0.41). conclusions In patients in the ICU, use of either 4 percent albumin or normal saline for fluid resuscitation results in similar outcomes at 28 days.", "date": "2004", "authors": ["Rinaldo Bellomo , Julie French , John Myburgh"], "references": [2107978811, 2768146862, 127034668, 2161328469, 1991864206, 2599527603, 1554783366, 2046852559, 1603121691, 2106228247]}, {"id": 2117958746, "title": "Intensity of renal support in critically ill patients with acute kidney injury", "abstract": "We randomly assigned critically ill patients with acute kidney injury and failure of at least one nonrenal organ or sepsis to receive intensive or less intensive renal-replacement therapy. The primary end point was death from any cause by day 60. In both study groups, hemodynamically stable patients underwent intermittent hemodialysis, and hemodynamically unstable patients underwent continuous venovenous hemodiafiltration or sustained low-efficiency dialysis. Patients receiving the intensive treatment strategy underwent intermittent hemodialysis and sustained low-efficiency dialysis six times per week and continuous venovenous hemodiafiltration at 35 ml per kilogram of body weight per hour; for patients receiving the less-intensive treatment strategy, the corresponding treatments were provided thrice weekly and at 20 ml per kilogram per hour. Results Baseline characteristics of the 1124 patients in the two groups were similar. The rate of death from any cause by day 60 was 53.6% with intensive therapy and 51.5% with less-intensive therapy (odds ratio, 1.09; 95% confidence interval, 0.86 to 1.40; P = 0.47). There was no significant difference between the two groups in the duration of renalreplacement therapy or the rate of recovery of kidney function or nonrenal organ failure. Hypotension during intermittent dialysis occurred in more patients randomly assigned to receive intensive therapy, although the frequency of hemodialysis sessions complicated by hypotension was similar in the two groups. Conclusions Intensive renal support in critically ill patients with acute kidney injury did not decrease mortality, improve recovery of kidney function, or reduce the rate of nonrenal organ failure as compared with less-intensive therapy involving a defined dose of intermittent hemodialysis three times per week and continuous renal-replacement therapy at 20 ml per kilogram per hour. (ClinicalTrials.gov number, NCT00076219.)", "date": "2008", "authors": ["Paul M. Palevsky , Jane Hongyuan Zhang , Theresa Z. O'Connor , Glenn M. Chertow , Susan T. Crowley , Devasmita Choudhury , Kevin Finkel , John A. Kellum , Emil Paganini , Roland M.H. Schein , Mark W. Smith , Kathleen M. Swanson , B. Taylor Thompson , Anitha Vijayan , Suzanne Watnick , Robert A. Star , Peter Peduzzi , E. Young , R. Fissel , W. Fissel , U. Patel , K. Belanger , A. Raine , N. Ricci , J. Lohr , P. Arora , D. Cloen , D. Wassel , L. Yohe , J. Amanzadeh , J. Penfield , M. Hussain , R. Katneni , A. Sajgure , A. Swann , G. Dolson , V. Ramanathan , G. Tasby , R. Bacallao , M. Jaradat , K. Graves , Q. Li , M. Krause , M. Shaver , M. Alam , K. Morris , T. Bland , E. Satter , J. Kraut , A. Felsenfeld +158"], "references": [2149687213, 1898928487, 2917359691, 1996020381, 2148973700, 2043132544, 2069722312, 1997278766, 1980228387, 2107538404]}, {"id": 1531106656, "title": "Intensity of continuous renal-replacement therapy in critically ill patients.", "abstract": "Background The optimal intensity of continuous renal-replacement therapy remains unclear. We conducted a multicenter, randomized trial to compare the effect of this therapy, delivered at two different levels of intensity, on 90-day mortality among critically ill patients with acute kidney injury. Methods We randomly assigned critically ill adults with acute kidney injury to continuous renal-replacement therapy in the form of postdilution continuous venovenous hemodiafiltration with an effluent flow of either 40 ml per kilogram of body weight per hour (higher intensity) or 25 ml per kilogram per hour (lower intensity). The primary outcome measure was death within 90 days after randomization. Results Of the 1508 enrolled patients, 747 were randomly assigned to higher-intensity therapy, and 761 to lower-intensity therapy with continuous venovenous hemodiafiltration. Data on primary outcomes were available for 1464 patients (97.1%): 721 in the higher-intensity group and 743 in the lower-intensity group. The two study groups had similar baseline characteristics and received the study treatment for an average of 6.3 and 5.9 days, respectively (P = 0.35). At 90 days after randomization, 322 deaths had occurred in the higher-intensity group and 332 deaths in the lower-intensity group, for a mortality of 44.7% in each group (odds ratio, 1.00; 95% confidence interval [CI], 0.81 to 1.23; P = 0.99). At 90 days, 6.8% of survivors in the higher-intensity group (27 of 399), as compared with 4.4% of survivors in the lower-intensity group (18 of 411), were still receiving renal-replacement therapy (odds ratio, 1.59; 95% CI, 0.86 to 2.92; P = 0.14). Hypophosphatemia was more common in the higher-intensity group than in the lower-intensity group (65% vs. 54%, P Conclusions In critically ill patients with acute kidney injury, treatment with higher-intensity continuous renal-replacement therapy did not reduce mortality at 90 days. (ClinicalTrials.gov number, NCT00221013.)", "date": "2009", "authors": ["Rinaldo Bellomo 1, Alan Cass 2, Louise Cole 3, Simon Finfer 4, Martin Gallagher 4, Serigne Lo 4, Colin McArthur 5, Shay McGuinness 5, John Myburgh 6, Robyn Norton 4, Carlos Scheinkestel 7, Steve Su 8"], "references": [2149687213, 2098931866, 1489794536, 2117958746, 2116909658, 2148973700, 1970593590, 2156916779, 2159674113, 2100460458]}, {"id": 2157775267, "title": "Intensive insulin therapy and mortality among critically ill patients: a meta-analysis including NICE-SUGAR study data", "abstract": "Background: Hyperglycemia is associated with increased mortality in critically ill patients. Randomized trials of intensive insulin therapy have reported inconsistent effects on mortality and increased rates of severe hypoglycemia. We conducted a meta-analysis to update the totality of evidence regarding the influence of intensive insulin therapy compared with conventional insulin therapy on mortality and severe hypoglycemia in the intensive care unit (ICU). Methods: We conducted searches of electronic databases, abstracts from scientific conferences and bibliographies of relevant articles. We included published randomized controlled trials conducted in the ICU that directly compared intensive insulin therapy with conventional glucose management and that documented mortality. We included in our meta-analysis the data from the recent NICE-SUGAR (Normoglycemia in Intensive Care Evaluation \u2014 Survival Using Glucose Algorithm Regulation) study. Results: We included 26 trials involving a total of 13 567 patients in our meta-analysis. Among the 26 trials that reported mortality, the pooled relative risk (RR) of death with intensive insulin therapy compared with conventional therapy was 0.93 (95% confidence interval [CI] 0.83\u20131.04). Among the 14 trials that reported hypoglycemia, the pooled RR with intensive insulin therapy was 6.0 (95% CI 4.5\u20138.0). The ICU setting was a contributing factor, with patients in surgical ICUs appearing to benefit from intensive insulin therapy (RR 0.63, 95% CI 0.44\u20130.91); patients in the other ICU settings did not (medical ICU: RR 1.0, 95% CI 0.78\u20131.28; mixed ICU: RR 0.99, 95% CI 0.86\u20131.12). The different targets of intensive insulin therapy (glucose level \u2264 6.1 mmol/L v. \u2264 8.3 mmol/L) did not influence either mortality or risk of hypoglycemia. Interpretation: Intensive insulin therapy significantly increased the risk of hypoglycemia and conferred no overall mortality benefit among critically ill patients. However, this therapy may be beneficial to patients admitted to a surgical ICU.", "date": "2009", "authors": ["Donald E.G. Griesdale , Russell J. de Souza , Rob M. van Dam , Daren K. Heyland , Deborah J. Cook , Atul Malhotra , Rupinder Dhaliwal , William R. Henderson , Dean R. Chittock , Simon Finfer , Daniel Talmor"], "references": [2125435699, 2118858814, 1986215651, 1980717583, 2145053281, 2107328434, 2115285670, 2148706741, 2081040380, 2007884458]}, {"id": 2028701043, "title": "Long-term Risk of Mortality and Other Adverse Outcomes After Acute Kidney Injury: A Systematic Review and Meta-analysis", "abstract": "Background Acute kidney injury (AKI) is common in hospitalized patients. The impact of AKI on long-term outcomes is controversial. Study Design Systematic review and meta-analysis. Setting & Participants Persons with AKI. Selection Criteria for Studies MEDLINE and EMBASE databases were searched from 1985 through October 2007. Original studies describing outcomes of AKI for patients who survived hospital discharge were included. Studies were excluded from review when participants were followed up for less than 6 months. Predictor AKI, defined as acute changes in serum creatinine level or acute need for renal replacement therapy. Outcomes Chronic kidney disease (CKD), cardiovascular disease, and mortality. Results 48 studies that contained a total of 47,017 participants were reviewed; 15 studies reported long-term data for patients without AKI. The incidence rate of mortality was 8.9 deaths/100 person-years in survivors of AKI and 4.3 deaths/100 patient-years in survivors without AKI (rate ratio [RR], 2.59; 95% confidence interval, 1.97 to 3.42). AKI was associated independently with mortality risk in 6 of 6 studies that performed multivariate adjustment (adjusted RR, 1.6 to 3.9) and with myocardial infarction in 2 of 2 studies (RR, 2.05; 95% confidence interval, 1.61 to 2.61). The incidence rate of CKD after an episode of AKI was 7.8 events/100 patient-years, and the rate of end-stage renal disease was 4.9 events/100 patient-years. Limitations The relative risk for CKD and end-stage renal disease after AKI was unattainable because of lack of follow-up of appropriate controls without AKI. Conclusions The development of AKI, defined as acute changes in serum creatinine level, characterizes hospitalized patients at increased risk of long-term adverse outcomes.", "date": "2009", "authors": ["Steven G. Coca 1, Bushra Yusuf 1, Michael G. Shlipak 2, 3, Amit X. Garg 4, Chirag R. Parikh 1"], "references": [2125435699, 2536590171, 1979423827, 75245760, 2107328434, 2131419242, 2133482423, 2153104532, 2122814783, 2111704803]}, {"id": 2111704803, "title": "Incidence and Outcomes in Acute Kidney Injury: A Comprehensive Population-Based Study", "abstract": "Epidemiological studies of acute kidney injury (AKI) and acute-on-chronic renal failure (ACRF) are surprisingly sparse and confounded by differences in definition. Reported incidences vary, with few studies being population-based. Given this and our aging population, the incidence of AKI may be much higher than currently thought. We tested the hypothesis that the incidence is higher by including all patients with AKI (in a geographical population base of 523,390) regardless of whether they required renal replacement therapy irrespective of the hospital setting in which they were treated. We also tested the hypothesis that the Risk, Injury, Failure, Loss, and End-Stage Kidney (RIFLE) classification predicts outcomes. We identified all patients with serum creatinine concentrations \u2265150 \u03bcmol/L (male) or \u2265130\u03bcmol/L (female) over a 6-mo period in 2003. Clinical outcomes were obtained from each patient9s case records. The incidences of AKI and ACRF were 1811 and 336 per million population, respectively. Median age was 76 yr for AKI and 80.5 yr for ACRF. Sepsis was a precipitating factor in 47% of patients. The RIFLE classification was useful for predicting full recovery of renal function ( P P P P = 0.035). RIFLE did not predict mortality at 90 d or 6 mo. Thus the incidence of AKI is much higher than previously thought, with implications for service planning and providing information to colleagues about methods to prevent deterioration of renal function. The RIFLE classification is useful for identifying patients at greatest risk of adverse short-term outcomes.", "date": "2007", "authors": ["Tariq Ali , Izhar Khan , William Simpson , Gordon Prescott , John Andrew Townend , William Cairns Stewart Smith , Alison Murray MacLeod"], "references": [2487377689, 2139937737, 2149687213, 2069722312, 2155939210, 2036544704, 2139529386, 2171344771, 2095573004, 2150098992]}, {"id": 2135163018, "title": "Chronic Dialysis and Death Among Survivors of Acute Kidney Injury Requiring Dialysis", "abstract": "Context Severe acute kidney injury among hospitalized patients often necessitates initiation of short-term dialysis. Little is known about the long-term outcome of those who survive to hospital discharge. Objective To assess the risk of chronic dialysis and all-cause mortality in individuals who experience an episode of acute kidney injury requiring dialysis. Design, Setting, and Participants We conducted a population-based cohort study of all adult patients in Ontario, Canada, with acute kidney injury who required in-hospital dialysis and survived free of dialysis for at least 30 days after discharge between July 1, 1996, and December 31, 2006. These individuals were matched with patients without acute kidney injury or dialysis during their index hospitalization. Matching was by age plus or minus 5 years, sex, history of chronic kidney disease, receipt of mechanical ventilation during the index hospitalization, and a propensity score for developing acute kidney injury requiring dialysis. Patients were followed up until March 31, 2007. Main Outcome Measures The primary end point was the need for chronic dialysis and the secondary outcome was all-cause mortality. Results We identified 3769 adults with acute kidney injury requiring in-hospital dialysis and 13 598 matched controls. The mean age was 62 years and median follow-up was 3 years. The incidence rate of chronic dialysis was 2.63 per 100 person-years among individuals with acute kidney injury requiring dialysis, and 0.91 per 100 person-years among control participants (adjusted hazard ratio, 3.23; 95% confidence interval, 2.70-3.86). All-cause mortality rates were 10.10 and 10.85 per 100 person-years, respectively (adjusted hazard ratio, 0.95; 95% confidence interval, 0.89-1.02). Conclusions Acute kidney injury necessitating in-hospital dialysis was associated with an increased risk of chronic dialysis but not all-cause mortality.", "date": "2009", "authors": ["Ron Wald , Robert R. Quinn , Jin Luo , Ping Li , Damon C. Scales , Muhammad M. Mamdani , Joel G. Ray"], "references": [2000445173, 2149687213, 2131419242, 2052806549, 2117958746, 2111945961, 2163900716, 2043132544, 2069722312, 1988629947]}, {"id": 2042074736, "title": "Acute Renal Failure After Coronary Intervention: Incidence, Risk Factors, and Relationship to Mortality", "abstract": "Abstract PURPOSE: This study set out to define the incidence, predictors, and mortality related to acute renal failure (ARF) and acute renal failure requiring dialysis (ARFD) after coronary intervention. PATIENTS AND METHODS: Derivation-validation set methods were used in 1,826 consecutive patients undergoing coronary intervention with evaluation of baseline creatinine clearance (CrCl), diabetic status, contrast exposure, postprocedure creatinine, ARF, ARFD, in-hospital mortality, and long-term survival (derivation set). Multiple logistic regression was used to derive the prior probability of ARFD in a second set of 1,869 consecutive patients (validation set). RESULTS: The incidence of ARF and ARFD was 144.6/1,000 and 7.7/1,000 cases respectively. The cutoff dose of contrast below which there was no ARFD was 100 mL. No patient with a CrCl > 47 mL/min developed ARFD. These thresholds were confirmed in the validation set. Multivariate analysis found CrCl [odds ratio (OR) = 0.83, 95% confidence interval (CI) 0.77 to 0.89, P P = 0.01), and contrast dose (OR = 1.008, 95% CI 1.002 to 1.013, P = 0.01) to be independent predictors of ARFD. Patients in the validation set who underwent dialysis had a predicted prior probability of ARFD of between 0.07 and 0.73. The in-hospital mortality for those who developed ARFD was 35.7% and the 2-year survival was 18.8%. CONCLUSION: The occurrence of ARFD after coronary intervention is rare (", "date": "1997", "authors": ["Peter A McCullough , Robert Wolyn , Leslie L Rocher , Robert N Levin , William W O\u2019Neill"], "references": [1973948212, 1550111394, 2029723446, 2071273488, 2330776976, 1996698106, 2018224788, 2056699935, 2316461329, 2073085411]}, {"id": 2100820722, "title": "Identification of Severe Acute Respiratory Syndrome in Canada", "abstract": "background Severe acute respiratory syndrome (SARS) is a condition of unknown cause that has recently been recognized in patients in Asia, North America, and Europe. This report summarizes the initial epidemiologic findings, clinical description, and diagnostic findings that followed the identification of SARS in Canada. methods SARS was first identified in Canada in early March 2003. We collected epidemiologic, clinical, and diagnostic data from each of the first 10 cases prospectively as they were identified. Specimens from all cases were sent to local, provincial, national, and international laboratories for studies to identify an etiologic agent. results The patients ranged from 24 to 78 years old; 60 percent were men. Transmission occurred only after close contact. The most common presenting symptoms were fever (in 100 percent of cases) and malaise (in 70 percent), followed by nonproductive cough (in 100 percent) and dyspnea (in 80 percent) associated with infiltrates on chest radiography (in 100 percent). Lymphopenia (in 89 percent of those for whom data were available), elevated lactate dehydrogenase levels (in 80 percent), elevated aspartate aminotransferase levels (in 78 percent), and elevated creatinine kinase levels (in 56 percent) were common. Empirical therapy most commonly included antibiotics, oseltamivir, and intravenous ribavirin. Mechanical ventilation was required in five patients. Three patients died, and five have had clinical improvement. The results of laboratory investigations were negative or not clinically significant except for the amplification of human metapneumovirus from respiratory specimens from five of nine patients and the isolation and amplification of a novel coronavirus from five of nine patients. In four cases both pathogens were isolated. conclusions SARS is a condition associated with substantial morbidity and mortality. It appears to be of viral origin, with patterns suggesting droplet or contact transmission. The role of human metapneumovirus, a novel coronavirus, or both requires further investigation.", "date": "2003", "authors": ["Susan M. Poutanen 1, 2, Donald E. Low 1, Bonnie Henry 3, Sandy Finkelstein 4, David Rose 4, Karen Green 1, Raymond Tellier 5, 6, Ryan Draker 1, Dena Adachi 1, Melissa Ayers 1, Adrienne K. Chan 1, Danuta M. Skowronski 7, Irving Salit 1, Andrew E. Simor 1, Arthur S. Slutsky 1, Patrick W. Doyle 8, Mel Krajden 7, Martin Petric 7, Robert C. Brunham 8, Allison J. McGeer 1"], "references": [2597070792, 2161328469, 2170881661, 2093852073, 2463755683, 2152552492, 2135259291, 2136166622, 2097665403, 1898899939]}, {"id": 2125251240, "title": "A cluster of cases of severe acute respiratory syndrome in Hong Kong.", "abstract": "Background Information on the clinical features of the severe acute respiratory syndrome (SARS) will be of value to physicians caring for patients suspected of having this disorder. Methods We abstracted data on the clinical presentation and course of disease in 10 epidemiologically linked Chinese patients (5 men and 5 women 38 to 72 years old) in whom SARS was diagnosed between February 22, 2003, and March 22, 2003, at our hospitals in Hong Kong, China. Results Exposure between the source patient and subsequent patients ranged from minimal to that between patient and health care provider. The incubation period ranged from 2 to 11 days. All patients presented with fever (temperature, >38\u00b0C for over 24 hours), and most presented with rigor, dry cough, dyspnea, malaise, headache, and hypoxemia. Physical examination of the chest revealed crackles and percussion dullness. Lymphopenia was observed in nine patients, and most patients had mildly elevated aminotransferase levels but normal serum creatinine levels...", "date": "2003", "authors": ["Kenneth W. Tsang , Pak L. Ho , Gaik C. Ooi , Wilson K. Yee , Teresa Wang , Moira Chan-Yeung , Wah K. Lam , Wing H. Seto , Loretta Y. Yam , Thomas M. Cheung , Poon C. Wong , Bing Lam , Mary S. Ip , Jane Chan , Kwok Y. Yuen , Kar N. Lai"], "references": [1607298558, 1982444609, 2041775285, 2132293969]}, {"id": 2107922358, "title": "Rapid detection and quantification of RNA of Ebola and Marburg viruses, Lassa virus, Crimean-Congo hemorrhagic fever virus, Rift Valley fever virus, dengue virus, and yellow fever virus by real-time reverse transcription-PCR.", "abstract": "Viral hemorrhagic fevers (VHFs) are acute infections with high case fatality rates. Important VHF agents are Ebola and Marburg viruses (MBGV/EBOV), Lassa virus (LASV), Crimean-Congo hemorrhagic fever virus (CCHFV), Rift Valley fever virus (RVFV), dengue virus (DENV), and yellow fever virus (YFV). VHFs are clinically difficult to diagnose and to distinguish; a rapid and reliable laboratory diagnosis is required in suspected cases. We have established six one-step, real-time reverse transcription-PCR assays for these pathogens based on the Superscript reverse transcriptase-Platinum Taq polymerase enzyme mixture. Novel primers and/or 5\u2032-nuclease detection probes were designed for RVFV, DENV, YFV, and CCHFV by using the latest DNA database entries. PCR products were detected in real time on a LightCycler instrument by using 5\u2032-nuclease technology (RVFV, DENV, and YFV) or SybrGreen dye intercalation (MBGV/EBOV, LASV, and CCHFV). The inhibitory effect of SybrGreen on reverse transcription was overcome by initial immobilization of the dye in the reaction capillaries. Universal cycling conditions for SybrGreen and 5\u2032-nuclease probe detection were established. Thus, up to three assays could be performed in parallel, facilitating rapid testing for several pathogens. All assays were thoroughly optimized and validated in terms of analytical sensitivity by using in vitro-transcribed RNA. The \u226595% detection limits as determined by probit regression analysis ranged from 1,545 to 2,835 viral genome equivalents/ml of serum (8.6 to 16 RNA copies per assay). The suitability of the assays was exemplified by detection and quantification of viral RNA in serum samples of VHF patients.", "date": "2002", "authors": ["Christian Drosten , Stephan G\u00f6ttig , Stefan Schilling , Marcel Asper , Marcus Panning , Herbert Schmitz , Stephan G\u00fcnther"], "references": [2047480444, 1994091239, 2070721758, 2131589770, 2137089963, 2163760194, 2149579937, 2134971582, 2171308211, 2124990542]}, {"id": 2127062009, "title": "Viruses and Bacteria in the Etiology of the Common Cold", "abstract": "Two hundred young adults with common colds were studied during a 10-month period. Virus culture, antigen detection, PCR, and serology with paired samples were used to identify the infection. Viral etiology was established for 138 of the 200 patients (69%). Rhinoviruses were detected in 105 patients, coronavirus OC43 or 229E infection was detected in 17, influenza A or B virus was detected in 12, and single infections with parainfluenza virus, respiratory syncytial virus, adenovirus, and enterovirus were found in 14 patients. Evidence for bacterial infection was found in seven patients. Four patients had a rise in antibodies against Chlamydia pneumoniae, one had a rise in antibodies against Haemophilus influenzae, one had a rise in antibodies against Streptococcus pneumoniae, and one had immunoglobulin M antibodies against Mycoplasma pneumoniae. The results show that although approximately 50% of episodes of the common cold were caused by rhinoviruses, the etiology can vary depending on the epidemiological situation with regard to circulating viruses. Bacterial infections were rare, supporting the concept that the common cold is almost exclusively a viral disease.", "date": "1998", "authors": ["Mika J. M\u00e4kel\u00e4 1, Tuomo Puhakka 1, Olli Ruuskanen 1, Maija Leinonen 2, Pekka Saikku 2, Marko Kimpim\u00e4ki 3, Soile Blomqvist 3, Timo Hyypi\u00e4 4, Pertti Arstila 4"], "references": [2055750915, 2337555053, 2098388207, 163073849, 2146133178, 2032842024, 2018812376, 1856165804, 1830634530, 1699035432]}, {"id": 2084994773, "title": "Phylogenetic analysis of a highly conserved region of the polymerase gene from 11 coronaviruses and development of a consensus polymerase chain reaction assay.", "abstract": "Abstract Viruses in the genus Coronavirus are currently placed in three groups based on antigenic cross-reactivity and sequence analysis of structural protein genes. Consensus polymerase chain reaction (PCR) primers were used to obtain cDNA, then cloned and sequenced a highly conserved 922 nucleotide region in open reading frame (ORF) 1b of the polymerase (pol) gene from eight coronaviruses. These sequences were compared with published sequences for three additional coronaviruses. In this comparison, it was found that nucleotide substitution frequencies (per 100 nucleotides) varied from 46.40 to 50.13 when viruses were compared among the traditional coronavirus groups and, with one exception (the human coronavirus (HCV) 229E), varied from 2.54 to 15.89 when compared within these groups. (The substitution frequency for 229E, as compared to other members of the same group, varied from 35.37 to 35.72.) Phylogenetic analysis of these pol gene sequences resulted in groupings which correspond closely with the previously described groupings, including recent data which places the two avian coronaviruses\u2014infectious bronchitis virus (IBV) of chickens and turkey coronavirus (TCV)\u2014in the same group [Guy, J.S., Barnes, H.J., Smith L.G., Breslin, J., 1997. Avian Dis. 41:583\u2013590]. A single pair of degenerate primers was identified which amplify a 251 bp region from coronaviruses of all three groups using the same reaction conditions. This consensus PCR assay for the genus Coronavirus may be useful in identifying as yet unknown coronaviruses.", "date": "1999", "authors": ["Charles B. Stephensen , Donald B. Casebolt , Nupur N. Gangopadhyay"], "references": [2134812217, 2009310436, 132455992, 2156596665, 1582561043, 2087363345, 2149495938, 2329318335, 1994193749, 3011200155]}, {"id": 2149579937, "title": "Imported Lassa fever in Germany: molecular characterization of a new Lassa virus strain.", "abstract": "We describe the isolation and characterization of a new Lassa virus strain imported into Germany by a traveler who had visited Ghana, Cote D\u2019Ivoire, and Burkina Faso. This strain, designated \u201cAV,\u201d originated from a region in West Africa where Lassa fever has not been reported. Viral S RNA isolated from the patient\u2019s serum was amplified and sequenced. A long-range reverse transcription polymerase chain reaction allowed amplification of the full-length (3.4 kb) S RNA. The coding sequences of strain AV differed from those of all known Lassa prototype strains (Josiah, Nigeria, and LP) by approximately 20%, mainly at third codon positions. Phylogenetically, strain AV appears to be most closely related to strain Josiah from Sierra Leone. Lassa viruses comprise a group of genetically highly diverse strains, which has implications for vaccine development. The new method for full-length S RNA amplification may facilitate identification and molecular analysis of new arenaviruses or arenavirus strains.", "date": "2000", "authors": ["S. G\u00fcnther , P. Emmerich , T. Laue , O. K\u00fchle , M. Asper , A. Jung , T. Grewing , J. ter Meulen , H. Schmitz"], "references": [2154128645, 1967150940, 2044967254, 1845818816, 2163443142, 2326037208, 1941129807, 2109638242, 2056690053, 1509033271]}, {"id": 2090060897, "title": "Evaluation of concurrent shedding of bovine coronavirus via the respiratory tract and enteric route in feedlot cattle.", "abstract": "Objective\u2014To assess the relationship between shedding of bovine coronavirus (BCV) via the respiratory tract and enteric routes and the association with weight gain in feedlot cattle. Animals\u201456 crossbred steers. Procedures\u2014Paired fecal samples and nasal swab specimens were obtained and were tested for BCV, using antigen-capture ELISA. Paired serum samples obtained were tested for antibodies to BCV, using antibody-detection ELISA. Information was collected on weight gain, clinical signs, and treatments for enteric and respiratory tract disease during the study period. Results\u2014Number of samples positive for bovine respiratory coronavirus (BRCV) or bovine enteric coro navirus (BECV) was 37/224 (17%) and 48/223 (22%), respectively. Some cattle (25/46, 45%) shed BECV and BRCV. There were 25/29 (86%) cattle positive for BECV that shed BRCV, but only 1/27 (4%) cattle negative to BECV shed BRCV. Twenty-seven of 48 (56%) paired nasal swab specimens and fecal samples positive for BECV were positive for BRCV. In con...", "date": "2001", "authors": ["Kyoung Oh Cho 1, Armando E. Hoet 2, Steven C. Loerch 3, Thomas E. Wittum 2, Linda J. Saif 2"], "references": [50597173, 2072381230, 2409643934, 1987051173, 2045765248, 1963591796, 2406151794, 2032346601, 1979854169, 2302897180]}, {"id": 2004869546, "title": "TTV a common virus, but pathogenic?", "abstract": "", "date": "1998", "authors": ["Yvonne Cossart"], "references": [2038264706, 2089551619, 2016137045, 2328399749, 2022277835, 2028973331]}, {"id": 2030133843, "title": "HGV: hepatitis G virus or harmless G virus?", "abstract": "The discovery of the hepatitis G virus (HGV) has given hepatologists a new lease on life. Just when they were becoming frustrated with the slow rate of progress in unravell ing the pathobiological consequences of hepatitis B and C virus infections, along comes another candidate virus. HGV, a single-stranded ribonucleic acid (RNA) virus that belongs to the Flaviviridae family, has a global distribution. The virus is present in 1-2% of blood donors in the USA, a frequency higher than that of either HCV or hepatitis B virus (HBV) (Alter). Even more striking is the seroprevalence of 15\"2% reported in West African residents (JMed Virol 1996; 50: 97). HGVexis t s in a chronic carrier state. The virus is transmitted parenterally and is often present in patients who have received multiple transfusions or who are on haemodialysis (N Engl J Med 1996; 334: 1485), and in intravenous drug users. There is preliminary evidence for perinatal transmission (Lancet 1996; 347: 615). HGV RNA sequences have been identified in serum from patients with non-A-E acute and chronic hepatitis and cirrhosis. Impressive data comes from Brescia, Italy, where 35% of patients with acute hepatitis and 39% of those with chronic hepatitis were positive for HGV RNA (Fiordalisi). Among blood donors the virus is more common in those with raised serum aminotransferase concentrations (3\"9%) than in those with normal concentrations (0\"8%) (ff Med Virol 1996; 50: 97). These findings imply that HGV is a human pathogen, but is it? Other information is more consistent with HGV being an innocent passenger. The great majority of individuals who become HGV-RNA positive after blood transfusion have normal serum aminotronsperase concentrations and neither they, nor those found positive for HGV RNA in other circumstances, develop liver disease during prolonged follow-up (Alter). Moreover, when serum enzyme concentrations are raised they seldom accord with levels of viraemia. HGV and HCV are often, and HGV and HBV less often, found together in serum. In those coinfected with HGV and HCV, aminotransterases run parallel to HCV rather than HGV, and the presence of the latter seems to have no effect on outcome. HGV usually accounts for only a minority of cases of acute non-A-E hepatitis, and there is no evidence yet of progression over time to chronic hepatitis, cirrhosis, or hepatocellular", "date": "1996", "authors": ["Michael C Kew , Chris Kassianides"], "references": [2083266836, 2313004219, 2023962288, 2155517838, 2075432722, 1984200234]}, {"id": 2106882534, "title": "CLUSTAL W: IMPROVING THE SENSITIVITY OF PROGRESSIVE MULTIPLE SEQUENCE ALIGNMENT THROUGH SEQUENCE WEIGHTING, POSITION-SPECIFIC GAP PENALTIES AND WEIGHT MATRIX CHOICE", "abstract": "The sensitivity of the commonly used progressive multiple sequence alignment method has been greatly improved for the alignment of divergent protein sequences. Firstly, individual weights are assigned to each sequence in a partial alignment in order to down-weight near-duplicate sequences and up-weight the most divergent ones. Secondly, amino acid substitution matrices are varied at different alignment stages according to the divergence of the sequences to be aligned. Thirdly, residue-specific gap penalties and locally reduced gap penalties in hydrophilic regions encourage new gaps in potential loop regions rather than regular secondary structure. Fourthly, positions in early alignments where gaps have been opened receive locally reduced gap penalties to encourage the opening up of new gaps at these positions. These modifications are incorporated into a new program, CLUSTAL W which is freely available.", "date": "1994", "authors": ["Julie D. Thompson 1, Desmond G. Higgins 2, Toby J. Gibson 2"], "references": [2097706568, 2015292449, 2009310436, 2143210482, 2065461553, 1998300401, 2045391589, 2149208773, 2008708467, 2102122585]}, {"id": 2463755683, "title": "Update: Outbreak of severe acute respiratory syndrome - Worldwide, 2003", "abstract": "", "date": "2002", "authors": ["T. Tsang , L. Pak-Yin , M. Lee , J.-S. Wu , Y.-C. Wu , I.-H. Chiang , K.-T. Chen , K.-H. Hsu , T.-J. Chen , H.-T. Lee , S.-J. Twu , S. Chunsuttiwat , P. Sawanpanyalert , K. Ungchusak , A. Chaovavanich"], "references": [2089784797]}, {"id": 2403756321, "title": "Microbial Threats to Health: Emergence, Detection, and Response", "abstract": "", "date": "2002", "authors": ["Smolinski Ms , Hamburg Ma , Lederberg J"], "references": [1981665776, 2104548316, 2127435093, 2119629693, 2101273146, 2126707939, 2114878600, 2556073860, 2023092952, 2156522255]}, {"id": 2127949919, "title": "Nipah Virus: A Recently Emergent Deadly Paramyxovirus", "abstract": "A paramyxovirus virus termed Nipah virus has been identified as the etiologic agent of an outbreak of severe encephalitis in people with close contact exposure to pigs in Malaysia and Singapore. The outbreak was first noted in late September 1998 and by mid-June 1999, more than 265 encephalitis cases, including 105 deaths, had been reported in Malaysia, and 11 cases of encephalitis or respiratory illness with one death had been reported in Singapore. Electron microscopic, serologic, and genetic studies indicate that this virus belongs to the family Paramyxoviridae and is most closely related to the recently discovered Hendra virus. We suggest that these two viruses are representative of a new genus within the family Paramyxoviridae. Like Hendra virus, Nipah virus is unusual among the paramyxoviruses in its ability to infect and cause potentially fatal disease in a number of host species, including humans.", "date": "2000", "authors": ["K. B. Chua 1, W. J. Bellini 2, P. A. Rota 2, B. H. Harcourt 2, A. Tamin 2, S. K. Lam 1, T. G. Ksiazek 2, P. E. Rollin 2, S. R. Zaki 2, W.-J. Shieh 2, C. S. Goldsmith 2, D. J. Gubler 3, J. T. Roehrig 3, B. Eaton 4, A. R. Gould 4, J. Olson 2, P. Daniels 4, A. E. Ling 5, C. J. Peters 2, L. J. Anderson 2, B. W. J. Mahy 2"], "references": [2094644220, 1981329413, 1983097458, 2130324980, 2040567164, 2028665290, 1699123896, 2029131064, 2147898590, 367527820]}, {"id": 1576737979, "title": "Microarray-based detection and genotyping of viral pathogens", "abstract": "The detection of viral pathogens is of critical importance in biology, medicine, and agriculture. Unfortunately, existing techniques to screen for a broad spectrum of viruses suffer from severe limitations. To facilitate the comprehensive and unbiased analysis of viral prevalence in a given biological setting, we have developed a genomic strategy for highly parallel viral screening. The cornerstone of this approach is a long oligonucleotide (70-mer) DNA microarray capable of simultaneously detecting hundreds of viruses. Using virally infected cell cultures, we were able to efficiently detect and identify many diverse viruses. Related viral serotypes could be distinguished by the unique pattern of hybridization generated by each virus. Furthermore, by selecting microarray elements derived from highly conserved regions within viral families, individual viruses that were not explicitly represented on the microarray were still detected, raising the possibility that this approach could be used for virus discovery. Finally, by using a random PCR amplification strategy in conjunction with the microarray, we were able to detect multiple viruses in human respiratory specimens without the use of sequence-specific or degenerate primers. This method is versatile and greatly expands the spectrum of detectable viruses in a single assay while simultaneously providing the capability to discriminate among viral subtypes.", "date": "2002", "authors": ["David Wang 1, Laurent Coscoy 2, Maxine Zylberberg 2, Pedro C. Avila 2, Homer A. Boushey 2, Don Ganem 2, Joseph L. DeRisi 2"], "references": [2055043387, 1502936039, 1549647993, 2028318125, 2137089963, 2099369211, 2169391021, 2033380151, 2037142940, 2133247102]}, {"id": 2128788856, "title": "Human Metapneumovirus Infections in Young and Elderly Adults", "abstract": "Human metapneumovirus virus (hMPV) is a newly discovered respiratory pathogen with limited epidemiological data available. Cohorts of young and older adults were prospectively evaluated for hMPV infection during 2 winter seasons. Patients hospitalized for cardiopulmonary conditions during that period were also studied. Overall, 44 (4.5%) of 984 illnesses were associated with hMPV infection, and 9 (4.1%) of 217 asymptomatic subjects were infected. There was a significant difference in rates of hMPV illnesses between years 1 and 2 (7/452 [1.5%] vs. 37/532 [7.0%]; P<.0001). In the second year, 11% of hospitalized patients had evidence of hMPV infection. Infections occurred in all age groups but were most common among young adults. Frail elderly people with hMPV infection frequently sought medical attention. In conclusion, hMPV infection occurs in adults of all ages and may account for a significant portion of persons hospitalized with respiratory infections during some years.", "date": "2003", "authors": ["Ann R Falsey 1, Dean Erdman 2, Larry J Anderson 2, Edward E Walsh 1"], "references": [2170881661, 2152552492, 2048618475, 1589490904, 2110198546, 2165127900, 2076770315, 2079378048, 2068094897, 2065996927]}, {"id": 2076620790, "title": "A morbillivirus that caused fatal disease in horses and humans", "abstract": "A morbillivirus has been isolated and added to an increasing list of emerging viral diseases. This virus caused an outbreak of fatal respiratory disease in horses and humans. Genetic analyses show it to be only distantly related to the classic morbilliviruses rinderpest, measles, and canine distemper. When seen by electron microscopy, viruses had 10- and 18-nanometer surface projections that gave them a \"double-fringed\" appearance. The virus induced syncytia that developed in the endothelium of blood vessels, particularly the lungs.", "date": "1995", "authors": ["K Murray , P Selleck , P Hooper , A Hyatt , A Gould , L Gleeson , H Westbury , L Hiley , L Selvey , B Rodwell"], "references": [2034148010, 2123427059, 1972759043, 2020526182, 2002849005, 2078406556, 1582863396, 2148205933, 1997387663, 2106255335]}, {"id": 2123324969, "title": "Guidelines for the management of adults with community-acquired pneumonia. Diagnosis, assessment of severity, antimicrobial therapy, and prevention.", "abstract": "", "date": "2000", "authors": ["M. S. Niederman , L. A. Mandell , A. Anzueto , J. B. Bass , W. A. Broughton , G. D. Campbell , N. Dean , T. File , M. J. Fine , P. A. Gross , F. Martinez , T. J. Marrie , J. F. Plouffe , J. Ramirez , G. A. Sarosi , A. Torres , R. Wilson , V. L. Yu"], "references": [2320270386, 2130141864, 2004554957, 2115071132, 2035760184, 2158075347, 1988729025, 2339696769, 2067476021, 1965015119]}, {"id": 2130141864, "title": "Practice Guidelines for the Management of Community-Acquired Pneumonia in Adults", "abstract": "John G. Bartlett,1 Scott F Dowell,2 Lionel A. Mandell,6 Thomas M. File, Jr.,3 Daniel M. Musher,4 and Michael J. Fine5 'Johns Hopkins University School of Medicine, Baltimore, Maryland, 2Centers for Disease Control and Prevention, Atlanta, Georgia, 3Northeastern Ohio Universities College of Medicine, Cleveland, Ohio, 4Baylor College of Medicine and Veterans Affairs Medical Center, Houston, Texas, and 5University of Pittsburgh, Pennsylvania, USA; and 6McMaster University, Toronto, Canada", "date": "2000", "authors": ["John G Bartlett 1, Scott F Dowell 2, Lionel A Mandell 3, Thomas M File 4, Daniel M Musher 5, Michael J Fine 6"], "references": [1833207062, 2320270386, 2109779439, 2464952627, 3024375665, 2103828083, 3042797117, 2112302527, 2037712857, 2035760184]}, {"id": 1991467275, "title": "Bronchiolitis Obliterans Organizing Pneumonia", "abstract": "Bronchiolar disorders can be divided into 2 general categories: (1) airway disorders (cellular bronchiolitis and obliterative bronchiolitis) and (2) parenchymal disorders (respiratory bronchiolitis-interstitial lung disease, which occurs in smokers and is treatable with smoking cessation or corticosteroid therapy, and bronchiolitis obliterans organizing pneumonia, an inflammatory lung disease simultaneously involving the terminal bronchioles and alveoli). This article reviews the clinical findings and therapeutic management of bronchiolitis obliterans organizing pneumonia.", "date": "2001", "authors": ["Gary R. Epler"], "references": []}, {"id": 1982444609, "title": "Bronchiolitis obliterans organizing pneumonia: CT features in 14 patients.", "abstract": "Bronchiolitis obliterans organizing pneumonia is a disease characterized by the presence of granulation tissue within small airways and the presence of areas of organizing pneumonia. We retrospectively reviewed the chest radiographs, CT scans, and biopsy specimens in 14 consecutive patients with proved bronchiolitis obliterans organizing pneumonia. Six patients were immunocompromised because of leukemia or bone-marrow transplantation. In all patients, 10-mm collimation CT scans were available. In 11 of the 14 patients, select 1.5-mm scans were obtained. The CT findings included patchy unilateral (n = 1) or bilateral air-space consolidation (n = 9), small nodular opacities (n = 7), irregular linear opacities (n = 2), bronchial wall thickening and dilatation (n = 6), and small pleural effusions (n = 4). All patients had areas of air-space consolidation, small nodules, or both. A predominantly subpleural distribution of the air-space consolidation was apparent on the radiographs of two patients and on CT scans of six. Pathologically, the nodules and the consolidation represented different degrees of inflammation in bronchioles, alveolar ducts, and alveoli. Although most of the findings were apparent on the radiographs, the CT scans depicted the anatomic distribution and extent of bronchiolitis obliterans organizing pneumonia more accurately than did the plain chest radiographs.", "date": "1990", "authors": ["N L M\u00fcller , C A Staples , R R Miller"], "references": [2131262274, 2149661971, 2125251240, 2416914730, 2103258766, 2111742711, 2163026870, 2151996610, 2114857071, 2150387305]}, {"id": 2107053896, "title": "Hospital Outbreak of Middle East Respiratory Syndrome Coronavirus", "abstract": "Background In September 2012, the World Health Organization reported the first cases of pneumonia caused by the novel Middle East respiratory syndrome coronavirus (MERS-CoV). We describe a cluster of health care\u2013acquired MERS-CoV infections. Methods Medical records were reviewed for clinical and demographic information and determination of potential contacts and exposures. Case patients and contacts were interviewed. The incubation period and serial interval (the time between the successive onset of symptoms in a chain of transmission) were estimated. Viral RNA was sequenced. Results Between April 1 and May 23, 2013, a total of 23 cases of MERS-CoV infection were reported in the eastern province of Saudi Arabia. Symptoms included fever in 20 patients (87%), cough in 20 (87%), shortness of breath in 11 (48%), and gastrointestinal symptoms in 8 (35%); 20 patients (87%) presented with abnormal chest radiographs. As of June 12, a total of 15 patients (65%) had died, 6 (26%) had recovered, and 2 (9%) remained ...", "date": "2013", "authors": ["Abdullah Assiri , Allison McGeer , Trish M. Perl , Connie S. Price , Abdullah A. Al Rabeeah , Derek A.T. Cummings , Zaki N. Alabdullatif , Maher Assad , Abdulmohsen Almulhim , Hatem Makhdoom , Hossam Madani , Rafat Alhakeem , Jaffar A. Al-Tawfiq , Matthew Cotten , Simon J. Watson , Paul Kellam , Alimuddin I. Zumla , Ziad A. Memish"], "references": [2166867592, 2132260239, 2100820722, 1703839189, 2147166346, 2116586125, 2045002682, 2058144955, 2113457186, 2111412754]}, {"id": 2112147913, "title": "Middle East respiratory syndrome coronavirus (MERS-CoV): announcement of the Coronavirus Study Group.", "abstract": "During the summer of 2012, in Jeddah, Saudi Arabia, a hitherto unknown coronavirus (CoV) was isolated from the sputum of a patient with acute pneumonia and renal failure ([1][1], [2][2]). The isolate was provisionally called human coronavirus Erasmus Medical Center (EMC) ([3][3]). Shortly thereafter", "date": "2013", "authors": ["Raoul J. de Groot 1, Susan C. Baker 2, Ralph S. Baric 3, Caroline S. Brown 4, Christian Drosten 5, Luis Enjuanes 6, Ron A. M. Fouchier 7, Monica Galiano 8, Alexander E. Gorbalenya 9, Ziad A. Memish 10, Stanley Perlman 11, Leo L. M. Poon 12, Eric J. Snijder 9, Gwen M. Stephens 10, Patrick C. Y. Woo 12, Ali M. Zaki 13, Maria Zambon 8, John Ziebuhr 14"], "references": [2166867592, 2113457186, 1690366459, 2066347985, 2105558355, 2102229939, 2060720058, 2079206979]}, {"id": 2045002682, "title": "Family Cluster of Middle East Respiratory Syndrome Coronavirus Infections", "abstract": "A human coronavirus, called the Middle East respiratory syndrome coronavirus (MERS-CoV), was first identified in September 2012 in samples obtained from a Saudi Arabian businessman who died from acute respiratory failure. Since then, 49 cases of infections caused by MERS-CoV (previously called a novel coronavirus) with 26 deaths have been reported to date. In this report, we describe a family case cluster of MERS-CoV infection, including the clinical presentation, treatment outcomes, and household relationships of three young men who became ill with MERS-CoV infection after the hospitalization of an elderly male relative, who died of the disease. Twenty-four other family members living in the same household and 124 attending staff members at the hospitals did not become ill. MERS-CoV infection may cause a spectrum of clinical illness. Although an animal reservoir is suspected, none has been discovered. Meanwhile, global concern rests on the ability of MERS-CoV to cause major illness in close contacts of patients.", "date": "2013", "authors": ["Ziad A. Memish 1, Alimuddin I. Zumla 2, Rafat F. Al-Hakeem 3, Abdullah A. Al-Rabeeah 3, Gwen M. Stephens 3"], "references": [2166867592, 2129542667, 1703839189, 2158773042, 2130450914, 2088479029, 297155885]}, {"id": 1852588318, "title": "Assays for laboratory confirmation of novel human coronavirus (hCoV-EMC) infections.", "abstract": "We present a rigorously validated and highly sensitive confirmatory real-time RT-PCR assay (1A assay) that can be used in combination with the previously reported upE assay. Two additional RT-PCR assays for sequencing are described, targeting the RdRp gene (RdRpSeq assay) and N gene (NSeq assay), where an insertion/deletion polymorphism might exist among different hCoV-EMC strains. Finally, a simplified and biologically safe protocol for detection of antibody response by immunofluorescence microscopy was developed using convalescent patient serum.", "date": "2012", "authors": ["Victor Corman 1, Marcel M\u00fcller 1, U. Costabel 2, J. Timm 2, Tabea Binger 1, Bernhard Meyer 1, P. Kreher 3, Erik Lattwein 4, Monika Eschbach-Bludau 1, A. Nitsche 3, T. Bleicker 1, O. Landt 5, Brunhilde Schweiger 3, Jan-Felix Drexler 1, Albert Osterhaus 6, Bart Haagmans 6, U. Dittmer 2, F. Bonin 2, Thorsten Wolff 3, Christian Drosten 1"], "references": [2166867592, 2129542667, 1703839189, 1690366459, 2167080692, 2082755732, 1593955729, 2145810580, 2122612816, 2136039989]}, {"id": 2163627712, "title": "Clinical features and short-term outcomes of 144 patients with SARS in the greater Toronto area.", "abstract": "ContextSevere acute respiratory syndrome (SARS) is an emerging infectious disease that first manifested in humans in China in November 2002 and has subsequently spread worldwide.ObjectivesTo describe the clinical characteristics and short-term outcomes of SARS in the first large group of patients in North America; to describe how these patients were treated and the variables associated with poor outcome.Design, Setting, and PatientsRetrospective case series involving 144 adult patients admitted to 10 academic and community hospitals in the greater Toronto, Ontario, area between March 7 and April 10, 2003, with a diagnosis of suspected or probable SARS. Patients were included if they had fever, a known exposure to SARS, and respiratory symptoms or infiltrates observed on chest radiograph. Patients were excluded if an alternative diagnosis was determined.Main Outcome MeasuresLocation of exposure to SARS; features of the history, physical examination, and laboratory tests at admission to the hospital; and 21-day outcomes such as death or intensive care unit (ICU) admission with or without mechanical ventilation.ResultsOf the 144 patients, 111 (77%) were exposed to SARS in the hospital setting. Features of the clinical examination most commonly found in these patients at admission were self-reported fever (99%), documented elevated temperature (85%), nonproductive cough (69%), myalgia (49%), and dyspnea (42%). Common laboratory features included elevated lactate dehydrogenase (87%), hypocalcemia (60%), and lymphopenia (54%). Only 2% of patients had rhinorrhea. A total of 126 patients (88%) were treated with ribavirin, although its use was associated with significant toxicity, including hemolysis (in 76%) and decrease in hemoglobin of 2 g/dL (in 49%). Twenty-nine patients (20%) were admitted to the ICU with or without mechanical ventilation, and 8 patients died (21-day mortality, 6.5%; 95% confidence interval [CI], 1.9%-11.8%). Multivariable analysis showed that the presence of diabetes (relative risk [RR], 3.1; 95% CI, 1.4-7.2; P = .01) or other comorbid conditions (RR, 2.5; 95% CI, 1.1-5.8; P = .03) were independently associated with poor outcome (death, ICU admission, or mechanical ventilation).ConclusionsThe majority of cases in the SARS outbreak in the greater Toronto area were related to hospital exposure. In the event that contact history becomes unreliable, several features of the clinical presentation will be useful in raising the suspicion of SARS. Although SARS is associated with significant morbidity and mortality, especially in patients with diabetes or other comorbid conditions, the vast majority (93.5%) of patients in our cohort survived.Published online May 6, 2003 (doi:10.1001/jama.289.21.JOC30885).", "date": "2003", "authors": ["Christopher M. Booth 1, Larissa M. Matukas 1, George A. Tomlinson 1, Anita R. Rachlis 1, David B. Rose 1, Hy A. Dwosh 1, Sharon L. Walmsley 1, Tony Mazzulli 1, 2, Monica Avendano 1, Peter Derkach 1, Issa E. Ephtimios 1, Ian Kitai 1, Barbara D. Mederski 1, Steven B. Shadowitz 1, Wayne L. Gold 1, 2, Laura A. Hawryluck 1, Elizabeth Rea 1, Jordan S. Chenkin 1, David W. Cescon 1, Susan M. Poutanen 1, 2, 3, Allan S. Detsky 1"], "references": [2115709314, 2104548316, 2131262274, 2100820722, 2125251240, 2136883754, 1966714873, 2463755683, 2136166622, 2158075347]}, {"id": 2140143765, "title": "Clinical features and virological analysis of a case of Middle East respiratory syndrome coronavirus infection", "abstract": "Summary Background The Middle East respiratory syndrome coronavirus (MERS-CoV) is an emerging virus involved in cases and case clusters of severe acute respiratory infection in the Arabian Peninsula, T unisia, Morocco, France, Italy, Germany, and the UK. We provide a full description of a fatal case of MERS-CoV infection and associated phylogenetic analyses. Methods We report data for a patient who was admitted to the Klinikum Schwabing (Munich, Germany) for severe acute respiratory infection. We did diagnostic RT -PCR and indirect immunofl uorescence. From time of diagnosis, respiratory, faecal, and urine samples were obtained for virus quantifi cation. We constructed a maximum likelihood tree of the fi ve available complete MERS-CoV genomes.", "date": "2013", "authors": ["Christian Drosten 1, Michael Seilmaier 2, Victor M Corman 1, Wulf Hartmann 2, Gregor Scheible 2, Stefan Sack 2, Wolfgang Guggemos 2, Rene Kallies 1, Doreen Muth 1, Sandra Junglen 1, Marcel A M\u00fcller 1, Walter Haas 3, Hana Guberina 4, Tim R\u00f6hnisch 5, Monika Schmid-Wendtner 5, Souhaib Aldabbagh 1, Ulf Dittmer 4, Hermann Gold 6, Petra Graf 6, Frank Bonin 7, Andrew Rambaut 8, 9, Clemens-Martin Wendtner 2"], "references": [2166867592, 2103546861, 2132260239, 1703839189, 2119111857, 2112147913, 2045002682, 1852588318, 2119775949, 1690366459]}, {"id": 2119775949, "title": "Clinical features and viral diagnosis of two cases of infection with Middle East Respiratory Syndrome coronavirus: a report of nosocomial transmission", "abstract": "Summary Background Human infection with a novel coronavirus named Middle East Respiratory Syndrome coronavirus (MERS-CoV) was first identified in Saudi Arabia and the Middle East in September, 2012, with 44 laboratory-confirmed cases as of May 23, 2013. We report detailed clinical and virological data for two related cases of MERS-CoV disease, after nosocomial transmission of the virus from one patient to another in a French hospital. Methods Patient 1 visited Dubai in April, 2013; patient 2 lives in France and did not travel abroad. Both patients had underlying immunosuppressive disorders. We tested specimens from the upper (nasopharyngeal swabs) or the lower (bronchoalveolar lavage, sputum) respiratory tract and whole blood, plasma, and serum specimens for MERS-CoV by real-time RT-PCR targeting the upE and Orf1A genes of MERS-CoV. Findings Initial clinical presentation included fever, chills, and myalgia in both patients, and for patient 1, diarrhoea. Respiratory symptoms rapidly became predominant with acute respiratory failure leading to mechanical ventilation and extracorporeal membrane oxygenation (ECMO). Both patients developed acute renal failure. MERS-CoV was detected in lower respiratory tract specimens with high viral load (eg, cycle threshold [Ct] values of 22\u00b79 for upE and 24 for Orf1a for a bronchoalveolar lavage sample from patient 1; Ct values of 22\u00b75 for upE and 23\u00b79 for Orf1a for an induced sputum sample from patient 2), whereas nasopharyngeal specimens were weakly positive or inconclusive. The two patients shared the same room for 3 days. The incubation period was estimated at 9\u201312 days for the second case. No secondary transmission was documented in hospital staff despite the absence of specific protective measures before the diagnosis of MERS-CoV was suspected. Patient 1 died on May 28, due to refractory multiple organ failure. Interpretation Patients with respiratory symptoms returning from the Middle East or exposed to a confirmed case should be isolated and investigated for MERS-CoV with lower respiratory tract sample analysis and an assumed incubation period of 12 days. Immunosuppression should also be taken into account as a risk factor. Funding French Institute for Public Health Surveillance, ANR grant Labex Integrative Biology of Emerging Infectious Diseases, and the European Community's Seventh Framework Programme projects EMPERIE and PREDEMICS.", "date": "2013", "authors": ["Benoit Guery 1, Julien Poissy 1, Loubna El Mansouf 2, Caroline S\u00e9journ\u00e9 3, Nicolas Ettahar 4, Xavier Lemaire 2, Fanny Vuotto 1, Anne Goffard 1, Sylvie Behillil 5, 6, 7, Vincent Enouf 5, 6, 7, Val\u00e9rie Caro 5, Alexandra Mailles 8, Didier Che 8, Jean Claude Manuguerra 5, Daniel Mathieu 1, Arnaud Fontanet 5, 9, Sylvie Van Der Werf 5, 6, 7"], "references": [2166867592, 2129542667, 1703839189, 2112147913, 2113457186, 1852588318, 2163627712, 2046153984, 1690366459, 1998725525]}, {"id": 2195009776, "title": "A SARS-like cluster of circulating bat coronaviruses shows potential for human emergence", "abstract": "The emergence of severe acute respiratory syndrome coronavirus (SARS-CoV) and Middle East respiratory syndrome (MERS)-CoV underscores the threat of cross-species transmission events leading to outbreaks in humans. Here we examine the disease potential of a SARS-like virus, SHC014-CoV, which is currently circulating in Chinese horseshoe bat populations. Using the SARS-CoV reverse genetics system, we generated and characterized a chimeric virus expressing the spike of bat coronavirus SHC014 in a mouse-adapted SARS-CoV backbone. The results indicate that group 2b viruses encoding the SHC014 spike in a wild-type backbone can efficiently use multiple orthologs of the SARS receptor human angiotensin converting enzyme II (ACE2), replicate efficiently in primary human airway cells and achieve in vitro titers equivalent to epidemic strains of SARS-CoV. Additionally, in vivo experiments demonstrate replication of the chimeric virus in mouse lung with notable pathogenesis. Evaluation of available SARS-based immune-therapeutic and prophylactic modalities revealed poor efficacy; both monoclonal antibody and vaccine approaches failed to neutralize and protect from infection with CoVs using the novel spike protein. On the basis of these findings, we synthetically re-derived an infectious full-length SHC014 recombinant virus and demonstrate robust viral replication both in vitro and in vivo. Our work suggests a potential risk of SARS-CoV re-emergence from viruses currently circulating in bat populations.", "date": "2015", "authors": ["Vineet D. Menachery 1, Boyd L. Yount 1, Kari Debbink 1, Sudhakar Agnihothram 2, Lisa E. Gralinski 1, Jessica A. Plante 1, Rachel L. Graham 1, Trevor Scobey 1, Xing Yi Ge 3, Eric F. Donaldson 1, Scott H. Randell 1, Antonio Lanzavecchia 4, Wayne A. Marasco 5, Zhengli Li Shi 3, Ralph S. Baric 1"], "references": [1993577573, 2094993149, 2126707939, 2092969802, 2152528032, 2143230291, 2074618762, 1995367098, 1963580683, 1567281461]}, {"id": 2115555188, "title": "Middle East Respiratory Syndrome Coronavirus: Another Zoonotic Betacoronavirus Causing SARS-Like Disease", "abstract": "The source of the severe acute respiratory syndrome (SARS) epidemic was traced to wildlife market civets and ultimately to bats. Subsequent hunting for novel coronaviruses (CoVs) led to the discovery of two additional human and over 40 animal CoVs, including the prototype lineage C betacoronaviruses, Tylonycteris bat CoV HKU4 and Pipistrellus bat CoV HKU5; these are phylogenetically closely related to the Middle East respiratory syndrome (MERS) CoV, which has affected more than 1,000 patients with over 35% fatality since its emergence in 2012. All primary cases of MERS are epidemiologically linked to the Middle East. Some of these patients had contacted camels which shed virus and/or had positive serology. Most secondary cases are related to health care-associated clusters. The disease is especially severe in elderly men with comorbidities. Clinical severity may be related to MERS-CoV's ability to infect a broad range of cells with DPP4 expression, evade the host innate immune response, and induce cytokine dysregulation. Reverse transcription-PCR on respiratory and/or extrapulmonary specimens rapidly establishes diagnosis. Supportive treatment with extracorporeal membrane oxygenation and dialysis is often required in patients with organ failure. Antivirals with potent in vitro activities include neutralizing monoclonal antibodies, antiviral peptides, interferons, mycophenolic acid, and lopinavir. They should be evaluated in suitable animal models before clinical trials. Developing an effective camel MERS-CoV vaccine and implementing appropriate infection control measures may control the continuing epidemic.", "date": "2015", "authors": ["Jasper F. W. Chan , Susanna K. P. Lau , Kelvin K. W. To , Vincent C. C. Cheng , Patrick C. Y. Woo , Kwok-Yung Yuen"], "references": [2166867592, 2107053896, 2025170735, 2006434809, 2129542667, 1993577573, 1703839189, 2119111857, 2565805236, 2149508011]}, {"id": 2298153446, "title": "SARS-like WIV1-CoV poised for human emergence", "abstract": "Outbreaks from zoonotic sources represent a threat to both human disease as well as the global economy. Despite a wealth of metagenomics studies, methods to leverage these datasets to identify future threats are underdeveloped. In this study, we describe an approach that combines existing metagenomics data with reverse genetics to engineer reagents to evaluate emergence and pathogenic potential of circulating zoonotic viruses. Focusing on the severe acute respiratory syndrome (SARS)-like viruses, the results indicate that the WIV1-coronavirus (CoV) cluster has the ability to directly infect and may undergo limited transmission in human populations. However, in vivo attenuation suggests additional adaptation is required for epidemic disease. Importantly, available SARS monoclonal antibodies offered success in limiting viral infection absent from available vaccine approaches. Together, the data highlight the utility of a platform to identify and prioritize prepandemic strains harbored in animal reservoirs and document the threat posed by WIV1-CoV for emergence in human populations.", "date": "2016", "authors": ["Vineet D. Menachery 1, Boyd L. Yount 1, Amy C Sims 1, Kari Debbink 1, Sudhakar S. Agnihothram 2, Lisa E. Gralinski 1, Rachel Lauren Graham 1, Trevor Scobey 1, Jessica A. Plante 1, Scott R. Royal 1, Jesica Swanstrom 1, Timothy Patrick Sheahan 1, Raymond J Pickles 1, 3, Davide Corti 4, Scott H Randell 1, Antonio Lanzavecchia 4, Wayne A. Marasco 5, Ralph S Baric 1, 3"], "references": [1993577573, 2094993149, 2126707939, 2101176145, 2092969802, 2152528032, 1998383538, 2143230291, 1995367098, 1963580683]}, {"id": 2525468044, "title": "Viral Load Kinetics of MERS Coronavirus Infection.", "abstract": "Middle East respiratory syndrome coronavirus continues to circulate in the Middle East. During a recent outbreak in Korea, changes in MERS coronavirus viral load were determined during the course of illness in 17 patients.", "date": "2016", "authors": ["Myoung Don Oh 1, Wan Beom Park 1, Pyoeng Gyun Choe 1, Su Jin Choi 2, Jong I.I. Kim 1, Jeesoo Chae 1, Sung Sup Park 1, Eui Chong Kim 1, Hong Sang Oh 1, Eun Jung Kim 1, Eun Young Nam 1, Sun Hee Na 1, Dong Ki Kim 1, Sang Min Lee 1, Kyoung Ho Song 1, Ji Hwan Bang 1, Eu Suk Kim 1, Hong Bin Kim 1, Sang Won Park 1, Nam Joong Kim 1"], "references": [2126707939, 2103118479, 2405185968, 2134527559]}, {"id": 1945961678, "title": "Treatment With Lopinavir/Ritonavir or Interferon-\u03b21b Improves Outcome of MERS-CoV Infection in a Nonhuman Primate Model of Common Marmoset", "abstract": "Middle East respiratory syndrome coronavirus (MERS-CoV) causes severe disease in human with an overall case-fatality rate of >35%. Effective antivirals are crucial for improving the clinical outcome of MERS. Although a number of repurposed drugs, convalescent-phase plasma, antiviral peptides, and neutralizing antibodies exhibit anti-MERS-CoV activity in vitro, most are not readily available or have not been evaluated in nonhuman primates. We assessed 3 repurposed drugs with potent in vitro anti-MERS-CoV activity (mycophenolate mofetil [MMF], lopinavir/ritonavir, and interferon-\u03b21b) in common marmosets with severe disease resembling MERS in humans. The lopinavir/ritonavir-treated and interferon-\u03b21b-treated animals had better outcome than the untreated animals, with improved clinical (mean clinical scores \u219350.9%-95.0% and \u2193weight loss than the untreated animals), radiological (minimal pulmonary infiltrates), and pathological (mild bronchointerstitial pneumonia) findings, and lower mean viral loads in necropsied lung (\u21930.59-1.06 log10 copies/glyceraldehyde 3-phosphate dehydrogenase [GAPDH]; P < .050) and extrapulmonary (\u21930.11-1.29 log10 copies/GAPDH; P < .050 in kidney) tissues. In contrast, all MMF-treated animals developed severe and/or fatal disease with higher mean viral loads (\u21910.15-0.54 log10 copies/GAPDH) than the untreated animals. The mortality rate at 36 hours postinoculation was 67% (untreated and MMF-treated) versus 0-33% (lopinavir/ritonavir-treated and interferon-\u03b21b-treated). Lopinavir/ritonavir and interferon-\u03b21b alone or in combination should be evaluated in clinical trials. MMF alone may worsen MERS and should not be used.", "date": "2015", "authors": ["Jasper Fuk Woo Chan 1, Yanfeng Yao 2, Man Lung Yeung 3, Wei Deng 2, Linlin Bao 2, Lilong Jia 3, Fengdi Li 2, Chong Xiao 2, Hong Gao 2, Pin Yu 2, Jian Piao Cai 3, Hin Chu 3, Jie Zhou 3, Honglin Chen 1, Chuan Qin 2, Kwok Yung Yuen 1"], "references": [2166867592, 2006434809, 2115555188, 2034462612, 1977050884, 2150120685, 1947409115, 2078121682, 2017248106, 2096238447]}, {"id": 2099941783, "title": "Presence of Middle East respiratory syndrome coronavirus antibodies in Saudi Arabia: a nationwide, cross-sectional, serological study", "abstract": "Summary Background Scientific evidence suggests that dromedary camels are the intermediary host for the Middle East respiratory syndrome coronavirus (MERS-CoV). However, the actual number of infections in people who have had contact with camels is unknown and most index patients cannot recall any such contact. We aimed to do a nationwide serosurvey in Saudi Arabia to establish the prevalence of MERS-CoV antibodies, both in the general population and in populations of individuals who have maximum exposure to camels. Methods In the cross-sectional serosurvey, we tested human serum samples obtained from healthy individuals older than 15 years who attended primary health-care centres or participated in a national burden-of-disease study in all 13 provinces of Saudi Arabia. Additionally, we tested serum samples from shepherds and abattoir workers with occupational exposure to camels. Samples were screened by recombinant ELISA and MERS-CoV seropositivity was confirmed by recombinant immunofluorescence and plaque reduction neutralisation tests. We used two-tailed Mann Whitney U exact tests, \u03c7 2 , and Fisher's exact tests to analyse the data. Findings Between Dec 1, 2012, and Dec 1, 2013, we obtained individual serum samples from 10\u2008009 individuals. Anti-MERS-CoV antibodies were confirmed in 15 (0\u00b715%; 95% CI 0\u00b709\u20130\u00b724) of 10\u2008009 people in six of the 13 provinces. The mean age of seropositive individuals was significantly younger than that of patients with reported, laboratory-confirmed, primary Middle Eastern respiratory syndrome (43\u00b75 years [SD 17\u00b73] vs 53\u00b78 years [17\u00b75]; p=0\u00b7008). Men had a higher antibody prevalence than did women (11 [0\u00b725%] of 4341 vs two [0\u00b705%] of 4378; p=0\u00b7028) and antibody prevalence was significantly higher in central versus coastal provinces (14 [0\u00b726%] of 5479 vs one [0\u00b702%] of 4529; p=0\u00b7003). Compared with the general population, seroprevalence of MERS-CoV antibodies was significantly increased by 15 times in shepherds (two [2\u00b73%] of 87, p=0\u00b70004) and by 23 times in slaughterhouse workers (five [3\u00b76%] of 140; p Interpretation Seroprevalence of MERS-CoV antibodies was significantly higher in camel-exposed individuals than in the general population. By simple multiplication, a projected 44\u2008951 (95% CI 26\u2008971\u201371\u2008922) individuals older than 15 years might be seropositive for MERS-CoV in Saudi Arabia. These individuals might be the source of infection for patients with confirmed MERS who had no previous exposure to camels. Funding European Union, German Centre for Infection Research, Federal Ministry of Education and Research, German Research Council, and Ministry of Health of Saudi Arabia.", "date": "2015", "authors": ["Marcel A. M\u00fcller 1, Benjamin Meyer 1, Victor M. Corman 1, Malak Al-Masri 2, Abdulhafeez Turkestani 3, Daniel Ritz 1, Andrea Sieberg 1, Souhaib Aldabbagh 1, Berend J. Bosch 4, Erik Lattwein 5, Raafat F. Alhakeem 2, Abdullah M. Assiri 2, Ali M. Albarrak 6, Ali M. Al-Shangiti 7, Jaffar A. Al-Tawfiq 8, 9, Paul Wikramaratna 10, Abdullah A. Alrabeeah 11, Christian Drosten 1, Ziad A. Memish 12"], "references": []}, {"id": 3004280078, "title": "A pneumonia outbreak associated with a new coronavirus of probable bat origin", "abstract": "Since the outbreak of severe acute respiratory syndrome (SARS) 18 years ago, a large number of SARS-related coronaviruses (SARSr-CoVs) have been discovered in their natural reservoir host, bats1-4. Previous studies have shown that some bat SARSr-CoVs have the potential to infect humans5-7. Here we report the identification and characterization of a new coronavirus (2019-nCoV), which caused an epidemic of acute respiratory syndrome in humans in Wuhan, China. The epidemic, which started on 12 December 2019, had caused 2,794 laboratory-confirmed infections including 80 deaths by 26 January 2020. Full-length genome sequences were obtained from five patients at an early stage of the outbreak. The sequences are almost identical and share 79.6% sequence identity to SARS-CoV. Furthermore, we show that 2019-nCoV is 96% identical at the whole-genome level to a bat coronavirus. Pairwise protein sequence analysis of seven conserved non-structural proteins domains show that this virus belongs to the species of SARSr-CoV. In addition, 2019-nCoV virus isolated from the bronchoalveolar lavage fluid of a critically ill patient could be neutralized by sera from several patients. Notably, we confirmed that 2019-nCoV uses the same cell entry receptor-angiotensin converting enzyme II (ACE2)-as SARS-CoV.", "date": "2020", "authors": ["Peng Zhou 1, Xing Lou Yang 1, Xian Guang Wang 2, Ben Hu 1, Lei Zhang 1, Wei Zhang 1, Hao Rui Si 1, Yan Zhu 1, Bei Li 1, Chao Lin Huang 2, Hui Dong Chen 2, Jing Chen 1, Yun Luo 1, Hua Guo 1, Ren Di Jiang 1, Mei Qin Liu 1, Ying Chen 1, Xu Rui Shen 1, Xi Wang 1, Xiao Shuang Zheng 1, Kai Zhao 1, Quan Jiao Chen 1, Fei Deng 1, Lin Lin Liu 3, Bing Yan 1, Fa Xian Zhan 3, Yan Yi Wang 1, Geng Fu Xiao 1, Zheng Li Shi 1"], "references": [2903899730, 2166867592, 2132260239, 1993577573, 2775086803, 1966238900, 2195009776, 2103503670, 2918873120, 2298153446]}, {"id": 3009912996, "title": "SARS-CoV-2 Cell Entry Depends on ACE2 and TMPRSS2 and Is Blocked by a Clinically Proven Protease Inhibitor.", "abstract": "The recent emergence of the novel, pathogenic SARS-coronavirus 2 (SARS-CoV-2) in China and its rapid national and international spread pose a global health emergency. Cell entry of coronaviruses depends on binding of the viral spike (S) proteins to cellular receptors and on S protein priming by host cell proteases. Unravelling which cellular factors are used by SARS-CoV-2 for entry might provide insights into viral transmission and reveal therapeutic targets. Here, we demonstrate that SARS-CoV-2 uses the SARS-CoV receptor ACE2 for entry and the serine protease TMPRSS2 for S protein priming. A TMPRSS2 inhibitor approved for clinical use blocked entry and might constitute a treatment option. Finally, we show that the sera from convalescent SARS patients cross-neutralized SARS-2-S-driven entry. Our results reveal important commonalities between SARS-CoV-2 and SARS-CoV infection and identify a potential target for antiviral intervention.", "date": "2020", "authors": ["Markus Hoffmann 1, Hannah Kleine-Weber 1, Simon Schroeder 2, Nadine Kr\u00fcger 3, Tanja Herrler 4, Sandra Erichsen 5, Tobias S. Schiergens 6, Georg Herrler 7, Nai Huei Wu 7, Andreas Nitsche 8, Marcel A. M\u00fcller 2, Christian Drosten 2, 9, Stefan P\u00f6hlmann 1"], "references": [3001118548, 3001897055, 3002539152, 3004280078, 3001195213, 3001465255, 2799524357, 3002533507, 2470646526, 1993577573]}, {"id": 3007940623, "title": "Pathological findings of COVID-19 associated with acute respiratory distress syndrome.", "abstract": "", "date": "2020", "authors": ["Zhe Xu 1, Lei Shi 1, Yijin Wang 2, Jiyuan Zhang 1, Lei Huang 1, Chao Zhang 1, Shuhong Liu 2, Peng Zhao 1, Hongxia Liu 1, Li Zhu 2, Yanhong Tai 2, Changqing Bai 3, Tingting Gao 2, Jinwen Song 1, Peng Xia 1, Jinghui Dong 4, Jingmin Zhao 2, Fu Sheng Wang 1"], "references": [3001118548, 3002539152, 3003217347, 2256430766, 2158118659]}, {"id": 1948751323, "title": "Hypercolumns for object segmentation and fine-grained localization", "abstract": "Recognition algorithms based on convolutional networks (CNNs) typically use the output of the last layer as a feature representation. However, the information in this layer may be too coarse spatially to allow precise localization. On the contrary, earlier layers may be precise in localization but will not capture semantics. To get the best of both worlds, we define the hypercolumn at a pixel as the vector of activations of all CNN units above that pixel. Using hypercolumns as pixel descriptors, we show results on three fine-grained localization tasks: simultaneous detection and segmentation [22], where we improve state-of-the-art from 49.7 mean APr [22] to 60.0, keypoint localization, where we get a 3.3 point boost over [20], and part labeling, where we show a 6.6 point gain over a strong baseline.", "date": "2015", "authors": ["Bharath Hariharan 1, Pablo Arbelaez 2, Ross Girshick 3, Jitendra Malik 1"], "references": [2618530766, 2962835968, 2102605133, 1903029394, 2168356304, 2109255472, 2156303437, 2022508996, 2118585731, 1507506748]}, {"id": 2167510172, "title": "Deep Neural Networks Segment Neuronal Membranes in Electron Microscopy Images", "abstract": "We address a central problem of neuroanatomy, namely, the automatic segmentation of neuronal structures depicted in stacks of electron microscopy (EM) images. This is necessary to efficiently map 3D brain structure and connectivity. To segment biological neuron membranes, we use a special type of deep artificial neural network as a pixel classifier. The label of each pixel (membrane or non-membrane) is predicted from raw pixel values in a square window centered on it. The input layer maps each window pixel to a neuron. It is followed by a succession of convolutional and max-pooling layers which preserve 2D information and extract features with increasing levels of abstraction. The output layer produces a calibrated probability for each class. The classifier is trained by plain gradient descent on a 512 \u00d7 512 \u00d7 30 stack with known ground truth, and tested on a stack of the same size (ground truth unknown to the authors) by the organizers of the ISBI 2012 EM Segmentation Challenge. Even without problem-specific postprocessing, our approach outperforms competing techniques by a large margin in all three considered metrics, i.e. rand error, warping error and pixel error. For pixel error, our approach is the only one outperforming a second human observer.", "date": "2012", "authors": ["Dan Ciresan , Alessandro Giusti , Luca M. Gambardella , J\u00fcrgen Schmidhuber"], "references": [2310919327, 2141125852, 2143516773, 2156163116, 2148461049, 1624854622, 2132424367, 1969013163, 2149194912, 1523493493]}, {"id": 1893585201, "title": "Learning to generate chairs with convolutional neural networks", "abstract": "We train a generative convolutional neural network which is able to generate images of objects given object type, viewpoint, and color. We train the network in a supervised manner on a dataset of rendered 3D chair models. Our experiments show that the network does not merely learn all images by heart, but rather finds a meaningful representation of a 3D chair model allowing it to assess the similarity of different chairs, interpolate between given viewpoints to generate the missing ones, or invent new chair styles by interpolating between chairs from the training set. We show that the network can be used to find correspondences between different chairs from the dataset, outperforming existing approaches on this task.", "date": "2015", "authors": ["Alexey Dosovitskiy , Jost Tobias Springenberg , Thomas Brox"], "references": [2618530766, 2102605133, 2099471712, 2155893237, 2136922672, 1849277567, 2963542991, 1959608418, 2100495367, 2155541015]}, {"id": 2148349024, "title": "Discriminative Unsupervised Feature Learning with Convolutional Neural Networks", "abstract": "Current methods for training convolutional neural networks depend on large amounts of labeled samples for supervised training. In this paper we present an approach for training a convolutional neural network using only unlabeled data. We train the network to discriminate between a set of surrogate classes. Each surrogate class is formed by applying a variety of transformations to a randomly sampled 'seed' image patch. We find that this simple feature learning algorithm is surprisingly successful when applied to visual object recognition. The feature representation learned by our algorithm achieves classification results matching or outperforming the current state-of-the-art for unsupervised learning on several popular datasets (STL-10, CIFAR-10, Caltech-101).", "date": "2014", "authors": ["Alexey Dosovitskiy , Jost Tobias Springenberg , Martin Riedmiller , Thomas Brox"], "references": [2618530766, 2102605133, 2155893237, 1849277567, 2109255472, 3118608800, 1904365287, 2158899491, 2155541015, 2963911037]}, {"id": 1909499787, "title": "MERS, SARS, and Ebola: The Role of Super-Spreaders in Infectious Disease.", "abstract": "Super-spreading occurs when a single patient infects a disproportionate number of contacts. The 2015 MERS-CoV, 2003 SARS-CoV, and to a lesser extent 2014-15 Ebola virus outbreaks were driven by super-spreaders. We summarize documented super-spreading in these outbreaks, explore contributing factors, and suggest studies to better understand super-spreading.", "date": "2015", "authors": ["Gary Wong 1, Wenjun Liu 1, Yingxia Liu 2, Boping Zhou 2, Yuhai Bi 1, George F. Gao 3"], "references": [2115102869, 1975375203, 2096145431, 2227495319, 1994871753, 1979807576, 1942680103, 2024845268, 2135540513, 2089797630]}, {"id": 3027518954, "title": "Pathogen genomics in public health", "abstract": "", "date": "2020", "authors": ["Gregory L. Armstrong 1, Duncan R. MacCannell 1, Jill Taylor 2, Heather A. Carleton 1, Elizabeth B. Neuhaus 1, Richard S. Bradbury 3, James E. Posey 4, Marta Gwinn 1"], "references": [3001897055, 3012538234, 3013266235, 3095124337, 3036563213, 3009375872, 3023674326, 3096165595, 3103820312, 3045903687]}, {"id": 2792024998, "title": "From \u201cA\u201dIV to \u201cZ\u201dIKV: Attacks from Emerging and Re-emerging Pathogens", "abstract": "100 years after the infamous \u201cSpanish flu\u201d pandemic, the 2017\u20132018 flu season has been severe, with numerous infections worldwide. In between, there have been continuous, relentless attacks from (re-)emerging viruses. To fully understand viral pathogenesis and develop effective medical countermeasures, we must strengthen current surveillance and basic research efforts.", "date": "2018", "authors": ["George F. Gao"], "references": [1975375203, 2116682907, 2788045019, 1783641736, 1942680103, 1967283148, 2081635462, 2793181185, 2473338860, 2782496877]}, {"id": 2955025503, "title": "Viral and Bacterial Etiology of Acute Febrile Respiratory Syndrome among Patients in Qinghai, China", "abstract": "Abstract Objective This study was conducted to investigate the viral and bacterial etiology and epidemiology of patients with acute febrile respiratory syndrome (AFRS) in Qinghai using a commercial routine multiplex-ligation-nucleic acid amplification test (NAT)-based assay. Methods A total of 445 nasopharyngeal swabs specimens from patients with AFRS were analyzed using the RespiFinderSmart22kit (PathoFinder BV, Netherlands) and the LightCycler 480 real-time PCR system. Results Among the 225 (225/445, 51%) positive specimens, 329 positive pathogens were detected, including 298 (90.58%) viruses and 31 (9%) bacteria. The most commonly detected pathogens were influenza virus (IFV; 37.39%; 123/329), adenovirus (AdV; 17.02%; 56/329), human coronaviruses (HCoVs; 10.94%; 36/329), rhinovirus/enterovirus (RV/EV; 10.03%; 33/329), parainfluenza viruses (PIVs; 8.51%; 28/329), and Mycoplasma pneumoniae (M. pneu; 8.51%; 28/329), respectively. Among the co-infected cases (17.53%; 78/445), IFV/AdV and IFV/M. pneu were the most common co-infections. Most of the respiratory viruses were detected in summer and fall. Conclusion In our study, IFV-A was the most common respiratory pathogen among 22 detected pathogens, followed by AdV, HCoV, RV/EV, PIV, and M. pneu. Bacteria appeared less frequently than viruses, and co-infection was the most common phenomenon among viral pathogens. Pathogens were distributed among different age groups and respiratory viruses were generally active in July, September, and November. Enhanced surveillance and early detection can be useful in the diagnosis, treatment, and prevention of AFRS, as well as for guiding the development of appropriate public health strategies.", "date": "2019", "authors": ["Gao Shan Liu 1, Hong Li 2, Sheng Cang Zhao 2, Rou Jian Lu 1, Pei Hua Niu 1, Wen Jie Tan 1"], "references": [2078917493, 2120839730, 2287076968, 2083870139, 2081835188, 2131338055, 2412526156, 2159773954, 2129144407, 2016253387]}, {"id": 2257005270, "title": "Coronaviruses and the human airway: a universal system for virus-host interaction studies.", "abstract": "Human coronaviruses (HCoVs) are large RNA viruses that infect the human respiratory tract. The emergence of both Severe Acute Respiratory Syndrome and Middle East Respiratory syndrome CoVs as well as the yearly circulation of four common CoVs highlights the importance of elucidating the different mechanisms employed by these viruses to evade the host immune response, determine their tropism and identify antiviral compounds. Various animal models have been established to investigate HCoV infection, including mice and non-human primates. To establish a link between the research conducted in animal models and humans, an organotypic human airway culture system, that recapitulates the human airway epithelium, has been developed. Currently, different cell culture systems are available to recapitulate the human airways, including the Air-Liquid Interface (ALI) human airway epithelium (HAE) model. Tracheobronchial HAE cultures recapitulate the primary entry point of human respiratory viruses while the alveolar model allows for elucidation of mechanisms involved in viral infection and pathogenesis in the alveoli. These organotypic human airway cultures represent a universal platform to study respiratory virus-host interaction by offering more detailed insights compared to cell lines. Additionally, the epidemic potential of this virus family highlights the need for both vaccines and antivirals. No commercial vaccine is available but various effective antivirals have been identified, some with potential for human treatment. These morphological airway cultures are also well suited for the identification of antivirals, evaluation of compound toxicity and viral inhibition.", "date": "2016", "authors": ["Hulda Run Jonsdottir 1, 2, Ronald Dijkman 1, 2"], "references": [2166867592, 2132260239, 1993577573, 2119111857, 2116586125, 2195009776, 311927316, 2167384912, 2111412754, 2170933940]}, {"id": 3000834295, "title": "Coronavirus Infections-More Than Just the Common Cold.", "abstract": "", "date": "2020", "authors": ["Catharine I. Paules 1, Hilary D. Marston 2, Anthony S. Fauci 2"], "references": [3000413850, 2470646526, 2909194930, 3027659922, 3027264380, 1993435091, 2718090702, 2792208289, 3030422584, 3023275846]}, {"id": 3003951199, "title": "Importation and Human-to-Human Transmission of a Novel Coronavirus in Vietnam.", "abstract": "Human-to-Human Coronavirus Transmission in Vietnam The authors describe transmission of 2019-nCoV from a father, who had flown with his wife from Wuhan to Hanoi, to the son, who met his father and ...", "date": "2020", "authors": ["Lan T. Phan 1, Thuong V. Nguyen 1, Quang C. Luong 1, Thinh V. Nguyen 1, Hieu T. Nguyen 1, Hung Q. Le 2, Thuc T. Nguyen 2, Thang M. Cao 3, Quang D. Pham 3"], "references": [3001897055]}, {"id": 2999409984, "title": "The continuing 2019-nCoV epidemic threat of novel coronaviruses to global health - The latest 2019 novel coronavirus outbreak in Wuhan, China.", "abstract": "", "date": "2020", "authors": ["David S. Hui 1, Esam Ei Azhar 2, Tariq A. Madani 2, Francine Ntoumi 3, Richard Kock 4, Osman Dar 5, Giuseppe Ippolito 6, Timothy D. Mchugh 7, Ziad A. Memish 8, Christian Drosten 9, Alimuddin Zumla 7, Eskild Petersen 10"], "references": [2981657433, 2981752008, 3000092258, 2442480670, 2414957595]}, {"id": 1803784511, "title": "Acute respiratory distress syndrome: the Berlin Definition.", "abstract": "The acute respiratory distress syndrome (ARDS) was defined in 1994 by the American-European Consensus Conference (AECC); since then, issues regarding the reliability and validity of this definition have emerged. Using a consensus process, a panel of experts convened in 2011 (an initiative of the European Society of Intensive Care Medicine endorsed by the American Thoracic Society and the Society of Critical Care Medicine) developed the Berlin Definition, focusing on feasibility, reliability, validity, and objective evaluation of its performance. A draft definition proposed 3 mutually exclusive categories of ARDS based on degree of hypoxemia: mild (200 mm Hg < PaO2/FIO2 \u2264 300 mm Hg), moderate (100 mm Hg < PaO2/FIO2 \u2264 200 mm Hg), and severe (PaO2/FIO2 \u2264 100 mm Hg) and 4 ancillary variables for severe ARDS: radiographic severity, respiratory system compliance (\u226440 mL/cm H2O), positive end-expiratory pressure (\u226510 cm H2O), and corrected expired volume per minute (\u226510 L/min). The draft Berlin Definition was empirically evaluated using patient-level meta-analysis of 4188 patients with ARDS from 4 multicenter clinical data sets and 269 patients with ARDS from 3 single-center data sets containing physiologic information. The 4 ancillary variables did not contribute to the predictive validity of severe ARDS for mortality and were removed from the definition. Using the Berlin Definition, stages of mild, moderate, and severe ARDS were associated with increased mortality (27%; 95% CI, 24%-30%; 32%; 95% CI, 29%-34%; and 45%; 95% CI, 42%-48%, respectively; P < .001) and increased median duration of mechanical ventilation in survivors (5 days; interquartile [IQR], 2-11; 7 days; IQR, 4-14; and 9 days; IQR, 5-17, respectively; P < .001). Compared with the AECC definition, the final Berlin Definition had better predictive validity for mortality, with an area under the receiver operating curve of 0.577 (95% CI, 0.561-0.593) vs 0.536 (95% CI, 0.520-0.553; P < .001). This updated and revised Berlin Definition for ARDS addresses a number of the limitations of the AECC definition. The approach of combining consensus discussions with empirical evaluation may serve as a model to create more accurate, evidence-based, critical illness syndrome definitions and to better inform clinical care, research, and health services planning.", "date": "2012", "authors": ["Ards Definition Task Force 1, V Marco Ranieri 1, Gordon D Rubenfeld 2, B Taylor Thompson 2, Niall D Ferguson 3, Ellen Caldwell 2, Eddy Fan 4, Luigi Camporota 2, 5, Arthur S Slutsky 2"], "references": [2480133395, 2161328469, 2068854215, 2328176404, 2326364273, 1823772832, 2113752525, 1979469936, 2070070465, 2168829312]}, {"id": 2999318660, "title": "Outbreak of pneumonia of unknown etiology in Wuhan, China: The mystery and the miracle", "abstract": "", "date": "2020", "authors": ["Hongzhou Lu 1, Charles W. Stratton 2, Yi Wei Tang 3"], "references": [2470646526, 2132260239, 2103503670, 2255243349, 2766931063, 2134061616, 1997954607, 2158887145, 2121494157]}, {"id": 3002533507, "title": "A Novel Coronavirus Emerging in China - Key Questions for Impact Assessment.", "abstract": "A Novel Coronavirus Emerging in China A novel coronavirus, designated as 2019-nCoV, emerged in Wuhan, China, at the end of 2019. Although many details of the emergence of this virus remain unknown,...", "date": "2020", "authors": ["Vincent J. Munster 1, Marion Koopmans 2, Neeltje van Doremalen 1, Debby van Riel 2, Emmie de Wit 1"], "references": [3003668884, 3012099172, 3009912996, 3012284084, 3006645647, 3015792206, 3009834387, 3012454642, 3003901880, 3012415734]}, {"id": 3002715510, "title": "Another Decade, Another Coronavirus.", "abstract": "For the third time in as many decades, a zoonotic coronavirus has crossed species to infect human populations. This virus, provisionally called 2019-nCoV, was first identified in Wuhan, China, in p...", "date": "2020", "authors": ["Stanley Perlman"], "references": [3001897055, 2801339009, 2126707939, 2217313808, 2156273941, 2538584349, 96734778, 2002481497]}, {"id": 3001971765, "title": "Real-time tentative assessment of the epidemiological characteristics of novel coronavirus infections in Wuhan, China, as at 22 January 2020.", "abstract": "A novel coronavirus (2019-nCoV) causing severe acute respiratory disease emerged recently in Wuhan, China. Information on reported cases strongly indicates human-to-human spread, and the most recent information is increasingly indicative of sustained human-to-human transmission. While the overall severity profile among cases may change as more mild cases are identified, we estimate a risk of fatality among hospitalised cases at 14% (95% confidence interval: 3.9\u201332%).", "date": "2020", "authors": ["Peng Wu , Xinxin Hao , Eric H Y Lau , Jessica Y Wong , Kathy S M Leung , Joseph T Wu , Benjamin J Cowling , Gabriel M Leung"], "references": [2306794997, 1909499787, 2918873120, 2801339009, 2227495319, 1042757214, 2534644646, 2156614913, 2127974353, 2109088393]}, {"id": 2147166346, "title": "Transmission Dynamics and Control of Severe Acute Respiratory Syndrome", "abstract": "Severe acute respiratory syndrome (SARS) is a recently described illness of humans that has spread widely over the past 6 months. With the use of detailed epidemiologic data from Singapore and epidemic curves from other settings, we estimated the reproductive number for SARS in the absence of interventions and in the presence of control efforts. We estimate that a single infectious case of SARS will infect about three secondary cases in a population that has not yet instituted control measures. Public-health efforts to reduce transmission are expected to have a substantial impact on reducing the size of the epidemic.", "date": "2003", "authors": ["Marc Lipsitch 1, Ted Cohen 1, Ben Cooper 1, James M. Robins 1, Stefan Ma 2, Lyn James 2, Gowri Gopalakrishna 2, Suok Kai Chew 2, Chorh Chuan Tan 2, Matthew H. Samore 3, David Fisman 4, Megan Murray 1"], "references": [2132260239, 2104548316, 1606697907, 2011756067, 2318510691, 1965399019, 1979065938]}, {"id": 2149508011, "title": "Evidence for camel-to-human transmission of MERS coronavirus", "abstract": "We describe the isolation and sequencing of Middle East respiratory syndrome coronavirus (MERS-CoV) obtained from a dromedary camel and from a patient who died of laboratory-confirmed MERS-CoV infection after close contact with camels that had rhinorrhea. Nasal swabs collected from the patient and from one of his nine camels were positive for MERS-CoV RNA. In addition, MERS-CoV was isolated from the patient and the camel. The full genome sequences of the two isolates were identical. Serologic data indicated that MERS-CoV was circulating in the camels but not in the patient before the human infection occurred. These data suggest that this fatal case of human MERS-CoV infection was transmitted through close contact with an infected camel.", "date": "2014", "authors": ["Esam I. Azhar , Sherif A. El-Kafrawy , Suha A. Farraj , Ahmed M. Hassan , Muneera S. Al-Saeed , Anwar M. Hashem , Tariq A. Madani"], "references": [2166867592, 2107053896, 2160011624, 2045002682, 2113457186, 1852588318, 2145441153, 2119775949, 1690366459, 2049975503]}, {"id": 2999364275, "title": "Evolution of the novel coronavirus from the ongoing Wuhan outbreak and modeling of its spike protein for risk of human transmission", "abstract": "", "date": "2020", "authors": ["Xintian Xu 1, Ping Chen 1, Jingfang Wang 2, Jiannan Feng 3, Hui Zhou 1, Xuan Li 1, Wu Zhong 3, Pei Hao 1"], "references": [2605343262, 2775086803, 2119111857, 2404280981, 2060809301, 1982533785, 3021832855, 2126080553, 3000376083]}, {"id": 2991899552, "title": "Clinical Features Predicting Mortality Risk in Patients With Viral Pneumonia: The MuLBSTA Score.", "abstract": "Objective The aim of this study was to further clarify clinical characteristics and predict mortality risk among patients with viral pneumonia. Methods A total of 528 patients with viral pneumonia at RuiJin hospital in Shanghai from May 2015 to May 2019 were recruited. Multiplex real-time RT-PCR was used to detect respiratory viruses. Demographic information, comorbidities, routine laboratory examinations, immunological indexes, etiological detections, radiological images and treatment were collected on admission. Results 76 (14.4%) patients died within 90 days in hospital. A predictive MuLBSTA score was calculated on the basis of a multivariate logistic regression model in order to predict mortality with a weighted score that included multilobular infiltrates (OR = 5.20, 95% CI 1.41-12.52, p = 0.010; 5 points), lymphocyte \u2264 0.8\u2217109/L (OR = 4.53, 95% CI 2.55-8.05, p < 0.001; 4 points), bacterial coinfection (OR = 3.71, 95% CI 2.11-6.51, p < 0.001; 4 points), acute-smoker (OR = 3.19, 95% CI 1.34-6.26, p = 0.001; 3 points), quit-smoker (OR = 2.18, 95% CI 0.99-4.82, p = 0.054; 2 points), hypertension (OR = 2.39, 95% CI 1.55-4.26, p = 0.003; 2 points) and age \u226560 years (OR = 2.14, 95% CI 1.04-4.39, p = 0.038; 2 points). 12 points was used as a cut-off value for mortality risk stratification. This model showed sensitivity of 0.776, specificity of 0.778 and a better predictive ability than CURB-65 (AUROC = 0.773 vs. 0.717, p < 0.001). Conclusion Here, we designed an easy-to-use clinically predictive tool for assessing 90-day mortality risk of viral pneumonia. It can accurately stratify hospitalized patients with viral pneumonia into relevant risk categories and could provide guidance to make further clinical decisions.", "date": "2019", "authors": ["Lingxi Guo 1, Dong Wei 1, Xinxin Zhang 1, Yurong Wu 2, Qingyun Li 1, Min Zhou 1, Jieming Qu 1"], "references": [2133979383, 2065974896, 2159340685, 2091139031, 2103645914, 2948483377, 2155020492, 2021046603, 1975461687, 2048199366]}, {"id": 2909194930, "title": "From SARS to MERS, Thrusting Coronaviruses into the Spotlight", "abstract": "Coronaviruses (CoVs) have formerly been regarded as relatively harmless respiratory pathogens to humans. However, two outbreaks of severe respiratory tract infection, caused by the severe acute respiratory syndrome coronavirus (SARS-CoV) and the Middle East respiratory syndrome coronavirus (MERS-CoV), as a result of zoonotic CoVs crossing the species barrier, caused high pathogenicity and mortality rates in human populations. This brought CoVs global attention and highlighted the importance of controlling infectious pathogens at international borders. In this review, we focus on our current understanding of the epidemiology, pathogenesis, prevention, and treatment of SARS-CoV and MERS-CoV, as well as provides details on the pivotal structure and function of the spike proteins (S proteins) on the surface of each of these viruses. For building up more suitable animal models, we compare the current animal models recapitulating pathogenesis and summarize the potential role of host receptors contributing to diverse host affinity in various species. We outline the research still needed to fully elucidate the pathogenic mechanism of these viruses, to construct reproducible animal models, and ultimately develop countermeasures to conquer not only SARS-CoV and MERS-CoV, but also these emerging coronaviral diseases.", "date": "2019", "authors": ["Zhiqi Song 1, Yanfeng Xu 2, Linlin Bao 2, Ling Zhang 2, Pin Yu 2, Yajin Qu 2, Hua Zhu 2, Wenjie Zhao 2, Yunlin Han 2, Chuan Qin 3"], "references": [2903899730, 2166867592, 2470646526, 2132260239, 2104548316, 2107053896, 2131262274, 2006434809, 1993577573, 2138324310]}, {"id": 2103503670, "title": "Bats are natural reservoirs of SARS-like coronaviruses.", "abstract": "Severe acute respiratory syndrome (SARS) emerged in 2002 to 2003 in southern China. The origin of its etiological agent, the SARS coronavirus (SARS-CoV), remains elusive. Here we report that species of bats are a natural host of coronaviruses closely related to those responsible for the SARS outbreak. These viruses, termed SARS-like coronaviruses (SL-CoVs), display greater genetic variation than SARS-CoV isolated from humans or from civets. The human and civet isolates of SARS-CoV nestle phylogenetically within the spectrum of SL-CoVs, indicating that the virus responsible for the SARS outbreak was a member of this coronavirus group.", "date": "2005", "authors": ["Wendong Li , Zhengli Shi , Meng Yu , Wuze Ren , Craig Smith , Jonathan H. Epstein , Hanzhong Wang , Gary Crameri , Zhihong Hu , Huajun Zhang , Jianhong Zhang , Jennifer McEachern , Hume Field , Peter Daszak , Bryan T. Eaton , Shuyi Zhang , Lin-Fa Wang"], "references": [2104548316, 2025170735, 2116586125, 2169198329, 2134061616, 1990059132, 2127949919, 96734778, 2076620790, 2042499956]}, {"id": 2105637133, "title": "Discovery of Seven Novel Mammalian and Avian Coronaviruses in the Genus Deltacoronavirus Supports Bat Coronaviruses as the Gene Source of Alphacoronavirus and Betacoronavirus and Avian Coronaviruses as the Gene Source of Gammacoronavirus and Deltacoronavirus", "abstract": "Recently, we reported the discovery of three novel coronaviruses, bulbul coronavirus HKU11, thrush coronavirus HKU12, and munia coronavirus HKU13, which were identified as representatives of a novel genus, Deltacoronavirus, in the subfamily Coronavirinae. In this territory-wide molecular epidemiology study involving 3,137 mammals and 3,298 birds, we discovered seven additional novel deltacoronaviruses in pigs and birds, which we named porcine coronavirus HKU15, white-eye coronavirus HKU16, sparrow coronavirus HKU17, magpie robin coronavirus HKU18, night heron coronavirus HKU19, wigeon coronavirus HKU20, and common moorhen coronavirus HKU21. Complete genome sequencing and comparative genome analysis showed that the avian and mammalian deltacoronaviruses have similar genome characteristics and structures. They all have relatively small genomes (25.421 to 26.674 kb), the smallest among all coronaviruses. They all have a single papain-like protease domain in the nsp3 gene; an accessory gene, NS6 open reading frame (ORF), located between the M and N genes; and a variable number of accessory genes (up to four) downstream of the N gene. Moreover, they all have the same putative transcription regulatory sequence of ACACCA. Molecular clock analysis showed that the most recent common ancestor of all coronaviruses was estimated at approximately 8100 BC, and those of Alphacoronavirus, Betacoronavirus, Gammacoronavirus, and Deltacoronavirus were at approximately 2400 BC, 3300 BC, 2800 BC, and 3000 BC, respectively. From our studies, it appears that bats and birds, the warm blooded flying vertebrates, are ideal hosts for the coronavirus gene source, bats for Alphacoronavirus and Betacoronavirus and birds for Gammacoronavirus and Deltacoronavirus, to fuel coronavirus evolution and dissemination.", "date": "2012", "authors": ["Patrick C. Y. Woo 1, Susanna K. P. Lau 2, Carol S. F. Lam 3, Candy C. Y. Lau 3, Alan K. L. Tsang 3, John H. N. Lau 3, Ru Bai 3, Jade L. L. Teng 3, Chris C. C. Tsang 3, Ming Wang 1, Bo-Jian Zheng 2, Kwok-Hung Chan 3, Kwok-Yung Yuen 2"], "references": [2141885858, 2110835349, 2025170735, 1981646999, 2116586125, 2169198329, 2103503670, 2134061616, 2056584399, 2111412754]}, {"id": 2807736175, "title": "Saliva as a diagnostic specimen for testing respiratory virus by a point-of-care molecular assay: a diagnostic validity study.", "abstract": "Abstract Objectives Automated point-of-care molecular assays have greatly shortened the turnaround time of respiratory virus testing. One of the major bottlenecks now lies at the specimen collection step, especially in a busy clinical setting. Saliva is a convenient specimen type that can be provided easily by adult patients. This study assessed the diagnostic validity, specimen collection time and cost associated with the use of saliva. Methods This was a prospective diagnostic validity study comparing the detection rate of respiratory viruses between saliva and nasopharyngeal aspirate (NPA) among adult hospitalized patients using Xpert\u00ae Xpress Flu/RSV. The cost and time associated with the collection of saliva and nasopharyngeal specimens were also estimated. Results Between July and October 2017, 214 patients were recruited. The overall agreement between saliva and NPA was 93.3% (196/210, \u03ba 0.851, 95% CI 0.776\u20130.926). There was no significant difference in the detection rate of respiratory viruses between saliva and NPA (32.9% (69/210) versus 35.7% (75/210); p 0.146). The overall sensitivity and specificity were 90.8% (81.9%\u201396.2%) and 100% (97.3%\u2013100%), respectively, for saliva, and were 96.1% (88.9%\u201399.2%) and 98.5% (94.7%\u201399.8%), respectively, for NPA. The time and cost associated with the collection of saliva were 2.26-fold and 2.59-fold lower, respectively, than those of NPA. Conclusions Saliva specimens have high sensitivity and specificity in the detection of respiratory viruses by an automated multiplex Clinical Laboratory Improvement Amendments-waived point-of-care molecular assay when compared with those of NPA. The use of saliva also reduces the time and cost associated with specimen collection.", "date": "2019", "authors": ["K.K.W. To 1, C.C.Y. Yip 2, 3, C.Y.W. Lai 2, 3, C.K.H. Wong 2, D.T.Y. Ho 2, P.K.P. Pang 2, A.C.K. Ng 2, K.-H. Leung 3, R.W.S. Poon 2, 3, K.-H. Chan 2, V.C.C. Cheng 2, 3, I.F.N. Hung 2, 4, K.-Y. Yuen 1"], "references": [2164777277, 2000714505, 2102263282, 2103338644, 2752904261, 2101172433, 2605374894, 2623181050, 2752140764, 2105614110]}, {"id": 2889758689, "title": "Genomic characterization and infectivity of a novel SARS-like coronavirus in Chinese bats", "abstract": "SARS coronavirus (SARS-CoV), the causative agent of the large SARS outbreak in 2003, originated in bats. Many SARS-like coronaviruses (SL-CoVs) have been detected in bats, particularly those that reside in China, Europe, and Africa. To further understand the evolutionary relationship between SARS-CoV and its reservoirs, 334 bats were collected from Zhoushan city, Zhejiang province, China, between 2015 and 2017. PCR amplification of the conserved coronaviral protein RdRp detected coronaviruses in 26.65% of bats belonging to this region, and this number was influenced by seasonal changes. Full genomic analyses of the two new SL-CoVs from Zhoushan (ZXC21 and ZC45) showed that their genomes were 29,732 nucleotides (nt) and 29,802 nt in length, respectively, with 13 open reading frames (ORFs). These results revealed 81% shared nucleotide identity with human/civet SARS CoVs, which was more distant than that observed previously for bat SL-CoVs in China. Importantly, using pathogenic tests, we found that the virus can reproduce and cause disease in suckling rats, and further studies showed that the virus-like particles can be observed in the brains of suckling rats by electron microscopy. Thus, this study increased our understanding of the genetic diversity of the SL-CoVs carried by bats and also provided a new perspective to study the possibility of cross-species transmission of SL-CoVs using suckling rats as an animal model.", "date": "2018", "authors": ["Dan Hu 1, Changqiang Zhu 2, Lele Ai 2, Ting He 2, Yi Wang 3, Fuqiang Ye 2, Lu Yang 2, Chenxi Ding 2, Xuhui Zhu 2, Ruicheng Lv 2, Jin Zhu 2, Bachar Hassan 4, Youjun Feng 5, Weilong Tan 2, Changjun Wang 1"], "references": [2311203695, 2104548316, 1993577573, 2775086803, 2169198329, 2298153446, 2046153984, 2141008678, 2140338292, 2141877163]}, {"id": 2769543984, "title": "Human intestinal tract serves as an alternative infection route for Middle East respiratory syndrome coronavirus", "abstract": "Middle East respiratory syndrome coronavirus (MERS-CoV) has caused human respiratory infections with a high case fatality rate since 2012. However, the mode of virus transmission is not well understood. The findings of epidemiological and virological studies prompted us to hypothesize that the human gastrointestinal tract could serve as an alternative route to acquire MERS-CoV infection. We demonstrated that human primary intestinal epithelial cells, small intestine explants, and intestinal organoids were highly susceptible to MERS-CoV and can sustain robust viral replication. We also identified the evidence of enteric MERS-CoV infection in the stool specimen of a clinical patient. MERS-CoV was considerably resistant to fed-state gastrointestinal fluids but less tolerant to highly acidic fasted-state gastric fluid. In polarized Caco-2 cells cultured in Transwell inserts, apical MERS-CoV inoculation was more effective in establishing infection than basolateral inoculation. Notably, direct intragastric inoculation of MERS-CoV caused a lethal infection in human DPP4 transgenic mice. Histological examination revealed MERS-CoV enteric infection in all inoculated mice, as shown by the presence of virus-positive cells, progressive inflammation, and epithelial degeneration in small intestines, which were exaggerated in the mice pretreated with the proton pump inhibitor pantoprazole. With the progression of the enteric infection, inflammation, virus-positive cells, and live viruses emerged in the lung tissues, indicating the development of sequential respiratory infection. Taken together, these data suggest that the human intestinal tract may serve as an alternative infection route for MERS-CoV.", "date": "2017", "authors": ["Jie Zhou 1, Cun Li 1, Guangyu Zhao 2, Hin Chu 1, Dong Wang 1, Helen Hoi-Ning Yan 1, Vincent Kwok-Man Poon 1, Lei Wen 1, Bosco Ho-Yin Wong 1, Xiaoyu Zhao 1, Man Chun Chiu 1, Dong Yang 1, Yixin Wang 1, Rex K. H. Au-Yeung 1, Ivy Hau-Yee Chan 3, Shihui Sun 2, Jasper Fuk-Woo Chan 4, Kelvin Kai-Wang To 4, Ziad Ahmed Memish 5, 6, Victor M. Corman 7, Christian Drosten 7, Ivan Fan-Ngai Hung 1, Yusen Zhou 2, Suet Yi Leung 1, Kwok-Yung Yuen 4"], "references": [2166867592, 2107053896, 2025170735, 2006434809, 2045002682, 2115555188, 2002513358, 2144410942, 1757215199, 1501973965]}, {"id": 2140338292, "title": "Severe acute respiratory syndrome coronavirus-like virus in Chinese horseshoe bats", "abstract": "Although the finding of severe acute respiratory syndrome coronavirus (SARS-CoV) in caged palm civets from live animal markets in China has provided evidence for interspecies transmission in the genesis of the SARS epidemic, subsequent studies suggested that the civet may have served only as an amplification host for SARS-CoV. In a surveillance study for CoV in noncaged animals from the wild areas of the Hong Kong Special Administration Region, we identified a CoV closely related to SARS-CoV (bat-SARS-CoV) from 23 (39%) of 59 anal swabs of wild Chinese horseshoe bats (Rhinolophus sinicus) by using RT-PCR. Sequencing and analysis of three bat-SARS-CoV genomes from samples collected at different dates showed that bat-SARS-CoV is closely related to SARS-CoV from humans and civets. Phylogenetic analysis showed that bat-SARS-CoV formed a distinct cluster with SARS-CoV as group 2b CoV, distantly related to known group 2 CoV. Most differences between the bat-SARS-CoV and SARS-CoV genomes were observed in the spike genes, ORF 3 and ORF 8, which are the regions where most variations also were observed between human and civet SARS-CoV genomes. In addition, the presence of a 29-bp insertion in ORF 8 of bat-SARS-CoV genome, not in most human SARS-CoV genomes, suggests that it has a common ancestor with civet SARS-CoV. Antibody against recombinant bat-SARS-CoV nucleocapsid protein was detected in 84% of Chinese horseshoe bats by using an enzyme immunoassay. Neutralizing antibody to human SARS-CoV also was detected in bats with lower viral loads. Precautions should be exercised in the handling of these animals.", "date": "2005", "authors": ["Susanna K. P. Lau , Patrick C. Y. Woo , Kenneth S. M. Li , Yi Huang , Hoi-Wah Tsoi , Beatrice H. L. Wong , Samson S. Y. Wong , Suet-Yi Leung , Kwok-Hung Chan , Kwok-Yung Yuen"], "references": [2141885858, 2132260239, 2104548316, 2025170735, 2116586125, 2169198329, 2171091522, 1966238900, 2134061616, 2111412754]}, {"id": 2103441770, "title": "Fast and accurate short read alignment with Burrows\u2013Wheeler transform", "abstract": "Motivation: The enormous amount of short reads generated by the new DNA sequencing technologies call for the development of fast and accurate read alignment programs. A first generation of hash table-based methods has been developed, including MAQ, which is accurate, feature rich and fast enough to align short reads from a single individual. However, MAQ does not support gapped alignment for single-end reads, which makes it unsuitable for alignment of longer reads where indels may occur frequently. The speed of MAQ is also a concern when the alignment is scaled up to the resequencing of hundreds of individuals. Results: We implemented Burrows-Wheeler Alignment tool (BWA), a new read alignment package that is based on backward search with Burrows\u2013Wheeler Transform (BWT), to efficiently align short sequencing reads against a large reference sequence such as the human genome, allowing mismatches and gaps. BWA supports both base space reads, e.g. from Illumina sequencing machines, and color space reads from AB SOLiD machines. Evaluations on both simulated and real data suggest that BWA is ~10\u201320\u00d7 faster than MAQ, while achieving similar accuracy. In addition, BWA outputs alignment in the new standard SAM (Sequence Alignment/Map) format. Variant calling and other downstream analyses after the alignment can be achieved with the open source SAMtools software package. Availability: http://maq.sourceforge.net Contact: [email protected]", "date": "2009", "authors": ["Heng Li , Richard Durbin"], "references": [2108234281, 2158714788, 2124985265, 2112113834, 2136145671, 2139760555, 2132341951, 2015292449, 2055666215, 2142619120]}, {"id": 2141052558, "title": "RAxML version 8: a tool for phylogenetic analysis and post-analysis of large phylogenies.", "abstract": "Motivation: Phylogenies are increasingly used in all fields of medical and biological research. Moreover, because of the next-generation sequencing revolution, datasets used for conducting phylogenetic analyses grow at an unprecedented pace. RAxML (Randomized Axelerated Maximum Likelihood) is a popular program for phylogenetic analyses of large datasets under maximum likelihood. Since the last RAxML paper in 2006, it has been continuously maintained and extended to accommodate the increasingly growing input datasets and to serve the needs of the user community. Results: I present some of the most notable new features and extensions of RAxML, such as a substantial extension of substitution models and supported data types, the introduction of SSE3, AVX and AVX2 vector intrinsics, techniques for reducing the memory requirements of the code and a plethora of operations for conducting post-analyses on sets of trees. In addition, an up-to-date 50-page user manual covering all new RAxML options is available. Availability and implementation: The code is available under GNU GPL at https://github.com/stamatak/standard-RAxML. Contact: gro.sti-h@sikatamats.sordnaxela Supplementary information: Supplementary data are available at Bioinformatics online.", "date": "2014", "authors": ["Alexandros Stamatakis"], "references": [2168696662, 2111211467, 2127847431, 1794270752, 2012220164, 2068187483, 2151736966, 2100030044, 2122082385, 2156921764]}, {"id": 2804822363, "title": "SWISS-MODEL: homology modelling of protein structures and complexes.", "abstract": "Homology modelling has matured into an important technique in structural biology, significantly contributing to narrowing the gap between known protein sequences and experimentally determined structures. Fully automated workflows and servers simplify and streamline the homology modelling process, also allowing users without a specific computational expertise to generate reliable protein models and have easy access to modelling results, their visualization and interpretation. Here, we present an update to the SWISS-MODEL server, which pioneered the field of automated modelling 25 years ago and been continuously further developed. Recently, its functionality has been extended to the modelling of homo- and heteromeric complexes. Starting from the amino acid sequences of the interacting proteins, both the stoichiometry and the overall structure of the complex are inferred by homology modelling. Other major improvements include the implementation of a new modelling engine, ProMod3 and the introduction a new local model quality estimation method, QMEANDisCo. SWISS-MODEL is freely available at https://swissmodel.expasy.org.", "date": "2018", "authors": ["Andrew Waterhouse 1, 2, Martino Bertoni 1, 2, Stefan Bienert 1, 2, Gabriel Studer 1, 2, Gerardo Tauriello 1, 2, Rafal Gumienny 1, 2, Florian T Heer 1, 2, Tjaart A P de Beer 1, 2, Christine Rempfer 1, 2, Lorenza Bordoli 1, 2, Rosalba Lepore 1, 2, Torsten Schwede 1, 2"], "references": [2158714788, 2142678478, 2149525061, 2152301430, 2154139219, 2015642465, 2060809301, 2065283382, 2051210555, 2159614853]}, {"id": 3017468735, "title": "A Novel Coronavirus Genome Identified in a Cluster of Pneumonia Cases \u2014 Wuhan, China 2019\u22122020", "abstract": "", "date": "2019", "authors": ["Wenjie Tan , Xiang Zhao , Xuejun Ma , Wenling Wang , Peihua Niu , Wenbo Xu , George F. Gao , Guizhen Wu"], "references": [3001118548, 3004318991, 3010819577, 3007814559, 3002764620, 3005655936, 3009739970, 3006255311, 3005118804, 3010569238]}, {"id": 2991491848, "title": "A Randomized, Controlled Trial of Ebola Virus Disease Therapeutics.", "abstract": "Abstract Background Although several experimental therapeutics for Ebola virus disease (EVD) have been developed, the safety and efficacy of the most promising therapies need to be assessed in the ...", "date": "2019", "authors": ["Mulangu S , Dodd Le 1, Davey Rt , Tshiani Mbaya O 2, Proschan M 3, Mukadi D 4, Lusakibanza Manzo M 5, Nzolo D 6, Tshomba Oloma A , Ibanda A , Ali R 7, Coulibaly S , Levine Ac , Grais R , Diaz J , Lane Hc , Muyembe-Tamfum Jj 8, Sivahera B , Camara M , Kojan R , Walker R , Dighero-Kemp B , Cao H , Mukumbayi P , Mbala-Kingebeni P , Ahuka S , Albert S , Bonnett T , Crozier I , Duvenhage M , Proffitt C , Teitelbaum M , Moench T , Aboulhab J , Barrett K , Cahill K , Cone K , Eckes R , Hensley L , Herpin B , Higgs E , Ledgerwood J , Pierson J , Smolskis M , Sow Y , Tierney J , Sivapalasingam S , Holman W , Gettinger N , Vall\u00e9e D +1"], "references": [3003465021, 3005212621, 3013393665, 3018023298, 3015490759, 3010848803, 3014604938, 3013653866, 3006564542]}, {"id": 2605343262, "title": "GISAID: Global initiative on sharing all influenza data - from vision to reality.", "abstract": "", "date": "2017", "authors": ["Yuelong Shu 1, John McCauley 2"], "references": [2106173155, 2259815689, 2587970647, 2063651055, 2532120756, 2222043208, 2557499142, 2104424333, 2027869475, 2553154513]}, {"id": 3001388158, "title": "China coronavirus: Six questions scientists are asking", "abstract": "Researchers are racing to find out more about the epidemiology and genetic sequence of the coronavirus spreading in Asia and beyond. Researchers are racing to find out more about the epidemiology and genetic sequence of the coronavirus spreading in Asia and beyond.", "date": "2020", "authors": ["Ewen Callaway , David Cyranoski"], "references": [3002539152, 3000771439]}, {"id": 3004397688, "title": "Preliminary estimation of the basic reproduction number of novel coronavirus (2019-nCoV) in China, from 2019 to 2020: A data-driven analysis in the early phase of the outbreak.", "abstract": "Abstract Backgrounds An ongoing outbreak of a novel coronavirus (2019-nCoV) pneumonia hit a major city in China, Wuhan, December 2019 and subsequently reached other provinces/regions of China and other countries. We present estimates of the basic reproduction number, R0, of 2019-nCoV in the early phase of the outbreak. Methods Accounting for the impact of the variations in disease reporting rate, we modelled the epidemic curve of 2019-nCoV cases time series, in mainland China from January 10 to January 24, 2020, through the exponential growth. With the estimated intrinsic growth rate (\u03b3), we estimated R0 by using the serial intervals (SI) of two other well-known coronavirus diseases, MERS and SARS, as approximations for the true unknown SI. Findings The early outbreak data largely follows the exponential growth. We estimated that the mean R0 ranges from 2.24 (95%CI: 1.96\u20132.55) to 3.58 (95%CI: 2.89\u20134.39) associated with 8-fold to 2-fold increase in the reporting rate. We demonstrated that changes in reporting rate substantially affect estimates of R0. Conclusion The mean estimate of R0 for the 2019-nCoV ranges from 2.24 to 3.58, and is significantly larger than 1. Our findings indicate the potential of 2019-nCoV to cause outbreaks.", "date": "2020", "authors": ["Shi Zhao 1, Qianyin Lin 2, Jinjun Ran 3, Salihu S Musa 4, Guangpu Yang 1, Weiming Wang 5, Yijun Lou 3, Daozhou Gao 6, Lin Yang 4, Daihai He 4, Maggie H Wang 1"], "references": [2107053896, 3002764620, 3004026249, 2999612210, 2147166346, 1990049863, 3026046290, 2117002055, 2102187991, 3002747665]}, {"id": 3002764620, "title": "Novel coronavirus 2019-nCoV: early estimation of epidemiological parameters and epidemic predictions", "abstract": "Since first identified, the epidemic scale of the recently emerged novel coronavirus (2019-nCoV) in Wuhan, China, has increased rapidly, with cases arising across China and other countries and regions. using a transmission model, we estimate a basic reproductive number of 3.11 (95%CI, 2.39-4.13); 58-76% of transmissions must be prevented to stop increasing; Wuhan case ascertainment of 5.0% (3.6-7.4); 21022 (11090-33490) total infections in Wuhan 1 to 22 January.", "date": "2020", "authors": ["Read Jm 1, Bridgen 1, Cummings Da 2, Ho A 3, Jewell Cp 1"], "references": [2582743722, 3001118548, 3002539152, 2999612210, 3017468735, 2147166346, 3002533591, 1998725525, 3001343166, 3013444644]}, {"id": 3002533591, "title": "Transmission Dynamics of 2019 Novel Coronavirus (2019-nCoV)", "abstract": "Background: Since December 29, 2019, pneumonia infection with 2019-nCoV has rapidly spread out from Wuhan, Hubei Province, China to most others provinces and other counties. However, the transmission dynamics of 2019-nCoV remain unclear. Methods: Data of confirmed 2019-nCoV cases before January 23, 2020 were collected from medical records, epidemiological investigations or official websites. Data of severe acute respiratory syndrome (SARS) cases in Guangdong Province during 2002-2003 were obtained from Guangdong Provincial Center for Disease Control and Prevention (GDCDC). Exponential Growth (EG) and maximum likelihood estimation (ML) were applied to estimate the reproductive number (R) of 2019-nCoV and SARS. Findings: As of January 23, 2020, a total of 830 confirmed 2019-nCoV cases were identified across China, and 9 cases were reported overseas. The average incubation duration of 2019-nCoV infection was 4\u00b78\u00b12\u00b76 days. The average period from onset of symptoms to isolation of 2019-nCoV and SARS cases were 2\u00b79\u00b13\u00b70 and 4\u00b72\u00b13\u00b77 days, respectively. The R values of 2019-nCoV were 2\u00b790 (95%CI: 2\u00b732-3\u00b763) and 2\u00b792 (95%CI: 2\u00b728-3\u00b767) estimated using EG and ML respectively, while the corresponding R values of SARS-CoV were 1\u00b777 (95%CI: 1\u00b737-2\u00b727) and 1\u00b785 (95%CI: 1\u00b732-2\u00b749). We observe a decreasing trend of the period from onset to isolation and R values of both 2019-nCoV and SARS-CoV. Interpretation: The 2019-nCoV may have a higher pandemic risk than SARS broken out in 2003. The implemented public-health efforts have significantly decreased the pandemic risk of 2019-nCoV. However, more rigorous control and prevention strategies and measures to contain its further spread. Funding Statement: National Key Research and Development Program of China (2018YFA0606200, 2018YFA0606202), the Science and Technology Program of Guangdong Province (2018B020207006, 2019B020208005, 2019B111103001), Guangzhou Science and technology Plan Project (201804010383). Declaration of Interests: All authors declare no competing interests. Ethics Approval Statement: Data collection and analysis were determined by the National Health Commission of the People\u2019s Republic of China to be part of a continuing public health outbreak investigation, and were thus considered exempt from institutional review board approval.", "date": "2020", "authors": ["Tao Liu , Jianxiong Hu , Min Kang , Lifeng Lin , Haojie Zhong , Jianpeng Xiao , Guanhao He , Tie Song , Qiong Huang , Zuhua Rong , Aiping Deng , Weilin Zeng , Xiaohua Tan , Siqing Zeng , Zhihua Zhu , Jiansen Li , Donghua Wan , Jing Lu , Huihong Deng , Jianfeng He , Wenjun Ma"], "references": [3001118548, 3002539152, 2147166346, 2117002055, 2140763962, 2102187991, 2601351842, 2065075114, 3000039309, 2062059173]}, {"id": 2069251911, "title": "Superspreading and the effect of individual variation on disease emergence", "abstract": "Population-level analyses often use average quantities to describe heterogeneous systems, particularly when variation does not arise from identifiable groups. A prominent example, central to our current understanding of epidemic spread, is the basic reproductive number, R(0), which is defined as the mean number of infections caused by an infected individual in a susceptible population. Population estimates of R(0) can obscure considerable individual variation in infectiousness, as highlighted during the global emergence of severe acute respiratory syndrome (SARS) by numerous 'superspreading events' in which certain individuals infected unusually large numbers of secondary cases. For diseases transmitted by non-sexual direct contacts, such as SARS or smallpox, individual variation is difficult to measure empirically, and thus its importance for outbreak dynamics has been unclear. Here we present an integrated theoretical and statistical analysis of the influence of individual variation in infectiousness on disease emergence. Using contact tracing data from eight directly transmitted diseases, we show that the distribution of individual infectiousness around R(0) is often highly skewed. Model predictions accounting for this variation differ sharply from average-based approaches, with disease extinction more likely and outbreaks rarer but more explosive. Using these models, we explore implications for outbreak control, showing that individual-specific control measures outperform population-wide measures. Moreover, the dramatic improvements achieved through targeted control policies emphasize the need to identify predictive correlates of higher infectiousness. Our findings indicate that superspreading is a normal feature of disease spread, and to frame ongoing discussion we propose a rigorous definition for superspreading events and a method to predict their frequency.", "date": "2005", "authors": ["J. O. Lloyd-Smith 1, S. J. Schreiber 2, P. E. Kopp 3, W. M. Getz 1"], "references": [2331432542, 2009435671, 2147166346, 1606697907, 2146272590, 1965499304, 2096145431, 2104595316, 2463755683, 2140763962]}, {"id": 2096145431, "title": "Transmission dynamics of the etiological agent of SARS in Hong Kong: impact of public health interventions.", "abstract": "We present an analysis of the first 10 weeks of the severe acute respiratory syndrome (SARS) epidemic in Hong Kong. The epidemic to date has been characterized by two large clusters-initiated by two separate \"super-spread\" events (SSEs)-and by ongoing community transmission. By fitting a stochastic model to data on 1512 cases, including these clusters, we show that the etiological agent of SARS is moderately transmissible. Excluding SSEs, we estimate that 2.7 secondary infections were generated per case on average at the start of the epidemic, with a substantial contribution from hospital transmission. Transmission rates fell during the epidemic, primarily as a result of reductions in population contact rates and improved hospital infection control, but also because of more rapid hospital attendance by symptomatic individuals. As a result, the epidemic is now in decline, although continued vigilance is necessary for this to be maintained. Restrictions on longer range population movement are shown to be a potentially useful additional control measure in some contexts. We estimate that most currently infected persons are now hospitalized, which highlights the importance of control of nosocomial transmission.", "date": "2003", "authors": ["Steven Riley 1, Christophe Fraser 1, Christl A. Donnelly 1, Azra C. Ghani 1, Laith J. Abu-Raddad 1, Anthony J. Hedley 2, Gabriel M. Leung 2, Lai Ming Ho 2, Tai Hing Lam 2, Thuan Q. Thach 2, Patsy Chau 2, King Pan Chan 2, Su Vui Lo 3, Pak Yin Leung 4, Thomas Tsang 4, William Ho 5, Koon Hung Lee 5, Edith M.C. Lau 6, Neil M. Ferguson 1, Roy M. Anderson 1"], "references": [2104548316, 2025170735, 2131262274, 2100820722, 2169198329, 1606697907, 2104595316, 2124853344]}, {"id": 1815575713, "title": "Transmission characteristics of MERS and SARS in the healthcare setting: a comparative study", "abstract": "The Middle East respiratory syndrome (MERS) coronavirus has caused recurrent outbreaks in the Arabian Peninsula since 2012. Although MERS has low overall human-to-human transmission potential, there is occasional amplification in the healthcare setting, a pattern reminiscent of the dynamics of the severe acute respiratory syndrome (SARS) outbreaks in 2003. Here we provide a head-to-head comparison of exposure patterns and transmission dynamics of large hospital clusters of MERS and SARS, including the most recent South Korean outbreak of MERS in 2015.", "date": "2015", "authors": ["Gerardo Chowell 1, 2, Fatima Abdirizak 1, Sunmi Lee 3, Jonggul Lee 4, Eunok Jung 4, Hiroshi Nishiura 5, C\u00e9cile Viboud 2"], "references": [2166867592, 2107053896, 2006434809, 2138324310, 2147166346, 1990049863, 2069251911, 2096145431, 1968393246, 2130227690]}, {"id": 2104595316, "title": "Mathematical Epidemiology of Infectious Diseases: Model Building, Analysis and Interpretation", "abstract": "Provides systematic coverage of the mathematical theory of modelling epidemics in populations, with a clear and coherent discussion of the issues, concepts and phenomena. Mathematical modelling of epidemics is a vast and important area of study and this book helps the reader to translate, model, analyse and interpret, with numerous applications, examples and exercises to aid understanding.", "date": "2000", "authors": ["Odo Diekmann , J. A. P Heesterbeek"], "references": [3003573988, 2070722739, 1878853999, 2112680994, 2159301256, 2133131640, 3018782651, 3033562259, 2069251911, 2096145431]}, {"id": 1998725525, "title": "Incubation periods of acute respiratory viral infections: a systematic review", "abstract": "Summary Knowledge of the incubation period is essential in the investigation and control of infectious disease, but statements of incubation period are often poorly referenced, inconsistent, or based on limited data. In a systematic review of the literature on nine respiratory viral infections of public-health importance, we identified 436 articles with statements of incubation period and 38 with data for pooled analysis. We fitted a log-normal distribution to pooled data and found the median incubation period to be 5\u00b76 days (95% CI 4\u00b78\u20136\u00b73) for adenovirus, 3\u00b72 days (95% CI 2\u00b78\u20133\u00b77) for human coronavirus, 4\u00b70 days (95% CI 3\u00b76\u20134\u00b74) for severe acute respiratory syndrome coronavirus, 1\u00b74 days (95% CI 1\u00b73\u20131\u00b75) for influenza A, 0\u00b76 days (95% CI 0\u00b75\u20130\u00b76) for influenza B, 12\u00b75 days (95% CI 11\u00b78\u201313\u00b73) for measles, 2\u00b76 days (95% CI 2\u00b71\u20133\u00b71) for parainfluenza, 4\u00b74 days (95% CI 3\u00b79\u20134\u00b79) for respiratory syncytial virus, and 1\u00b79 days (95% CI 1\u00b74\u20132\u00b74) for rhinovirus. When using the incubation period, it is important to consider its full distribution: the right tail for quarantine policy, the central regions for likely times and sources of infection, and the full distribution for models used in pandemic planning. Our estimates combine published data to give the detail necessary for these and other applications.", "date": "2009", "authors": ["Justin Lessler 1, Nicholas G Reich 1, Ron Brookmeyer 1, Trish M Perl 1, 2, Kenrad E Nelson 1, Derek A T Cummings 1"], "references": [2100820722, 2125251240, 2156537900, 2162707016, 1990049863, 2146453846, 2140763962, 2120176508, 2086300406, 1985322325]}, {"id": 2121863487, "title": "Reinforcement Learning: An Introduction", "abstract": "Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives when interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications. The only necessary mathematical background is familiarity with elementary concepts of probability. The book is divided into three parts. Part I defines the reinforcement learning problem in terms of Markov decision processes. Part II provides basic solution methods: dynamic programming, Monte Carlo methods, and temporal-difference learning. Part III presents a unified view of the solution methods and incorporates artificial neural networks, eligibility traces, and planning; the two final chapters present case studies and consider the future of reinforcement learning.", "date": "1987", "authors": ["R.S. Sutton , A.G. Barto"], "references": [1639032689, 2154642048, 3017143921, 2100677568, 1535810436, 1603765807, 3011120880, 1569320505, 94523489, 1540723801]}, {"id": 2952509347, "title": "The arcade learning environment: an evaluation platform for general agents", "abstract": "In this extended abstract we introduce the Arcade Learning Environment (ALE): both a challenge problem and a platform and methodology for evaluating the development of general, domain-independent AI technology. ALE provides an interface to hundreds of Atari 2600 game environments, each one different, interesting, and designed to be a challenge for human players. ALE presents significant research challenges for reinforcement learning, model learning, model-based planning, imitation learning, transfer learning, and intrinsic motivation. Most importantly, it provides a rigorous testbed for evaluating and comparing approaches to these problems. We illustrate the promise of ALE by presenting a benchmark set of domain-independent agents designed using well-established AI techniques for both reinforcement learning and planning. In doing so, we also propose an evaluation methodology made possible by ALE, reporting empirical results on over 55 different games. We conclude with a brief update on the latest ALE developments. All of the software, including the benchmark agents, is publicly available.", "date": "2015", "authors": ["Marc G. Bellemare 1, Yavar Naddaf 2, Joel Veness 1, Michael Bowling 1"], "references": [2145339207, 2952509347, 2126316555, 1515851193, 1625390266, 1502916507, 2099587183, 2013391942, 2101355568, 2132622533]}, {"id": 2001141328, "title": "A Global Geometric Framework for Nonlinear Dimensionality Reduction", "abstract": "Scientists working with large volumes of high-dimensional data, such as global climate patterns, stellar spectra, or human gene distributions, regularly confront the problem of dimensionality reduction: finding meaningful low-dimensional structures hidden in their high-dimensional observations. The human brain confronts the same problem in everyday perception, extracting from its high-dimensional sensory inputs-30,000 auditory nerve fibers or 10(6) optic nerve fibers-a manageably small number of perceptually relevant features. Here we describe an approach to solving dimensionality reduction problems that uses easily measured local metric information to learn the underlying global geometry of a data set. Unlike classical techniques such as principal component analysis (PCA) and multidimensional scaling (MDS), our approach is capable of discovering the nonlinear degrees of freedom that underlie complex natural observations, such as human handwriting or images of a face under different viewing conditions. In contrast to previous algorithms for nonlinear dimensionality reduction, ours efficiently computes a globally optimal solution, and, for an important class of data manifolds, is guaranteed to converge asymptotically to the true structure.", "date": "2000", "authors": ["J. B. Tenenbaum 1, V. de Silva 1, J. C. Langford 2"], "references": [2138451337, 2099741732, 3110653090, 2587818897, 2123977795, 2107636931, 2122538988, 2047870719, 2070320140, 2032647857]}, {"id": 2121122425, "title": "Dimension reduction by local principal component analysis", "abstract": "Reducing or eliminating statistical redundancy between the components of high-dimensional vector data enables a lower-dimensional representation without significant loss of information. Recognizing the limitations of principal component analysis (PCA), researchers in the statistics and neural network communities have developed nonlinear extensions of PCA. This article develops a local linear approach to dimension reduction that provides accurate representations and is fast to compute. We exercise the algorithms on speech and image data, and compare performance with PCA and with neural network implementations of nonlinear PCA. We find that both nonlinear techniques can provide more accurate representations than PCA and show that the local linear techniques outperform neural network implementations.", "date": "1997", "authors": ["Nandakishore Kambhatla , Todd K. Leen"], "references": [2313307644, 2137983211, 3004157836, 1634005169, 2140196014, 1971735090, 2913399920, 2096710051, 2122538988, 2017977879]}, {"id": 2032647857, "title": "Replicator neural networks for universal optimal source coding.", "abstract": "Replicator neural networks self-organize by using their inputs as desired outputs; they internally form a compressed representation for the input data. A theorem shows that a class of replicator networks can, through the minimization of mean squared reconstruction error (for instance, by training on raw data examples), carry out optimal data compression for arbitrary data vector sources. Data manifolds, a new general model of data sources, are then introduced and a second theorem shows that, in a practically important limiting case, optimal-compression replicator networks operate by creating an essentially unique natural coordinate system for the manifold.", "date": "1995", "authors": ["Robert Hecht-Nielsen"], "references": [2137983211, 2166116275, 1993845689, 2122538988, 5731987, 2142228262, 2063971957, 2079782346, 2089419199, 2162604518]}, {"id": 2021774695, "title": "Learning sets of filters using back-propagation", "abstract": "Abstract A learning procedure, called back-propagation, for layered networks of deterministic, neuron-like units has been described previously. The ability of the procedure automatically to discover useful internal representations makes it a powerful tool for attacking difficult problems like speech recognition. This paper describes further research on the learning procedure and presents an example in which a network learns a set of filters that enable it to discriminate formant-like patterns in the presence of noise. The generality of the learning procedure is illustrated by a second example in which a similar network learns an edge detection task. The speed of learning is strongly dependent on the shape of the surface formed by the error measure in \u201cweight space\u201d. Examples are given of the error surface for a simple task and an acceleration method that speeds up descent in weight space is illustrated. The main drawback of the learning procedure is the way it scales as the size of the task and the network increases. Some preliminary results on scaling are reported and it is shown how the magnitude of the optimal weight changes depends on the fan-in of the units. Additional results show how the amount of interaction between the weights affects the learning speed. The paper is concluded with a discussion of the difficulties that are likely to be encounted in applying back-propagation to more realistic problems in speech recognition, and some promising approaches to overcoming these difficulties.", "date": "1987", "authors": ["David C. Plaut , Geoffrey E. Hinton"], "references": [2154642048, 1652505363, 1498436455, 1507849272, 2155487652, 1995169133, 2591802459, 2010581677]}, {"id": 2158899491, "title": "Natural Language Processing (Almost) from Scratch", "abstract": "We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements.", "date": "2011", "authors": ["Ronan Collobert , Jason Weston 1, L\u00e9on Bottou , Michael Karlen , Koray Kavukcuoglu 2, Pavel Kuksa 3"], "references": [2136922672, 2310919327, 2147880316, 2125838338, 2110798204, 2158139315, 2159080219, 2098162425, 2150102617, 2296073425]}, {"id": 2162915993, "title": "Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories", "abstract": "This paper presents a method for recognizing scene categories based on approximate global geometric correspondence. This technique works by partitioning the image into increasingly fine sub-regions and computing histograms of local features found inside each sub-region. The resulting \"spatial pyramid\" is a simple and computationally efficient extension of an orderless bag-of-features image representation, and it shows significantly improved performance on challenging scene categorization tasks. Specifically, our proposed method exceeds the state of the art on the Caltech-101 database and achieves high accuracy on a large database of fifteen natural scene categories. The spatial pyramid framework also offers insights into the success of several recently proposed image descriptions, including Torralba\u0092s \"gist\" and Lowe\u0092s SIFT descriptors.", "date": "2006", "authors": ["S. Lazebnik 1, C. Schmid 2, J. Ponce 3"], "references": [1880262756, 2154422044, 2107034620, 1566135517, 2166049352, 2104978738, 2914885528, 2168002178, 2134731454, 2165828254]}, {"id": 2913932916, "title": "Semantic hashing", "abstract": "We show how to learn a deep graphical model of the word-count vectors obtained from a large set of documents. The values of the latent variables in the deepest layer are easy to infer and give a much better representation of each document than Latent Semantic Analysis. When the deepest layer is forced to use a small number of binary variables (e.g. 32), the graphical model performs ''semantic hashing'': Documents are mapped to memory addresses in such a way that semantically similar documents are located at nearby addresses. Documents similar to a query document can then be found by simply accessing all the addresses that differ by only a few bits from the address of the query document. This way of extending the efficiency of hash-coding to approximate matching is much faster than locality sensitive hashing, which is the fastest current method. By using semantic hashing to filter the documents given to TF-IDF, we achieve higher accuracy than applying TF-IDF to the entire document set.", "date": "2009", "authors": ["Ruslan Salakhutdinov , Geoffrey Hinton"], "references": [1880262756, 2136922672, 2100495367, 2116064496, 2147152072, 2150102617, 2162006472, 2038276547, 1978394996, 2157364932]}, {"id": 2103359087, "title": "Phone Recognition with the Mean-Covariance Restricted Boltzmann Machine", "abstract": "Straightforward application of Deep Belief Nets (DBNs) to acoustic modeling produces a rich distributed representation of speech data that is useful for recognition and yields impressive results on the speaker-independent TIMIT phone recognition task. However, the first-layer Gaussian-Bernoulli Restricted Boltzmann Machine (GRBM) has an important limitation, shared with mixtures of diagonal-covariance Gaussians: GRBMs treat different components of the acoustic input vector as conditionally independent given the hidden state. The mean-covariance restricted Boltzmann machine (mcRBM), first introduced for modeling natural images, is a much more representationally efficient and powerful way of modeling the covariance structure of speech data. Every configuration of the precision units of the mcRBM specifies a different precision matrix for the conditional distribution over the acoustic space. In this work, we use the mcRBM to learn features of speech data that serve as input into a standard DBN. The mcRBM features combined with DBNs allow us to achieve a phone error rate of 20.5%, which is superior to all published results on speaker-independent TIMIT to date.", "date": "2010", "authors": ["George Dahl , Marc'aurelio Ranzato , Abdel-rahman Mohamed , Geoffrey E. Hinton"], "references": [2136922672, 1498436455, 1567512734, 137106866, 1983334819, 2161000554, 2161893161, 2083380015, 2131700150, 2110871230]}, {"id": 2033819227, "title": "Multiple view geometry in computer vision", "abstract": "From the Publisher: A basic problem in computer vision is to understand the structure of a real world scene given several images of it. Recent major developments in the theory and practice of scene reconstruction are described in detail in a unified framework. The book covers the geometric principles and how to represent objects algebraically so they can be computed and applied. The authors provide comprehensive background material and explain how to apply the methods and implement the algorithms directly.", "date": "1999", "authors": ["Richard Hartley 1, Andrew Zisserman 2"], "references": [2339009915, 1599699181, 1583242767]}, {"id": 2124386111, "title": "Object recognition from local scale-invariant features", "abstract": "An object recognition system has been developed that uses a new class of local image features. The features are invariant to image scaling, translation, and rotation, and partially invariant to illumination changes and affine or 3D projection. These features share similar properties with neurons in inferior temporal cortex that are used for object recognition in primate vision. Features are efficiently detected through a staged filtering approach that identifies stable points in scale space. Image keys are created that allow for local geometric deformations by representing blurred image gradients in multiple orientation planes and at multiple scales. The keys are used as input to a nearest neighbor indexing method that identifies candidate object matches. Final verification of each match is achieved by finding a low residual least squares solution for the unknown model parameters. Experimental results show that robust object recognition can be achieved in cluttered partially occluded images with a computation time of under 2 seconds.", "date": "1999", "authors": ["D.G. Lowe"], "references": [2914885528, 2124087378, 2123977795, 2011891945, 22745672, 2096077837, 2096600681, 2131806657, 2042243448, 1553558465]}, {"id": 2154422044, "title": "Object class recognition by unsupervised scale-invariant learning", "abstract": "We present a method to learn and recognize object class models from unlabeled and unsegmented cluttered scenes in a scale invariant manner. Objects are modeled as flexible constellations of parts. A probabilistic representation is used for all aspects of the object: shape, appearance, occlusion and relative scale. An entropy-based feature detector is used to select regions and their scale within the image. In learning the parameters of the scale-invariant object model are estimated. This is done using expectation-maximization in a maximum-likelihood setting. In recognition, this model is used in a Bayesian manner to classify images. The flexible nature of the model is demonstrated by excellent results over a range of datasets including geometrically constrained classes (e.g. faces, cars) and flexible objects (such as animals).", "date": "2003", "authors": ["R. Fergus 1, P. Perona 2, A. Zisserman 1"], "references": [2164598857, 2217896605, 2049633694, 2119747362, 2109200236, 2159686933, 2155511848, 1949116567, 2160225842, 1699734612]}, {"id": 2012778485, "title": "Invariant Features from Interest Point Groups", "abstract": "This paper approaches the problem of \u00afnding correspondences between images in which there are large changes in viewpoint, scale and illumi- nation. Recent work has shown that scale-space `interest points' may be found with good repeatability in spite of such changes. Further- more, the high entropy of the surrounding image regions means that local descriptors are highly discriminative for matching. For descrip- tors at interest points to be robustly matched between images, they must be as far as possible invariant to the imaging process. In this work we introduce a family of features which use groups of interest points to form geometrically invariant descriptors of image regions. Feature descriptors are formed by resampling the image rel- ative to canonical frames de\u00afned by the points. In addition to robust matching, a key advantage of this approach is that each match implies a hypothesis of the local 2D (projective) transformation. This allows us to immediately reject most of the false matches using a Hough trans- form. We reject remaining outliers using RANSAC and the epipolar constraint. Results show that dense feature matching can be achieved in a few seconds of computation on 1GHz Pentium III machines.", "date": "2002", "authors": ["Matthew Brown , David G. Lowe"], "references": [2124386111, 2124087378, 2119747362, 2109200236, 2165497495, 2103504761, 1505641881, 2011891945, 22745672, 2005433550]}, {"id": 2124404372, "title": "Robust wide-baseline stereo from maximally stable extremal regions", "abstract": "Abstract The wide-baseline stereo problem, i.e. the problem of establishing correspondences between a pair of images taken from different viewpoints is studied. A new set of image elements that are put into correspondence, the so called extremal regions , is introduced. Extremal regions possess highly desirable properties: the set is closed under (1) continuous (and thus projective) transformation of image coordinates and (2) monotonic transformation of image intensities. An efficient (near linear complexity) and practically fast detection algorithm (near frame rate) is presented for an affinely invariant stable subset of extremal regions, the maximally stable extremal regions (MSER). A new robust similarity measure for establishing tentative correspondences is proposed. The robustness ensures that invariants from multiple measurement regions (regions obtained by invariant constructions from extremal regions), some that are significantly larger (and hence discriminative) than the MSERs, may be used to establish tentative correspondences. The high utility of MSERs, multiple measurement regions and the robust metric is demonstrated in wide-baseline experiments on image pairs from both indoor and outdoor scenes. Significant change of scale (3.5\u00d7), illumination conditions, out-of-plane rotation, occlusion, locally anisotropic scale change and 3D translation of the viewpoint are all present in the test problems. Good estimates of epipolar geometry (average distance from corresponding points to the epipolar line below 0.09 of the inter-pixel distance) are obtained.", "date": "2004", "authors": ["Jiri Matas 1, Ondrej Chum 2, Martin Urban 2, Tom\u00e1s Pajdla 2"], "references": [2033819227, 2124386111, 1676552347, 2124087378, 2119747362, 2165497495, 1541642243, 3111446822, 2132332894, 2143753158]}, {"id": 1676552347, "title": "An Affine Invariant Interest Point Detector", "abstract": "This paper presents a novel approach for detecting affine invariant interest points. Our method can deal with significant affine transformations including large scale changes. Such transformations introduce significant changes in the point location as well as in the scale and the shape of the neighbourhood of an interest point. Our approach allows to solve for these problems simultaneously. It is based on three key ideas : 1) The second moment matrix computed in a point can be used to normalize a region in an affine invariant way (skew and stretch). 2) The scale of the local structure is indicated by local extrema of normalized derivatives over scale. 3) An affine-adapted Harris detector determines the location of interest points. A multi-scale version of this detector is used for initialization. An iterative algorithm then modifies location, scale and neighbourhood of each point and converges to affine invariant points. For matching and recognition, the image is characterized by a set of affine invariant points; the affine transformation associated with each point allows the computation of an affine invariant descriptor which is also invariant to affine illumination changes. A quantitative comparison of our detector with existing ones shows a significant improvement in the presence of large affine deformations. Experimental results for wide baseline matching show an excellent performance in the presence of large perspective transformations including significant scale changes. Results for recognition are very good for a database with more than 5000 images.", "date": "2002", "authors": ["K. Mikolajczyk , C. Schmid"], "references": [2124386111, 2124087378, 2119747362, 2109200236, 2111308925, 2165497495, 1991605728, 2112328181, 2005433550, 1970269179]}, {"id": 2124087378, "title": "Local grayvalue invariants for image retrieval", "abstract": "This paper addresses the problem of retrieving images from large image databases. The method is based on local grayvalue invariants which are computed at automatically detected interest points. A voting algorithm and semilocal constraints make retrieval possible. Indexing allows for efficient retrieval from a database of more than 1,000 images. Experimental results show correct retrieval in the case of partial visibility, similarity transformations, extraneous features, and small perspective deformations.", "date": "1997", "authors": ["C. Schmid 1, R. Mohr 2"], "references": [2914885528, 2111308925, 2098693229, 2123977795, 2095757522, 2011891945, 2109863423, 2112328181, 2022735534, 3111324825]}, {"id": 2111308925, "title": "A COMBINED CORNER AND EDGE DETECTOR", "abstract": "The problem we are addressing in Alvey Project MMI149 is that of using computer vision to understand the unconstrained 3D world, in which the viewed scenes will in general contain too wide a diversity of objects for topdown recognition techniques to work. For example, we desire to obtain an understanding of natural scenes, containing roads, buildings, trees, bushes, etc., as typified by the two frames from a sequence illustrated in Figure 1. The solution to this problem that we are pursuing is to use a computer vision system based upon motion analysis of a monocular image sequence from a mobile camera. By extraction and tracking of image features, representations of the 3D analogues of these features can be constructed.", "date": "1987", "authors": ["Christopher G. Harris , Mike Stephens"], "references": [1639227073, 1756736144, 2063599328, 2048192053, 2039106392, 2997169974]}, {"id": 2165497495, "title": "Reliable feature matching across widely separated views", "abstract": "We present a robust method for automatically matching features in images corresponding to the same physical point on an object seen from two arbitrary viewpoints. Unlike conventional stereo matching approaches we assume no prior knowledge about the relative camera positions and orientations. In fact in our application this is the information we wish to determine from the image feature matches. Features are detected in two or more images and characterised using affine texture invariants. The problem of window effects is explicitly addressed by our method-our feature characterisation is invariant to linear transformations of the image data including rotation, stretch and skew. The feature matching process is optimised for a structure-from-motion application where we wish to ignore unreliable matches at the expense of reducing the number of feature matches.", "date": "2000", "authors": ["A. Baumberg"], "references": [2124386111, 2130103520, 2124087378, 2111308925, 2085261163, 2112328181, 3022352042, 1970269179, 2143753158, 1549739843]}, {"id": 1949116567, "title": "Unsupervised Learning of Models for Recognition", "abstract": "We present a method to learn object class models from unlabeled and unsegmented cluttered scenes for the purpose of visual object recognition. We focus on a particular type of model where objects are represented as flexible constellations of rigid parts (features). The variability within a class is represented by a joint probability density function (pdf) on the shape of the constellation and the output of part detectors. In a first stage, the method automatically identifies distinctive parts in the training set by applying a clustering algorithm to patterns selected by an interest operator. It then learns the statistical shape model using expectation maximization. The method achieves very good classification results on human faces and rear views of cars.", "date": "2000", "authors": ["Markus Weber 1, Max Welling 1, Pietro Perona 1, 2"], "references": [2049633694, 3017143921, 1564419782, 2095757522, 1958762911, 2124722975, 2117138270, 2125791971, 2029727948, 1628541567]}, {"id": 2177274842, "title": "A performance evaluation of local descriptors", "abstract": "In this paper, we compare the performance of descriptors computed for local interest regions, as, for example, extracted by the Harris-Affine detector [Mikolajczyk, K and Schmid, C, 2004]. Many different descriptors have been proposed in the literature. It is unclear which descriptors are more appropriate and how their performance depends on the interest region detector. The descriptors should be distinctive and at the same time robust to changes in viewing conditions as well as to errors of the detector. Our evaluation uses as criterion recall with respect to precision and is carried out for different image transformations. We compare shape context [Belongie, S, et al., April 2002], steerable filters [Freeman, W and Adelson, E, Setp. 1991], PCA-SIFT [Ke, Y and Sukthankar, R, 2004], differential invariants [Koenderink, J and van Doorn, A, 1987], spin images [Lazebnik, S, et al., 2003], SIFT [Lowe, D. G., 1999], complex filters [Schaffalitzky, F and Zisserman, A, 2002], moment invariants [Van Gool, L, et al., 1996], and cross-correlation for different types of interest regions. We also propose an extension of the SIFT descriptor and show that it outperforms the original method. Furthermore, we observe that the ranking of the descriptors is mostly independent of the interest region detector and that the SIFT-based descriptors perform best. Moments and steerable filters show the best performance among the low dimensional descriptors.", "date": "2005", "authors": ["K. Mikolajczyk 1, C. Schmid 2"], "references": [2151103935, 2033819227, 2124386111, 2163352848, 2131846894, 2057175746, 2154422044, 2145023731, 1980911747, 2145072179]}, {"id": 2131846894, "title": "Video Google: a text retrieval approach to object matching in videos", "abstract": "We describe an approach to object and scene retrieval which searches for and localizes all the occurrences of a user outlined object in a video. The object is represented by a set of viewpoint invariant region descriptors so that recognition can proceed successfully despite changes in viewpoint, illumination and partial occlusion. The temporal continuity of the video within a shot is used to track the regions in order to reject unstable regions and reduce the effects of noise in the descriptors. The analogy with text retrieval is in the implementation where matches on descriptors are pre-computed (using vector quantization), and inverted file systems and document rankings are used. The result is that retrieved is immediate, returning a ranked list of key frames/shots in the manner of Google. The method is illustrated for matching in two full length feature films.", "date": "2003", "authors": ["Sivic , Zisserman"], "references": [3013264884, 2124386111, 2177274842, 1660390307, 2124404372, 1676552347, 2124087378, 2165497495, 2160484851, 1541642243]}, {"id": 1980911747, "title": "A Comparison of Affine Region Detectors", "abstract": "The paper gives a snapshot of the state of the art in affine covariant region detectors, and compares their performance on a set of test images under varying imaging conditions. Six types of detectors are included: detectors based on affine normalization around Harris (Mikolajczyk and Schmid, 2002; Schaffalitzky and Zisserman, 2002) and Hessian points (Mikolajczyk and Schmid, 2002), a detector of `maximally stable extremal regions', proposed by Matas et al. (2002); an edge-based region detector (Tuytelaars and Van Gool, 1999) and a detector based on intensity extrema (Tuytelaars and Van Gool, 2000), and a detector of `salient regions', proposed by Kadir, Zisserman and Brady (2004). The performance is measured against changes in viewpoint, scale, illumination, defocus and image compression. The objective of this paper is also to establish a reference test set of images and performance software, so that future detectors can be evaluated in the same framework.", "date": "2005", "authors": ["K. Mikolajczyk 1, T. Tuytelaars 2, C. Schmid 3, A. Zisserman 1, J. Matas 4, F. Schaffalitzky 1, T. Kadir 1, L. Van Gool 2"], "references": [2151103935, 2033819227, 2124386111, 2177274842, 2131846894, 2154422044, 2145023731, 1625255723, 2172188317, 2124404372]}, {"id": 2104978738, "title": "The pyramid match kernel: discriminative classification with sets of image features", "abstract": "Discriminative learning is challenging when examples are sets of features, and the sets vary in cardinality and lack any sort of meaningful ordering. Kernel-based classification methods can learn complex decision boundaries, but a kernel over unordered set inputs must somehow solve for correspondences epsivnerally a computationally expensive task that becomes impractical for large set sizes. We present a new fast kernel function which maps unordered feature sets to multi-resolution histograms and computes a weighted histogram intersection in this space. This \"pyramid match\" computation is linear in the number of features, and it implicitly finds correspondences based on the finest resolution histogram cell where a matched pair first appears. Since the kernel does not penalize the presence of extra features, it is robust to clutter. We show the kernel function is positive-definite, making it valid for use in learning algorithms whose optimal solutions are guaranteed only for Mercer kernels. We demonstrate our algorithm on object recognition tasks and show it to be accurate and dramatically faster than current approaches", "date": "2005", "authors": ["K. Grauman , T. Darrell"], "references": [2151103935, 2153635508, 2148603752, 2752885492, 2131846894, 1563088657, 2057175746, 1510073064, 2145072179, 2914885528]}, {"id": 2172188317, "title": "Scale & Affine Invariant Interest Point Detectors", "abstract": "In this paper we propose a novel approach for detecting interest points invariant to scale and affine transformations. Our scale and affine invariant detectors are based on the following recent results: (1) Interest points extracted with the Harris detector can be adapted to affine transformations and give repeatable results (geometrically stable). (2) The characteristic scale of a local structure is indicated by a local extremum over scale of normalized derivatives (the Laplacian). (3) The affine shape of a point neighborhood is estimated based on the second moment matrix. Our scale invariant detector computes a multi-scale representation for the Harris interest point detector and then selects points at which a local measure (the Laplacian) is maximal over scales. This provides a set of distinctive points which are invariant to scale, rotation and translation as well as robust to illumination changes and limited changes of viewpoint. The characteristic scale determines a scale invariant region for each point. We extend the scale invariant detector to affine invariance by estimating the affine shape of a point neighborhood. An iterative algorithm modifies location, scale and neighborhood of each point and converges to affine invariant points. This method can deal with significant affine transformations including large scale changes. The characteristic scale and the affine shape of neighborhood determine an affine invariant region for each point. We present a comparative evaluation of different detectors and show that our approach provides better results than existing methods. The performance of our detector is also confirmed by excellent matching resultss the image is described by a set of scale/affine invariant descriptors computed on the regions associated with our points.", "date": "2004", "authors": ["Krystian Mikolajczyk , Cordelia Schmid"], "references": [2033819227, 2124386111, 2012778485, 2124404372, 1676552347, 2124087378, 2119747362, 2109200236, 2111308925, 2165497495]}, {"id": 2147717514, "title": "Approximate nearest neighbors: towards removing the curse of dimensionality", "abstract": "We present two algorithms for the approximate nearest neighbor problem in high-dimensional spaces. For data sets of size n living in R d , the algorithms require space that is only polynomial in n and d, while achieving query times that are sub-linear in n and polynomial in d. We also show applications to other high-dimensional geometric problems, such as the approximate minimum spanning tree. The article is based on the material from the authors' STOC'98 and FOCS'01 papers. It unifies, generalizes and simplifies the results from those papers.", "date": "1998", "authors": ["Piotr Indyk , Rajeev Motwani"], "references": [2752885492, 2147152072, 1634005169, 2295428206, 1956559956, 2160066518, 1502916507, 3017143921, 2427881153, 2152565070]}, {"id": 2162006472, "title": "Locality-sensitive hashing scheme based on p-stable distributions", "abstract": "We present a novel Locality-Sensitive Hashing scheme for the Approximate Nearest Neighbor Problem under lp norm, based on p-stable distributions.Our scheme improves the running time of the earlier algorithm for the case of the lp norm. It also yields the first known provably efficient approximate NN algorithm for the case p<1. We also show that the algorithm finds the exact near neigbhor in O(log n) time for data satisfying certain \"bounded growth\" condition.Unlike earlier schemes, our LSH scheme works directly on points in the Euclidean space without embeddings. Consequently, the resulting query time bound is free of large factors and is simple and easy to implement. Our experiments (on synthetic data sets) show that the our data structure is up to 40 times faster than kd-tree.", "date": "2004", "authors": ["Mayur Datar 1, Nicole Immorlica 2, Piotr Indyk 2, Vahab S. Mirrokni 2"], "references": [2147717514, 1502916507, 1541459201, 2520931985, 2165533158, 2045533739, 2169351022, 2109034006, 2048779798, 2028904582]}, {"id": 2107034620, "title": "A Bayesian hierarchical model for learning natural scene categories", "abstract": "We propose a novel approach to learn and recognize natural scene categories. Unlike previous work, it does not require experts to annotate the training set. We represent the image of a scene by a collection of local regions, denoted as codewords obtained by unsupervised learning. Each region is represented as part of a \"theme\". In previous work, such themes were learnt from hand-annotations of experts, while our method learns the theme distributions as well as the codewords distribution over the themes without supervision. We report satisfactory categorization performances on a large set of 13 categories of complex scenes.", "date": "2005", "authors": ["L. Fei-Fei , P. Perona"], "references": [1880262756, 2124386111, 2045656233, 1566135517, 1484228140, 2127006916, 2171188998, 1699734612, 2104924585, 2094414211]}, {"id": 1566135517, "title": "Modeling the Shape of the Scene: A Holistic Representation of the Spatial Envelope", "abstract": "In this paper, we propose a computational model of the recognition of real world scenes that bypasses the segmentation and the processing of individual objects or regions. The procedure is based on a very low dimensional representation of the scene, that we term the Spatial Envelope. We propose a set of perceptual dimensions (naturalness, openness, roughness, expansion, ruggedness) that represent the dominant spatial structure of a scene. Then, we show that these dimensions may be reliably estimated using spectral and coarsely localized information. The model generates a multidimensional space in which scenes sharing membership in semantic categories (e.g., streets, highways, coasts) are projected closed together. The performance of the spatial envelope model shows that specific information about object shape or identity is not a requirement for scene categorization and that modeling a holistic representation of the scene informs about its probable semantic category.", "date": "2001", "authors": ["Aude Oliva 1, Antonio Torralba 2"], "references": [2117812871, 2128716185, 2012352340, 2130259898, 2156406284, 2180838288, 1524408959, 2142796031, 2104825706, 2167034998]}, {"id": 2166049352, "title": "Learning generative visual models from few training examples: An incremental Bayesian approach tested on 101 object categories", "abstract": "Current computational approaches to learning visual object categories require thousands of training images, are slow, cannot learn in an incremental manner and cannot incorporate prior information into the learning process. In addition, no algorithm presented in the literature has been tested on more than a handful of object categories. We present an method for learning object categories from just a few training images. It is quick and it uses prior information in a principled way. We test it on a dataset composed of images of objects belonging to 101 widely varied categories. Our proposed method is based on making use of prior information, assembled from (unrelated) object categories which were previously learnt. A generative probabilistic model is used, which represents the shape and appearance of a constellation of features belonging to the object. The parameters of the model are learnt incrementally in a Bayesian manner. Our incremental algorithm is compared experimentally to an earlier batch Bayesian algorithm, as well as to one based on maximum likelihood. The incremental and batch versions have comparable classification performance on small training sets, but incremental learning is significantly faster, making real-time learning feasible. Both Bayesian methods outperform maximum likelihood on small training sets.", "date": "2007", "authors": ["Li Fei-Fei 1, Rob Fergus 2, Pietro Perona 3"], "references": [2164598857, 2124386111, 2154422044, 2124087378, 2155511848, 1516111018, 1949116567, 1746680969, 2567948266, 1699734612]}, {"id": 2156598602, "title": "Photo tourism: exploring photo collections in 3D", "abstract": "We present a system for interactively browsing and exploring large unstructured collections of photographs of a scene using a novel 3D interface. Our system consists of an image-based modeling front end that automatically computes the viewpoint of each photograph as well as a sparse 3D model of the scene and image to model correspondences. Our photo explorer uses image-based rendering techniques to smoothly transition between photographs, while also enabling full 3D navigation and exploration of the set of images and world geometry, along with auxiliary information such as overhead maps. Our system also makes it easy to construct photo tours of scenic or historic locations, and to annotate image details, which are automatically transferred to other relevant images. We demonstrate our system on several large personal photo collections as well as images gathered from Internet photo sharing sites.", "date": "2006", "authors": ["Noah Snavely 1, Steven M. Seitz 1, Richard Szeliski 2"], "references": [2151103935, 3029645440, 2033819227, 2131846894, 2110764733, 1980911747, 2141282920, 2119781527, 2063366997, 2085261163]}, {"id": 3097096317, "title": "Robust Real-Time Face Detection", "abstract": "This paper describes a face detection framework that is capable of processing images extremely rapidly while achieving high detection rates. There are three key contributions. The first is the introduction of a new image representation called the \u201cIntegral Image\u201d which allows the features used by our detector to be computed very quickly. The second is a simple and efficient classifier which is built using the AdaBoost learning algorithm (Freund and Schapire, 1995) to select a small number of critical visual features from a very large set of potential features. The third contribution is a method for combining classifiers in a \u201ccascade\u201d which allows background regions of the image to be quickly discarded while spending more computation on promising face-like regions. A set of experiments in the domain of face detection is presented. The system yields face detection performance comparable to the best previous systems (Sung and Poggio, 1998; Rowley et al., 1998; Schneiderman and Kanade, 2000; Roth et al., 2000). Implemented on a conventional desktop, face detection proceeds at 15 frames per second.", "date": "2004", "authors": ["Paul Viola 1, Michael J. Jones 2"], "references": [1988790447, 2128272608, 2217896605, 2149706766, 2115763357, 1975846642, 2124351082, 2159686933, 2155511848, 2101522199]}, {"id": 1999478155, "title": "Efficient Graph-Based Image Segmentation", "abstract": "This paper addresses the problem of segmenting an image into regions. We define a predicate for measuring the evidence for a boundary between two regions using a graph-based representation of the image. We then develop an efficient segmentation algorithm based on this predicate, and show that although this algorithm makes greedy decisions it produces segmentations that satisfy global properties. We apply the algorithm to image segmentation using two different kinds of local neighborhoods in constructing the graph, and illustrate the results with both real and synthetic images. The algorithm runs in time nearly linear in the number of graph edges and is also fast in practice. An important characteristic of the method is its ability to preserve detail in low-variability image regions while ignoring detail in high-variability regions.", "date": "2004", "authors": ["Pedro F. Felzenszwalb 1, Daniel P. Huttenlocher 2"], "references": [2752885492, 2121947440, 1971784203, 1964443764, 2160167256, 2137560895, 2167077256, 2132603077, 1640070940, 2109562068]}, {"id": 2033419168, "title": "The FERET evaluation methodology for face-recognition algorithms", "abstract": "Two of the most critical requirements in support of producing reliable face-recognition systems are a large database of facial images and a testing procedure to evaluate systems. The Face Recognition Technology (FERET) program has addressed both issues through the FERET database of facial images and the establishment of the FERET tests. To date, 14,126 images from 1,199 individuals are included in the FERET database, which is divided into development and sequestered portions of the database. In September 1996, the FERET program administered the third in a series of FERET face-recognition tests. The primary objectives of the third test were to 1) assess the state of the art, 2) identify future areas of research, and 3) measure algorithm performance.", "date": "2000", "authors": ["P.J. Phillips 1, Hyeonjoon Moon 2, S.A. Rizvi 3, P.J. Rauss 4"], "references": []}, {"id": 2137659841, "title": "Overview of the face recognition grand challenge", "abstract": "Over the last couple of years, face recognition researchers have been developing new techniques. These developments are being fueled by advances in computer vision techniques, computer design, sensor design, and interest in fielding face recognition systems. Such advances hold the promise of reducing the error rate in face recognition systems by an order of magnitude over Face Recognition Vendor Test (FRVT) 2002 results. The face recognition grand challenge (FRGC) is designed to achieve this performance goal by presenting to researchers a six-experiment challenge problem along with data corpus of 50,000 images. The data consists of 3D scans and high resolution still imagery taken under controlled and uncontrolled conditions. This paper describes the challenge problem, data corpus, and presents baseline performance and preliminary results on natural statistics of facial imagery.", "date": "2005", "authors": ["P.J. Phillips 1, P.J. Flynn 2, T. Scruggs 3, K.W. Bowyer 2, Jin Chang 2, K. Hoffman 3, J. Marques 4, Jaesik Min 2, W. Worek 3"], "references": [2138451337, 2102773363, 2143542740, 2137385871, 2120838001, 1555969862, 138943044]}, {"id": 3111480503, "title": "From Few to Many: Illumination Cone Models for Face Recognition under Variable Lighting and Pose", "abstract": "", "date": "2001", "authors": ["Athinodoros S. Georghiades , Peter N. Belhumeur , David J. Kriegman"], "references": [2168068730, 2982788674, 2946068898, 199976970, 2043470575, 2158466431, 2511703955, 1948236804, 2052683696, 2254085846]}, {"id": 2098693229, "title": "Face recognition using eigenfaces", "abstract": "An approach to the detection and identification of human faces is presented, and a working, near-real-time face recognition system which tracks a subject's head and then recognizes the person by comparing characteristics of the face to those of known individuals is described. This approach treats face recognition as a two-dimensional recognition problem, taking advantage of the fact that faces are normally upright and thus may be described by a small set of 2-D characteristic views. Face images are projected onto a feature space ('face space') that best encodes the variation among known face images. The face space is defined by the 'eigenfaces', which are the eigenvectors of the set of faces; they do not necessarily correspond to isolated features such as eyes, ears, and noses. The framework provides the ability to learn to recognize new faces in an unsupervised manner. >", "date": "1991", "authors": ["M.A. Turk , A.P. Pentland"], "references": [2138451337, 2130259898, 2125848778, 2055712799, 2125999363, 1507699566, 1998186877, 2169718527]}, {"id": 2125310925, "title": "Recovering Surface Layout from an Image", "abstract": "Humans have an amazing ability to instantly grasp the overall 3D structure of a scene--ground orientation, relative positions of major landmarks, etc.--even from a single image. This ability is completely missing in most popular recognition algorithms, which pretend that the world is flat and/or view it through a patch-sized peephole. Yet it seems very likely that having a grasp of this \"surface layout\" of a scene should be of great assistance for many tasks, including recognition, navigation, and novel view synthesis. In this paper, we take the first step towards constructing the surface layout, a labeling of the image intogeometric classes. Our main insight is to learn appearance-based models of these geometric classes, which coarsely describe the 3D scene orientation of each image region. Our multiple segmentation framework provides robust spatial support, allowing a wide variety of cues (e.g., color, texture, and perspective) to contribute to the confidence in each geometric label. In experiments on a large set of outdoor images, we evaluate the impact of the individual cues and design choices in our algorithm. We further demonstrate the applicability of our method to indoor images, describe potential applications, and discuss extensions to a more complete notion of surface layout.", "date": "2007", "authors": ["Derek Hoiem , Alexei A. Efros , Martial Hebert"], "references": [2033819227, 2147880316, 2121947440, 1999478155, 2143516773, 1566135517, 2024046085, 2209124607, 1484228140, 2032210760]}, {"id": 2006793117, "title": "The CMU pose, illumination, and expression database", "abstract": "In the Fall of 2000, we collected a database of more than 40,000 facial images of 68 people. Using the Carnegie Mellon University 3D Room, we imaged each person across 13 different poses, under 43 different illumination conditions, and with four different expressions. We call this the CMU pose, illumination, and expression (PIE) database. We describe the imaging hardware, the collection procedure, the organization of the images, several possible uses, and how to obtain the database.", "date": "2003", "authors": ["T. Sim 1, S. Baker 2, M. Bsat 2"], "references": [2155759509, 2118774738, 2102760078, 2120420721, 2110822444, 2121114545, 2106143125, 2141503314, 2144855601]}, {"id": 1666447063, "title": "Object Recognition as Machine Translation: Learning a Lexicon for a Fixed Image Vocabulary", "abstract": "We describe a model of object recognition as machine translation. In this model, recognition is a process of annotating image regions with words. Firstly, images are segmented into regions, which are classified into region types using a variety of features. A mapping between region types and keywords supplied with the images, is then learned, using a method based around EM. This process is analogous with learning a lexicon from an aligned bitext. For the implementation we describe, these words are nouns taken from a large vocabulary. On a large test set, the method can predict numerous words with high accuracy. Simple methods identify words that cannot be predicted well. We show how to cluster words that individually are difficult to predict into clusters that can be predicted well -- for example, we cannot predict the distinction between train and locomotive using the current set of features, but we can predict the underlying concept. The method is trained on a substantial collection of images. Extensive experimental results illustrate the strengths and weaknesses of the approach.", "date": "2002", "authors": ["P. Duygulu 1, Kobus Barnard 1, J. F. G. de Freitas 2, David A. Forsyth 1"], "references": [2121947440, 1574901103, 1508960934, 2006969979, 1579838312, 1934863104, 2129765547, 2293605478, 1540386283, 1585814348]}, {"id": 1934863104, "title": "Learning the semantics of words and pictures", "abstract": "We present a statistical model for organizing image collections which integrates semantic information provided by associate text and visual information provided by image features. The model is very promising for information retrieval tasks such as database browsing and searching for images based on text and/or image features. Furthermore, since the model learns relationships between text and image features, it can be used for novel applications such as associating words with pictures, and unsupervised learning for object recognition.", "date": "2001", "authors": ["K. Barnard , D. Forsyth"], "references": [2049633694, 2160066518, 2135705692, 2125101937, 2062270497, 2155099190, 1587328194, 2099251025, 2011549082, 1485103000]}, {"id": 2166770390, "title": "Object Detection Using the Statistics of Parts", "abstract": "In this paper we describe a trainable object detector and its instantiations for detecting faces and cars at any size, location, and pose. To cope with variation in object orientation, the detector uses multiple classifiers, each spanning a different range of orientation. Each of these classifiers determines whether the object is present at a specified size within a fixed-size image window. To find the object at any location and size, these classifiers scan the image exhaustively. Each classifier is based on the statistics of localized parts. Each part is a transform from a subset of wavelet coefficients to a discrete set of values. Such parts are designed to capture various combinations of locality in space, frequency, and orientation. In building each classifier, we gathered the class-conditional statistics of these part values from representative samples of object and non-object images. We trained each classifier to minimize classification error on the training set by using Adaboost with Confidence-Weighted Predictions (Shapire and Singer, 1999). In detection, each classifier computes the part values within the image window and looks up their associated class-conditional probabilities. The classifier then makes a decision by applying a likelihood ratio test. For efficiency, the classifier evaluates this likelihood ratio in stages. At each stage, the classifier compares the partial likelihood ratio to a threshold and makes a decision about whether to cease evaluation\u2014labeling the input as non-object\u2014or to continue further evaluation. The detector orders these stages of evaluation from a low-resolution to a high-resolution search of the image. Our trainable object detector achieves reliable and efficient detection of human faces and passenger cars with out-of-plane rotation.", "date": "2004", "authors": ["Henry Schneiderman , Takeo Kanade"], "references": [2164598857, 2119821739, 1988790447, 2217896605, 2914885528, 2124351082, 1658679052, 2159686933, 2140785063, 2180187800]}, {"id": 1587328194, "title": "Finding Naked People", "abstract": "This paper demonstrates a content-based retrieval strategy that can tell whether there are naked people present in an image. No manual intervention is required. The approach combines color and texture properties to obtain an effective mask for skin regions. The skin mask is shown to be effective for a wide range of shades and colors of skin. These skin regions are then fed to a specialized grouper, which attempts to group a human figure using geometric constraints on human structure. This approach introduces a new view of object recognition, where an object model is an organized collection of grouping hints obtained from a combination of constraints on geometric properties such as the structure of individual parts, and the relationships between parts, and constraints on color and texture. The system is demonstrated to have 60% precision and 52% recall on a test set of 138 uncontrolled images of naked people, mostly obtained from the internet, and 1401 assorted control images, drawn from a wide collection of sources.", "date": "1996", "authors": ["Margaret M. Fleck 1, David A. Forsyth 2, Chris Bregler 2"], "references": [2145023731, 2123977795, 2093191240, 2008297189, 2068272887, 2102475035, 3110825305, 1530454533, 2037732452, 2069266228]}, {"id": 2293605478, "title": "Clustering art", "abstract": "We extend a recently developed method (K. Barnard and D. Forsyth, 2001) for learning the semantics of image databases using text and pictures. We incorporate statistical natural language processing in order to deal with free text. We demonstrate the current system on a difficult dataset, namely 10000 images of work from the Fine Arts Museum of San Francisco. The images include line drawings, paintings, and pictures of sculpture and ceramics. Many of the images have associated free text which varies greatly from physical description to interpretation and mood. We use WordNet to provide semantic grouping information and to help disambiguate word senses, as well as emphasize the hierarchical nature of semantic relationships. This allows us to impose a natural structure on the image collection that reflects semantics to a considerable degree. Our method produces a joint probability distribution for words and picture elements. We demonstrate that this distribution can be used: (a) to provide illustrations for given captions, and (b) to generate words for images outside the training set. Results from this annotation process yield a quantitative study of our method. Finally, the annotation process can be seen as a form of object recognizer that has been learned through a partially supervised process.", "date": "2000", "authors": ["K. Barnard , P. Duygulu , D. Forsyth"], "references": [2121947440, 2049633694, 2102381086, 2135705692, 2101210369, 1934863104, 2081687495, 2099251025, 1572100238, 2159882563]}, {"id": 2055225264, "title": "PicASHOW: pictorial authority search by hyperlinks on the web.", "abstract": "", "date": "2001", "authors": ["Ronny Lempel , Aya Soffer"], "references": [3013264884, 2138621811, 2006119904, 2008297189, 2079672501, 1987777228, 2089199911, 2099251025, 2140350208, 2117086609]}, {"id": 2050457084, "title": "Categories, Photographs & Predicaments: Exploratory Research on Representing Pictures for Access", "abstract": "", "date": "2005", "authors": ["Brian C. O'Connor , Mary Keeney O'Connor"], "references": [2141282920, 2001574834, 2152027810, 2273590290, 2252966386]}, {"id": 181417509, "title": "Storage and Retrieval of Feature Data for a Very Large Online Image Collection.", "abstract": "", "date": "1995", "authors": ["T. K. Rengarajan , Lucien A. Dimino , Dwayne Chung"], "references": [2141282920, 1947400014, 2153166546, 2086174602, 1978530472, 2079855258, 1587360652, 2171401675, 1488607950, 2116442280]}, {"id": 2612148268, "title": "Categories, photographs & predicaments : Exploratory research on representing pictures for access : Theory and practice in the organization of images and other visuo-spatial data for retrieval", "abstract": "", "date": "1998", "authors": ["B. C. O'connor , M. K. O'connor"], "references": [2141282920, 2152027810]}, {"id": 2217896605, "title": "Neural network-based face detection", "abstract": "We present a neural network-based upright frontal face detection system. A retinally connected neural network examines small windows of an image and decides whether each window contains a face. The system arbitrates between multiple networks to improve performance over a single network. We present a straightforward procedure for aligning positive face examples for training. To collect negative examples, we use a bootstrap algorithm, which adds false detections into the training set as training progresses. This eliminates the difficult task of manually selecting nonface training examples, which must be chosen to span the entire space of nonface images. Simple heuristics, such as using the fact that faces rarely overlap in images, can further improve the accuracy. Comparisons with several other state-of-the-art face detection systems are presented, showing that our system has comparable performance in terms of detection and false-positive rates.", "date": "1997", "authors": ["H.A. Rowley 1, S. Baluja 2, T. Kanade 1"], "references": [2139212933, 2313307644, 2133671888, 2124351082, 2147800946, 2098947662, 1997011019, 2173629880, 2042371054, 2159173611]}, {"id": 2045656233, "title": "Bayesian Data Analysis", "abstract": "FUNDAMENTALS OF BAYESIAN INFERENCE Probability and Inference Single-Parameter Models Introduction to Multiparameter Models Asymptotics and Connections to Non-Bayesian Approaches Hierarchical Models FUNDAMENTALS OF BAYESIAN DATA ANALYSIS Model Checking Evaluating, Comparing, and Expanding Models Modeling Accounting for Data Collection Decision Analysis ADVANCED COMPUTATION Introduction to Bayesian Computation Basics of Markov Chain Simulation Computationally Efficient Markov Chain Simulation Modal and Distributional Approximations REGRESSION MODELS Introduction to Regression Models Hierarchical Linear Models Generalized Linear Models Models for Robust Inference Models for Missing Data NONLINEAR AND NONPARAMETRIC MODELS Parametric Nonlinear Models Basic Function Models Gaussian Process Models Finite Mixture Models Dirichlet Process Models APPENDICES A: Standard Probability Distributions B: Outline of Proofs of Asymptotic Theorems C: Computation in R and Stan Bibliographic Notes and Exercises appear at the end of each chapter.", "date": "1994", "authors": ["Andrew Gelman , John B. Carlin , Hal S. Stern , David B. Dunson , Aki Vehtari , Donald B. Rubin"], "references": [1951724000, 1880262756, 2098126593, 2107034620, 2577537660, 2156267802, 2141913814, 2115733720]}, {"id": 2130416410, "title": "Markov Chain Monte Carlo in Practice", "abstract": "INTRODUCING MARKOV CHAIN MONTE CARLO Introduction The Problem Markov Chain Monte Carlo Implementation Discussion HEPATITIS B: A CASE STUDY IN MCMC METHODS Introduction Hepatitis B Immunization Modelling Fitting a Model Using Gibbs Sampling Model Elaboration Conclusion MARKOV CHAIN CONCEPTS RELATED TO SAMPLING ALGORITHMS Markov Chains Rates of Convergence Estimation The Gibbs Sampler and Metropolis-Hastings Algorithm INTRODUCTION TO GENERAL STATE-SPACE MARKOV CHAIN THEORY Introduction Notation and Definitions Irreducibility, Recurrence, and Convergence Harris Recurrence Mixing Rates and Central Limit Theorems Regeneration Discussion FULL CONDITIONAL DISTRIBUTIONS Introduction Deriving Full Conditional Distributions Sampling from Full Conditional Distributions Discussion STRATEGIES FOR IMPROVING MCMC Introduction Reparameterization Random and Adaptive Direction Sampling Modifying the Stationary Distribution Methods Based on Continuous-Time Processes Discussion IMPLEMENTING MCMC Introduction Determining the Number of Iterations Software and Implementation Output Analysis Generic Metropolis Algorithms Discussion INFERENCE AND MONITORING CONVERGENCE Difficulties in Inference from Markov Chain Simulation The Risk of Undiagnosed Slow Convergence Multiple Sequences and Overdispersed Starting Points Monitoring Convergence Using Simulation Output Output Analysis for Inference Output Analysis for Improving Efficiency MODEL DETERMINATION USING SAMPLING-BASED METHODS Introduction Classical Approaches The Bayesian Perspective and the Bayes Factor Alternative Predictive Distributions How to Use Predictive Distributions Computational Issues An Example Discussion HYPOTHESIS TESTING AND MODEL SELECTION Introduction Uses of Bayes Factors Marginal Likelihood Estimation by Importance Sampling Marginal Likelihood Estimation Using Maximum Likelihood Application: How Many Components in a Mixture? Discussion Appendix: S-PLUS Code for the Laplace-Metropolis Estimator MODEL CHECKING AND MODEL IMPROVEMENT Introduction Model Checking Using Posterior Predictive Simulation Model Improvement via Expansion Example: Hierarchical Mixture Modelling of Reaction Times STOCHASTIC SEARCH VARIABLE SELECTION Introduction A Hierarchical Bayesian Model for Variable Selection Searching the Posterior by Gibbs Sampling Extensions Constructing Stock Portfolios With SSVS Discussion BAYESIAN MODEL COMPARISON VIA JUMP DIFFUSIONS Introduction Model Choice Jump-Diffusion Sampling Mixture Deconvolution Object Recognition Variable Selection Change-Point Identification Conclusions ESTIMATION AND OPTIMIZATION OF FUNCTIONS Non-Bayesian Applications of MCMC Monte Carlo Optimization Monte Carlo Likelihood Analysis Normalizing-Constant Families Missing Data Decision Theory Which Sampling Distribution? Importance Sampling Discussion STOCHASTIC EM: METHOD AND APPLICATION Introduction The EM Algorithm The Stochastic EM Algorithm Examples GENERALIZED LINEAR MIXED MODELS Introduction Generalized Linear Models (GLMs) Bayesian Estimation of GLMs Gibbs Sampling for GLMs Generalized Linear Mixed Models (GLMMs) Specification of Random-Effect Distributions Hyperpriors and the Estimation of Hyperparameters Some Examples Discussion HIERARCHICAL LONGITUDINAL MODELLING Introduction Clinical Background Model Detail and MCMC Implementation Results Summary and Discussion MEDICAL MONITORING Introduction Modelling Medical Monitoring Computing Posterior Distributions Forecasting Model Criticism Illustrative Application Discussion MCMC FOR NONLINEAR HIERARCHICAL MODELS Introduction Implementing MCMC Comparison of Strategies A Case Study from Pharmacokinetics-Pharmacodynamics Extensions and Discussion BAYESIAN MAPPING OF DISEASE Introduction Hypotheses and Notation Maximum Likelihood Estimation of Relative Risks Hierarchical Bayesian Model of Relative Risks Empirical Bayes Estimation of Relative Risks Fully Bayesian Estimation of Relative Risks Discussion MCMC IN IMAGE ANALYSIS Introduction The Relevance of MCMC to Image Analysis Image Models at Different Levels Methodological Innovations in MCMC Stimulated by Imaging Discussion MEASUREMENT ERROR Introduction Conditional-Independence Modelling Illustrative examples Discussion GIBBS SAMPLING METHODS IN GENETICS Introduction Standard Methods in Genetics Gibbs Sampling Approaches MCMC Maximum Likelihood Application to a Family Study of Breast Cancer Conclusions MIXTURES OF DISTRIBUTIONS: INFERENCE AND ESTIMATION Introduction The Missing Data Structure Gibbs Sampling Implementation Convergence of the Algorithm Testing for Mixtures Infinite Mixtures and Other Extensions AN ARCHAEOLOGICAL EXAMPLE: RADIOCARBON DATING Introduction Background to Radiocarbon Dating Archaeological Problems and Questions Illustrative Examples Discussion Index", "date": "1997", "authors": ["W.R. Gilks , S. Richardson , David Spiegelhalter"], "references": [2108207895, 2163738067]}, {"id": 2049633694, "title": "Maximum likelihood from incomplete data via the EM algorithm", "abstract": "", "date": "1977", "authors": ["Arthur P. Dempster , Nan M. Laird , Donald B. Rubin"], "references": [2798643531, 2100358124, 2074673068, 2403035479, 2327022120, 1982585616, 1575431606, 2086699924, 2000084758, 2144578442]}, {"id": 2124351162, "title": "\"GrabCut\": interactive foreground extraction using iterated graph cuts", "abstract": "The problem of efficient, interactive foreground/background segmentation in still images is of great practical importance in image editing. Classical image segmentation tools use either texture (colour) information, e.g. Magic Wand, or edge (contrast) information, e.g. Intelligent Scissors. Recently, an approach based on optimization by graph-cut has been developed which successfully combines both types of information. In this paper we extend the graph-cut approach in three respects. First, we have developed a more powerful, iterative version of the optimisation. Secondly, the power of the iterative algorithm is used to simplify substantially the user interaction needed for a given quality of result. Thirdly, a robust algorithm for \"border matting\" has been developed to estimate simultaneously the alpha-matte around an object boundary and the colours of foreground pixels. We show that for moderately difficult examples the proposed method outperforms competitive tools.", "date": "2004", "authors": ["Carsten Rother , Vladimir Kolmogorov , Andrew Blake"], "references": [2104095591, 2169551590, 2049633694, 2101309634, 3110827747, 2077786999, 2103917701, 2740373864, 2103334940, 1785730614]}, {"id": 2024046085, "title": "Additive Logistic Regression : A Statistical View of Boosting", "abstract": "Boosting is one of the most important recent developments in classification methodology. Boosting works by sequentially applying a classification algorithm to reweighted versions of the training data and then taking a weighted majority vote of the sequence of classifiers thus produced. For many classification algorithms, this simple strategy results in dramatic improvements in performance. We show that this seemingly mysterious phenomenon can be understood in terms of well-known statistical principles, namely additive modeling and maximum likelihood. For the two-class problem, boosting can be viewed as an approximation to additive modeling on the logistic scale using maximum Bernoulli likelihood as a criterion. We develop more direct approximations and show that they exhibit nearly identical results to boosting. Direct multiclass generalizations based on multinomial likelihood are derived that exhibit performance comparable to other recently proposed multiclass generalizations of boosting in most situations, and far superior in some. We suggest a minor modification to boosting that can reduce computation, often by factors of 10 to 50. Finally, we apply these insights to produce an alternative formulation of boosting decision trees. This approach, based on best-first truncated tree induction, often leads to better performance, and can provide interpretable descriptions of the aggregate decision rule. It is also much faster computationally, making it more suitable to large-scale data mining applications.", "date": "2000", "authors": ["Jerome Friedman , Trevor Hastie , Robert Tibshirani"], "references": [1678356000, 1540007258, 2099968818, 139959648, 1881647329, 2141518341]}, {"id": 2169551590, "title": "Interactive graph cuts for optimal boundary & region segmentation of objects in N-D images", "abstract": "In this paper we describe a new technique for general purpose interactive segmentation of N-dimensional images. The user marks certain pixels as \"object\" or \"background\" to provide hard constraints for segmentation. Additional soft constraints incorporate both boundary and region information. Graph cuts are used to find the globally optimal segmentation of the N-dimensional image. The obtained solution gives the best balance of boundary and region properties among all segmentations satisfying the constraints. The topology of our segmentation is unrestricted and both \"object\" and \"background\" segments may consist of several isolated parts. Some experimental results are presented in the context of photo/video editing and medical image segmentation. We also demonstrate an interesting Gestalt example. A fast implementation of our segmentation method is possible via a new max-flow algorithm.", "date": "2001", "authors": ["Y.Y. Boykov 1, M.-P. Jolly 2"], "references": [2121947440, 2104095591, 2113137767, 1991113069, 1564419782, 2098152234, 2086921140, 2096139825, 1987983010, 2132603077]}, {"id": 2168002178, "title": "Shape matching and object recognition using low distortion correspondences", "abstract": "We approach recognition in the framework of deformable shape matching, relying on a new algorithm for finding correspondences between feature points. This algorithm sets up correspondence as an integer quadratic programming problem, where the cost function has terms based on similarity of corresponding geometric blur point descriptors as well as the geometric distortion between pairs of corresponding feature points. The algorithm handles outliers, and thus enables matching of exemplars to query images in the presence of occlusion and clutter. Given the correspondences, we estimate an aligning transform, typically a regularized thin plate spline, resulting in a dense correspondence between the two shapes. Object recognition is then handled in a nearest neighbor framework where the distance between exemplar and query is the matching cost between corresponding points. We show results on two datasets. One is the Caltech 101 dataset (Fei-Fei, Fergus and Perona), an extremely challenging dataset with large intraclass variation. Our approach yields a 48% correct classification rate, compared to Fei-Fei et al 's 16%. We also show results for localizing frontal and profile faces that are comparable to special purpose approaches tuned to faces.", "date": "2005", "authors": ["A.C. Berg , T.L. Berg , J. Malik"], "references": [2151103935, 3097096317, 2124386111, 2177274842, 2154422044, 2124087378, 2119823327, 2155511848, 2101522199, 2160754664]}, {"id": 1484228140, "title": "Representing and Recognizing the Visual Appearance of Materials using Three-dimensional Textons", "abstract": "We study the recognition of surfaces made from different materials such as concrete, rug, marble, or leather on the basis of their textural appearance. Such natural textures arise from spatial variation of two surface attributes: (1) reflectance and (2) surface normal. In this paper, we provide a unified model to address both these aspects of natural texture. The main idea is to construct a vocabulary of prototype tiny surface patches with associated local geometric and photometric properties. We call these 3D textons. Examples might be ridges, grooves, spots or stripes or combinations thereof. Associated with each texton is an appearance vector, which characterizes the local irradiance distribution, represented as a set of linear Gaussian derivative filter outputs, under different lighting and viewing conditions. Given a large collection of images of different materials, a clustering approach is used to acquire a small (on the order of 100) 3D texton vocabulary. Given a few (1 to 4) images of any material, it can be characterized using these textons. We demonstrate the application of this representation for recognition of the material viewed under novel lighting and viewing conditions. We also illustrate how the 3D texton model can be used to predict the appearance of materials under novel conditions.", "date": "2001", "authors": ["Thomas Leung , Jitendra Malik"], "references": [2170120409, 2138451337, 2117812871, 2130416410, 1997063559, 1634005169, 1481646516, 2116013899, 2123977795, 3017143921]}, {"id": 3034593359, "title": "Epidemiological and Clinical Characteristics of Cases During the Early Phase of COVID-19 Pandemic: A Systematic Review and Meta-Analysis", "abstract": "Background: On 29th December 2019, a cluster of cases displaying the symptoms of a \"pneumonia of unknown cause\" was identified in Wuhan, Hubei province of China. This systematic review and meta-analysis aims to review the epidemiological and clinical characteristics of COVID-19 cases in the early phase of the COVID-19 pandemic. Methods: The search strategy involved peer-reviewed studies published between 1st January and 11th February 2020 in Pubmed, Google scholar and China Knowledge Resource Integrated database. Publications identified were screened for their title and abstracts according to the eligibility criteria, and further shortlisted by full-text screening. Three independent reviewers extracted data from these studies, and studies were assessed for potential risk of bias. Studies comprising non-overlapping patient populations, were included for qualitative and quantitative synthesis of results. Pooled prevalence with 95% confidence intervals were calculated for patient characteristics. Results: A total of 29 publications were selected after full-text review. This comprised of 18 case reports, three case series and eight cross-sectional studies on patients admitted from mid-December of 2019 to early February of 2020. A total of 533 adult patients with pooled median age of 56 (95% CI: 49-57) and a pooled prevalence of male of 60% (95% CI: 52-68%) were admitted to hospital at a pooled median of 7 days (95% CI: 7-7) post-onset of symptoms. The most common symptoms at admission were fever, cough and fatigue, with a pooled prevalence of 90% (95% CI: 81-97%), 58% (95% CI: 47-68%), and 50% (95% CI: 29-71%), respectively. Myalgia, shortness of breath, headache, diarrhea and sore throat were less common with pooled prevalence of 27% (95% CI: 20-36%), 25% (95% CI: 15-35%), 10% (95% CI: 7-13%), 8% (95% CI: 5-13%), and 7% (95% CI: 1-15%), respectively. ICU patients had a higher proportion of shortness of breath at presentation, as well as pre-existing hypertension, cardiovascular disease and COPD, compared to non-ICU patients in 2 studies (n = 179). Conclusion: This study highlights the key epidemiological and clinical features of COVID-19 cases during the early phase of the COVID-19 pandemic.", "date": "2020", "authors": ["Jiayun Koh , Shimoni Urvish Shah , Pearleen Ee Yong Chua , Hao Gui , Junxiong Pang"], "references": [3001118548, 3001897055, 3008827533, 3005079553, 3003668884, 3002108456, 3002539152, 3008028633, 3001195213, 3003465021]}, {"id": 3035018050, "title": "Early Detection and Assessment of Covid-19", "abstract": "Background: Since the Covid-19 global pandemic emerged, developing countries have been facing multiple challenges over its diagnosis. We aimed to establish a relationship between the signs and symptoms of COVID-19 for early detection and assessment to reduce the transmission rate of SARS-Cov-2. Methods: We collected published data on the clinical features of Covid-19 retrospectively and categorized them into physical and blood biomarkers. Common features were assigned scores by the Borg scoring method with slight modifications and were incorporated into a newly-developed Hashmi-Asif Covid-19 assessment Chart. Correlations between signs and symptoms with the development of Covid-19 was assessed by Pearson correlation and Spearman Correlation coefficient (rho). Linear regression analysis was employed to assess the highest correlating features. The frequency of signs and symptoms in developing Covid-19 was assessed through Chi-square test two tailed with Cramer's V strength. Changes in signs and symptoms were incorporated into a chart that consisted of four tiers representing disease stages. Results: Data from 10,172 Covid-19 laboratory confirmed cases showed a correlation with Fever in 43.9% (P = 0.000) cases, cough 54.08% and dry mucus 25.68% equally significant (P = 0.000), Hyperemic pharyngeal mucus membrane 17.92% (P = 0.005), leukopenia 28.11% (P = 0.000), lymphopenia 64.35% (P = 0.000), thrombopenia 35.49% (P = 0.000), elevated Alanine aminotransferase 50.02% (P = 0.000), and Aspartate aminotransferase 34.49% (P = 0.000). The chart exhibited a maximum scoring of 39. Normal tier scoring was \u2264 12/39, mild state scoring was 13-22/39, and star values scoring was \u22657/15; this latter category on the chart means Covid-19 is progressing and quarantine should be adopted. Moderate stage scored 23-33 and severe scored 34-39 in the chart. Conclusion: The Hashmi-Asif Covid-19 Chart is significant in assessing subclinical and clinical stages of Covid-19 to reduce the transmission rate.", "date": "2020", "authors": ["Hafiz Abdul Sattar Hashmi , Hafiz Muhammad Asif"], "references": [3001118548, 3001897055, 3008827533, 3003668884, 3003465021, 3009912996, 3008696669, 3008818676, 3016535995, 3012747666]}, {"id": 3033301213, "title": "Does Early Childhood Vaccination Protect Against COVID-19?", "abstract": "The coronavirus disease 2019 (COVID-19) is an on-going pandemic caused by the SARS-coronavirus-2 (SARS-CoV-2) which targets the respiratory system of humans. The published data show that children, unlike adults, are less susceptible to contracting the disease. This article aims at understanding why children constitute a minor group among hospitalized COVID-19 patients. Here, we hypothesize that the measles, mumps, and rubella (MMR) vaccine could provide a broad neutralizing antibody against numbers of diseases, including COVID-19. Our hypothesis is based on the 30 amino acid sequence homology between the SARS-CoV-2 Spike (S) glycoprotein (PDB: 6VSB) of both the measles virus fusion (F1) glycoprotein (PDB: 5YXW_B) and the rubella virus envelope (E1) glycoprotein (PDB: 4ADG_A). Computational analysis of the homologous region detected the sequence as antigenic epitopes in both measles and rubella. Therefore, we believe that humoral immunity, created through the MMR vaccination, provides children with advantageous protection against COVID-19 as well, however, an experimental analysis is required.", "date": "2020", "authors": ["Karzan R Sidiq 1, Dana Khdr Sabir 2, Shakhawan M Ali 3, Rimantas Kodzius 4"], "references": [3004318991, 3003217347, 3008818676, 3011242477, 3007643904, 3010819577, 3012310845, 3004348779, 3035011439, 3004896487]}, {"id": 3021916232, "title": "Knowledge, awareness and practice of health care professionals amid sars-cov-2, corona virus disease outbreak", "abstract": "Objective: To assess the knowledge, awareness and practice level of health care workers towards Corona Virus disease - 2019 (COVID-19). Methods: A cross sectional study was conducted by administering a well-structured questionnaire comprising of three sections including knowledge, attitude and practice amongst health care professionals in various hospitals and clinics, over a duration of two months \u2018Feb-March\u2019 2020. The data from 810 participants were collected manually as well as through online survey registered on www.surveys.google.com, using a validated questionnaire. The questionnaire comprised of three sections assessing knowledge, awareness and practice of participants. The descriptive analysis was carried out for demographics and dependent variables with statistical program for social sciences. Spearman test was used to detect any relationship between the health care professional response with respect to their gender and level of education. A p-value of < 0.05 was considered statistically significant. Results: More than half (57.2%) of the health care professionals were working in a hospital setting. Fifty two percent of health care professionals had awareness and 72% were practicing adequate measures to combat COVID-19. The majority (81.9%) believed that the sign and symptoms are similar to a common flu and the main strata of population that could be affected by COVID-19 are elderly (79%). Seventy three percent of participants did not attend any lecture, workshop or seminar on COVID-19 for awareness purpose. Sixty seven percent of health care professionals were practicing universal precaution for infection control and 57.4% were using sodium hypochlorite as a surface disinfectant in dental surgeries. There was no significant relationship (p > 0.05) between the health care professionals\u2019 responses with gender and their education level. Conclusion: The study suggests that the vast majority of the health care professionals have adequate knowledge and awareness related to COVID-19. However some aspects of practice of health care professionals were found to be deficient including, following CDC guidelines during patient care, acquiring verified knowledge related to COVID-19, disinfection protocol and the use of N-95 mask. Mandatory Continued professional development programs including lectures and workshops on COVID-19 for all health care professionals are the need of the hour, to manage the pandemic and limiting the morbidity and mortality related to it. doi: https://doi.org/10.12669/pjms.36.COVID19-S4.2704 How to cite this:Ahmed N, Shakoor M, Vohra F, Abduljabbar T, Mariam Q, Rehman MA. Knowledge, Awareness and Practice of Health care Professionals amid SARS-CoV-2, Corona Virus Disease Outbreak. Pak J Med Sci. 2020;36(COVID19-S4):COVID19-S49-S56.  doi: https://doi.org/10.12669/pjms.36.COVID19-S4.2704 This is an Open Access article distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/3.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.", "date": "2020", "authors": ["Naseer Ahmed 1, Maria Shakoor 2, Fahim Vohra 3, Tariq Abduljabbar 3, Quratulain Mariam 4, Mariam Abdul Rehman 5"], "references": [3009885589, 3008028633, 3008696669, 3008818676, 3006645647, 3011176674, 3009607814, 3005798348, 3011802905, 3008196127]}, {"id": 3037552531, "title": "Analysis of clinical features and early warning signs in patients with severe COVID-19: A retrospective cohort study.", "abstract": "Coronavirus disease 2019 (COVID-19) was first identified in Wuhan, China, in December 2019. Although previous studies have described the clinical aspects of COVID-19, few studies have focused on the early detection of severe COVID-19. Therefore, this study aimed to identify the predictors of severe COVID-19 and to compare clinical features between patients with severe COVID-19 and those with less severe COVID-19. Patients admitted to designated hospital in the Henan Province of China who were either discharged or died prior to February 15, 2020 were enrolled retrospectively. Additionally, patients who underwent at least one of the following treatments were assigned to the severe group: continuous renal replacement therapy, high-flow oxygen absorption, noninvasive and invasive mechanical ventilation, or extracorporeal membrane oxygenation. The remaining patients were assigned to the non-severe group. Demographic information, initial symptoms, and first visit examination results were collected from the electronic medical records and compared between the groups. Multivariate logistic regression analysis was performed to determine the predictors of severe COVID-19. A receiver operating characteristic curve was used to identify a threshold for each predictor. Altogether,104 patients were enrolled in our study with 30 and 74 patients in the severe and non-severe groups, respectively. Multivariate logistic analysis indicated that patients aged \u226563 years (odds ratio = 41.0; 95% CI: 2.8, 592.4), with an absolute lymphocyte value of \u22641.02\u00d7109/L (odds ratio = 6.1; 95% CI = 1.5, 25.2) and a C-reactive protein level of \u226565.08mg/L (odds ratio = 8.9; 95% CI = 1.0, 74.2) were at a higher risk of severe illness. Thus, our results could be helpful in the early detection of patients at risk for severe illness, enabling the implementation of effective interventions and likely lowering the morbidity of COVID-19 patients.", "date": "2020", "authors": ["Xinkui Liu 1, Xinpei Yue 1, Furong Liu 1, Le Wei 1, Yuntian Chu 2, Honghong Bao 1, Yichao Dong 1, Wenjie Cheng 1, Linpeng Yang 1"], "references": [3001118548, 3008827533, 3005079553, 3009885589, 3004280078, 3008818676, 2131262274, 3009976289, 3003749070, 2163627712]}, {"id": 3031029566, "title": "Identification of RT-PCR-Negative Asymptomatic COVID-19 Patients via Serological Testing", "abstract": "Asymptomatic individuals with coronavirus disease (COVID-19) have been identified via nucleic acid testing for severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2); however, the epidemiologic characteristics and viral shedding pattern of asymptomatic patients remain largely unknown. In this study, serological testing was applied when identifying nine asymptomatic cases of COVID-19 who showed persistent negative RT-PCR test results for SARS-CoV-2 nucleic acid and no symptoms of COVID-19. Two asymptomatic cases were presumed to be index patients who had cleared the virus when their close contacts developed symptoms of COVID-19. Three of the asymptomatic cases were local individuals who spontaneously recovered before their presumed index patients developed symptoms of COVID-19. This report presents the epidemiologic and clinical characteristics of asymptomatic individuals with SARS-CoV-2 infection that were undetected on RT-PCR tests in previous epidemiologic investigations probably due to the transient viral shedding duration.", "date": "2020", "authors": ["Jinru Wu 1, 2, Xinyi Liu 1, Dan Zhou 2, Guangqian Qiu 2, Miao Dai 2, Qingting Yang 2, Zhonghui Pan 2, Ning Zhou 3, Pa Wu 1"], "references": [3002539152, 3006961006, 3008696669, 3008818676, 3015571324, 3008874180, 3010781325, 3035011439, 3015792206, 3012188173]}, {"id": 3037451072, "title": "Analysis of Risk Perceptions and Related Factors Concerning COVID-19 Epidemic in Chongqing, China.", "abstract": "To assess perceptions of risk and related factors concerning COVID-19 epidemic among residents in Chongqing city, China. With convenience sampling, a web questionnaire survey was conducted among 476 residents living in Chongqing on February 13rd to 14th in 2020, when citizens just started to get back to work. Residents\u2019 estimated perceived risks were (4.63\u2009\u00b1\u20090.57), (4.19\u2009\u00b1\u20090.76), (3.23\u2009\u00b1\u20090.91) and (2.29\u2009\u00b1\u20090.96) for the infectivity, pathogenicity, lethality and self-rated infection possibility of COVID-19, respectively. Females (OR\u2009=\u20094.234), people with income\u2009\u2265\u20092000 yuan (2000\u20134999 yuan: OR\u2009=\u20095.052, 5000\u20139999 yuan: OR\u2009=\u20094.301,\u2009\u2265\u200910,000 yuan: OR\u2009=\u200923.459), the married status (OR\u2009=\u20091.811), the divorced status, widows or widowers (OR\u2009=\u20093.038), people living with families including children (OR\u2009=\u20095.085) or chronic patients (OR\u2009=\u20092.423) had a higher perceived risk level, as well as people who used free media websites (OR\u2009=\u20091.756), community workers (OR\u2009=\u20094.064) or community information platforms (OR\u2009=\u20092.235) as main media information sources. The perceived risk increased by 4.9% for every one-year increase of age. People who used WeChat contacts (OR\u2009=\u20090.196) as the main media information source, reported a lower perceived risk. Residents reported a high level of risk perception towards COVID-19 in Chongqing and it was impacted by the population demographic characteristics. Media information sources, including community information platforms and community workers may cause the increase of public risk perceptions.", "date": "2020", "authors": ["Shan He 1, Siyu Chen 2, Lingna Kong 1, Weiwei Liu 1"], "references": [3001118548, 3001897055, 3003668884, 3001465255, 3008818676, 2792024998, 3016902371, 2091069417, 3022459584, 3006304225]}, {"id": 3037851904, "title": "Could urinary ACE2 protein level help identify individuals susceptible to SARS-CoV-2 infection and complication?", "abstract": "", "date": "2020", "authors": ["Xiaotian Ni 1, 2, Changqing Sun 1, Yaping Tian 3, Yanjie Huang 4, Tongqing Gong 5, Lan Song 2, Xing Yang 5, Kai Li 2, Nairen Zheng 2, Jianping Wang 5, Hongxing Wu 5, Ruoxian Zhang 2, Yi Wang 2, Guangshun Wang 1, Jun Qin 1, 2"], "references": [3008827533, 3004280078, 3008818676, 2601317354, 2946740876]}, {"id": 2102634410, "title": "Fleischner Society: Glossary of Terms for Thoracic Imaging", "abstract": "Members of the Fleischner Society compiled a glossary of terms for thoracic imaging that replaces previous glossaries published in 1984 and 1996 for thoracic radiography and computed tomography (CT), respectively. The need to update the previous versions came from the recognition that new words have emerged, others have become obsolete, and the meaning of some terms has changed. Brief descriptions of some diseases are included, and pictorial examples (chest radiographs and CT scans) are provided for the majority of terms.", "date": "2008", "authors": ["David M Hansell , Alexander A Bankier , Heber MacMahon , Theresa C McLoud , Nestor L M\u00fcller , Jacques Remy"], "references": [2416914730, 2626588662, 2017898137, 1924766221, 2121350896, 2107051779, 2041775285, 2157678780, 2101610532, 2114857071]}, {"id": 2800783955, "title": "Radiographic and CT Features of Viral Pneumonia.", "abstract": "Viruses are the most common causes of respiratory infection. The imaging findings of viral pneumonia are diverse and overlap with those of other nonviral infectious and inflammatory conditions. However, identification of the underlying viral pathogens may not always be easy. There are a number of indicators for identifying viral pathogens on the basis of imaging patterns, which are associated with the pathogenesis of viral infections. Viruses in the same viral family share a similar pathogenesis of pneumonia, and the imaging patterns have distinguishable characteristics. Although not all cases manifest with typical patterns, most typical imaging patterns of viral pneumonia can be classified according to viral families. Although a definite diagnosis cannot be achieved on the basis of imaging features alone, recognition of viral pneumonia patterns may aid in differentiating viral pathogens, thus reducing the use of antibiotics. Recently, new viruses associated with recent outbreaks including human metapneumovirus, severe acute respiratory syndrome coronavirus, and Middle East respiratory syndrome coronavirus have been discovered. The imaging findings of these emerging pathogens have been described in a few recent studies. This review focuses on the radiographic and computed tomographic patterns of viral pneumonia caused by different pathogens, including new pathogens. Clinical characteristics that could affect imaging, such as patient age and immune status, seasonal variation and community outbreaks, and pathogenesis, are also discussed. The first goal of this review is to indicate that there are imaging features that should raise the possibility of viral infections. Second, to help radiologists differentiate viral infections, viruses in the same viridae that have similar pathogenesis and can have similar imaging characteristics are shown. By considering both the clinical and radiologic characteristics, radiologists can suggest the diagnosis of viral pneumonia. \u00a9RSNA, 2018.", "date": "2018", "authors": ["Hyun Jung Koo , Soyeoun Lim , Jooae Choe , Sang Ho Choi , Heungsup Sung , Kyung Hyun Do"], "references": [3004906315, 3006643024, 3004668429, 3008207212, 3010061930, 3112195624, 3007670341, 3012817089, 3010182902, 3014298809]}, {"id": 2112136274, "title": "Middle East Respiratory Syndrome Coronavirus (MERS-CoV) Infection: Chest CT Findings", "abstract": "OBJECTIVE. The purpose of this study was to describe the chest CT findings in seven patients with Middle East respiratory syndrome coronavirus (MERS-CoV) infection. CONCLUSION. The most common CT finding in hospitalized patients with MERS-CoV infection is that of bilateral predominantly subpleural and basilar airspace changes, with more extensive ground-glass opacities than consolidation. The subpleural and peribronchovascular predilection of the abnormalities is suggestive of an organizing pneumonia pattern.", "date": "2014", "authors": ["Amr M. Ajlan , Rayan A. Ahyad , Lamia Ghazi Jamjoom , Ahmed Alharthy , Tariq A. Madani"], "references": [2166867592, 2107053896, 2006434809, 2149661971, 1703839189, 2045002682, 1852588318, 2109520345, 2119837294, 2049975503]}, {"id": 2056155046, "title": "Severe Acute Respiratory Syndrome: Temporal Lung Changes at Thin-Section CT in 30 Patients", "abstract": "PURPOSE: To evaluate lung abnormalities on serial thin-section computed tomographic (CT) scans in patients with severe acute respiratory syndrome (SARS) during acute and convalescent periods. MATERIALS AND METHODS: Serial thin-section CT scans in 30 patients (17 men, aged 42.5 years \u00b1 12.2 [SD]) with SARS were reviewed by two radiologists together for predominant patterns of lung abnormalities: ground-glass opacities, ground-glass opacities with superimposed linear opacities, consolidation, reticular pattern, and mixed pattern (consolidation, ground-glass opacities, and reticular pattern). Scans were classified according to duration in weeks after symptom onset. Longitudinal changes of specific abnormalities were documented in 17 patients with serial scans obtained during 3 weeks. Each lung was divided into three zones; each zone was evaluated for percentage of lung involvement. Summation of scores from all six lung zones provided overall CT score (maximal CT score, 24). RESULTS: Median CT scores increase...", "date": "2004", "authors": ["Gaik C. Ooi , Pek L. Khong , Nestor L. M\u00fcller , Wai C. Yiu , Lin J. Zhou , James C. M. Ho , Bing Lam , Savvas Nicolaou , Kenneth W. T. Tsang"], "references": [2025170735, 2131262274, 2125251240, 1971054351, 1976741900, 2151996610, 2119467724, 2099918622, 2098293966, 2124413369]}, {"id": 2279340859, "title": "Middle East Respiratory Syndrome-Coronavirus Infection: A Case Report of Serial Computed Tomographic Findings in a Young Male Patient.", "abstract": "Radiologic findings of Middle East respiratory syndrome (MERS), a novel coronavirus infection, have been rarely reported. We report a 30-year-old male presented with fever, abdominal pain, and diarrhea, who was diagnosed with MERS. A chest computed tomographic scan revealed rapidly developed multifocal nodular consolidations with ground-glass opacity halo and mixed consolidation, mainly in the dependent and peripheral areas. After treatment, follow-up imaging showed that these abnormalities markedly decreased but fibrotic changes developed.", "date": "2016", "authors": ["Won Jin Choi , Ki Nam Lee , Eun Ju Kang , Hyuck Lee"], "references": [2166867592, 2006434809, 1703839189, 2119837294, 2112136274, 2103118479, 2111742711]}, {"id": 2080286891, "title": "Severe acute respiratory syndrome: radiographic appearances and pattern of progression in 138 patients", "abstract": "PURPOSE: To retrospectively evaluate the radiographic appearances and pattern of progression of severe acute respiratory syndrome (SARS). MATERIALS AND METHODS: Chest radiographs obtained at clinical presentation and during treatment in 138 patients with confirmed SARS (66 men, 72 women; mean age, 39 years; age range, 20\u201383 years) were assessed. Radiographic appearances of pulmonary parenchymal abnormality, distribution, and extent of involvement on initial chest radiographs were documented. Recognizable patterns of radiographic progression were determined by comparing the overall mean percentage of lung involvement for each patient on serial radiographs. RESULTS: Initial chest radiographs were abnormal in 108 of 138 (78.3%) patients and showed air-space opacity. Lower lung zone (70 of 108, 64.8%) and right lung (82 of 108, 75.9%) were more commonly involved. In most patients, peripheral lung involvement was more common (81 of 108, 75.0%). Unifocal involvement (59 of 108, 54.6%) was more common than multi...", "date": "2003", "authors": ["K. T. Wong 1, Gregory E. Antonio , David S. C. Hui , Nelson Lee , Edmund H. Y. Yuen , Alan Wu , C. B. Leung , Timothy Rainer , Peter Cameron 2, Sydney S. C. Chung , Joseph J. Y. Sung , Anil T. Ahuja"], "references": [2131262274, 2151996610, 2119467724, 2129716802, 2321891473]}, {"id": 2162899218, "title": "Radiologic pattern of disease in patients with severe acute respiratory syndrome: the Toronto experience.", "abstract": "Severe acute respiratory syndrome (SARS) is a transmissible febrile respiratory illness caused by a recently discovered coronavirus. Various patterns of disease progression may be observed that have different implications for the prognosis in those affected by SARS. The appearance of the lungs on chest radiographs of patients with this condition may be normal or may include focal airspace opacity or multifocal or diffuse opacities. Thoracic computed tomography (CT) is more sensitive in depicting SARS than is conventional chest radiography, and CT images obtained in patients with normal chest radiographs may show extensive disease and airspace consolidation. However, because the radiologic appearance of SARS is not distinct from that of other diseases that cause lower respiratory tract infection, early identification of SARS will depend in part on the prompt recognition of clusters of cases of febrile respiratory tract illness. To aid in the differential diagnosis and management of SARS, radiologists must ...", "date": "2004", "authors": ["Narinder S. Paul 1, Heidi Roberts 2, Jagdish Butany 2, Tae Bong Chung 2, Wayne Gold 1, 2, Sangeeta Mehta 1, Eli Konen 2, Anuradha Rao 2, Yves Provost 2, Harry H. Hong 3, Leon Zelovitsky 2, Gordon L. Weisbrod 2"], "references": [2104548316, 2025170735, 2131262274, 2100820722, 2163627712, 2080286891, 2155100049, 2124413369, 2123101845, 2049695691]}, {"id": 3005272159, "title": "Chest CT Findings in 2019 Novel Coronavirus (2019-nCoV) Infections from Wuhan, China: Key Points for the Radiologist.", "abstract": "", "date": "2020", "authors": ["Jeffrey P Kanne"], "references": [3033251833, 3024264813, 3009749892, 3033101332, 3034802182]}, {"id": 3004802901, "title": "CT Manifestations of Two Cases of 2019 Novel Coronavirus (2019-nCoV) Pneumonia.", "abstract": "", "date": "2020", "authors": ["Yicheng Fang , Huangqi Zhang , Yunyu Xu , Jicheng Xie , Peipei Pang , Wenbin Ji"], "references": [3006643024, 3006882119, 3008627141, 3025334942, 3010659930, 3007355693, 3008928918, 3034593359, 3011414603]}, {"id": 2092969802, "title": "A decade after SARS: strategies for controlling emerging coronaviruses", "abstract": "Two novel coronaviruses have emerged in humans in the twenty-first century: severe acute respiratory syndrome coronavirus (SARS-CoV) and Middle East respiratory syndrome coronavirus (MERS-CoV), both of which cause acute respiratory distress syndrome (ARDS) and are associated with high mortality rates. There are no clinically approved vaccines or antiviral drugs available for either of these infections; thus, the development of effective therapeutic and preventive strategies that can be readily applied to new emergent strains is a research priority. In this Review, we describe the emergence and identification of novel human coronaviruses over the past 10 years, discuss their key biological features, including tropism and receptor use, and summarize approaches for developing broadly effective vaccines.", "date": "2013", "authors": ["Rachel L. Graham , Eric F. Donaldson , Ralph S. Baric"], "references": [2166867592, 2132260239, 2104548316, 2107053896, 2025170735, 2006434809, 1993577573, 2119111857, 1966238900, 2160011624]}, {"id": 3001465255, "title": "A novel coronavirus outbreak of global health concern.", "abstract": "", "date": "2020", "authors": ["Chen Wang 1, Peter W Horby 2, Frederick G Hayden 3, George F Gao 4"], "references": []}, {"id": 3001456238, "title": "Emerging coronaviruses: Genome structure, replication, and pathogenesis.", "abstract": "The recent emergence of a novel coronavirus (2019-nCoV), which is causing an outbreak of unusual viral pneumonia in patients in Wuhan, a central city in China, is another warning of the risk of CoVs posed to public health. In this minireview, we provide a brief introduction of the general features of CoVs and describe diseases caused by different CoVs in humans and animals. This review will help understand the biology and potential risk of CoVs that exist in richness in wildlife such as bats.", "date": "2020", "authors": ["Yu Chen 1, Qianyun Liu 1, Deyin Guo 2"], "references": [2903899730, 2470646526, 2306794997, 1993577573, 311927316, 2105637133, 2046153984, 2794573360, 2148822770, 2017248106]}, {"id": 3004668429, "title": "Emerging 2019 Novel Coronavirus (2019-nCoV) Pneumonia.", "abstract": "BackgroundThe chest CT findings of patients with 2019 Novel Coronavirus (2019-nCoV) pneumonia have not previously been described in detail.PurposeTo investigate the clinical, laboratory, and imaging findings of emerging 2019-nCoV pneumonia in humans.Materials and MethodsFifty-one patients (25 men and 26 women; age range 16-76 years) with laboratory-confirmed 2019-nCoV infection by using real-time reverse transcription polymerase chain reaction underwent thin-section CT. The imaging findings, clinical data, and laboratory data were evaluated.ResultsFifty of 51 patients (98%) had a history of contact with individuals from the endemic center in Wuhan, China. Fever (49 of 51, 96%) and cough (24 of 51, 47%) were the most common symptoms. Most patients had a normal white blood cell count (37 of 51, 73%), neutrophil count (44 of 51, 86%), and either normal (17 of 51, 35%) or reduced (33 of 51, 65%) lymphocyte count. CT images showed pure ground-glass opacity (GGO) in 39 of 51 (77%) patients and GGO with reticular and/or interlobular septal thickening in 38 of 51 (75%) patients. GGO with consolidation was present in 30 of 51 (59%) patients, and pure consolidation was present in 28 of 51 (55%) patients. Forty-four of 51 (86%) patients had bilateral lung involvement, while 41 of 51 (80%) involved the posterior part of the lungs and 44 of 51 (86%) were peripheral. There were more consolidated lung lesions in patients 5 days or more from disease onset to CT scan versus 4 days or fewer (431 of 712 lesions vs 129 of 612 lesions; P < .001). Patients older than 50 years had more consolidated lung lesions than did those aged 50 years or younger (212 of 470 vs 198 of 854; P < .001). Follow-up CT in 13 patients showed improvement in seven (54%) patients and progression in four (31%) patients.ConclusionPatients with fever and/or cough and with conspicuous ground-glass opacity lesions in the peripheral and posterior lungs on CT images, combined with normal or decreased white blood cells and a history of epidemic exposure, are highly suspected of having 2019 Novel Coronavirus (2019-nCoV) pneumonia.\u00a9 RSNA, 2020.", "date": "2020", "authors": ["Fengxiang Song , Nannan Shi , Fei Shan , Zhiyong Zhang , Jie Shen , Hongzhou Lu , Yun Ling , Yebin Jiang , Yuxin Shi"], "references": [3002108456, 2999409984, 1993577573, 3003901880, 2800783955, 2128312233, 2015323375, 2789752210, 1922522683, 1997020575]}, {"id": 3008627141, "title": "Coronavirus Disease 2019 (COVID-19): A Perspective from China", "abstract": "In December 2019, an outbreak of severe acute respiratory syndrome coronavirus 2 infection occurred in Wuhan, Hubei Province, China, and spread across China and beyond. On February 12, 2020, the World Health Organization officially named the disease caused by the novel coronavirus as coronavirus disease 2019 (COVID-19). Because most patients infected with COVID-19 had pneumonia and characteristic CT imaging patterns, radiologic examinations have become vital in early diagnosis and the assessment of disease course. To date, CT findings have been recommended as major evidence for clinical diagnosis of COVID-19 in Hubei, China. This review focuses on the etiology, epidemiology, and clinical symptoms of COVID-19 while highlighting the role of chest CT in prevention and disease control.", "date": "2020", "authors": ["Zi Yue Zu 1, Meng Di Jiang 2, Peng Peng Xu , Wen Chen , Qian Qian Ni , Guang Ming Lu , Long Jiang Zhang"], "references": [3005079553, 3003668884, 3002539152, 3004280078, 3004318991, 3004239190, 3005679569, 3003951199, 3004906315, 3006110666]}, {"id": 3025334942, "title": "Clinical Features, Diagnosis, and Treatment of COVID-19 in Hospitalized Patients: A Systematic Review of Case Reports and Case Series", "abstract": "Introduction: The 2019 novel coronavirus (COVID-19) has been declared a public health emergency worldwide. The objective of this systematic review was to characterize the clinical, diagnostic, and treatment characteristics of hospitalized patients presenting with COVID-19. Methods: We conducted a structured search using PubMed/Medline, Embase, and Web of Science to collect both case reports and case series on COVID-19 published up to April 24, 2020. There were no restrictions regarding publication language. Results: Eighty articles were included analyzing a total of 417 patients with a mean age of 48 years. The most common presenting symptom in patients who tested positive for COVID-19 was fever, reported in up to 62% of patients from 82% of the analyzed studies. Other symptoms including rhinorrhea, dizziness, and chills were less frequently reported. Additionally, in studies that reported C-reactive protein (CRP) measurements, a large majority of patients displayed an elevated CRP (60%). Progression to acute respiratory distress syndrome (ARDS) was the most common complication of patients testing positive for COVID-19 (21%). CT images displayed ground-glass opacification (GGO) patterns (80%) as well as bilateral lung involvement (69%). The most commonly used antiviral treatment modalities included, lopinavir (HIV protease inhibitor), arbidiol hydrochloride (influenza fusion inhibitor), and oseltamivir (neuraminidase inhibitor). Conclusions: Development of ARDS may play a role in estimating disease progression and mortality risk. Early detection of elevations in serum CRP, combined with a clinical COVID-19 symptom presentation may be used as a surrogate marker for the presence and severity of the disease. There is a paucity of data surrounding the efficacy of treatments. There is currently not a well-established gold standard therapy for the treatment of diagnosed COVID-19. Further prospective investigations are necessary.", "date": "2020", "authors": ["Azin Tahvildari 1, Mahta Arbabi 1, Yeganeh Farsi 1, Parnian Jamshidi 1, Saba Hasanzadeh 1, Tess Moore Calcagno 1, Mohammad Javad Nasiri 2, Mehdi Mirsaeidi 1"], "references": [3001118548, 3001897055, 3005079553, 3003668884, 3002108456, 3002539152, 2007872832, 3004318991, 2156098321, 3003465021]}, {"id": 3009992310, "title": "Molecular immune pathogenesis and diagnosis of COVID-19.", "abstract": "Coronavirus disease 2019 (COVID-19) is a kind of viral pneumonia which is caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). The emergence of SARS-CoV-2 has been marked as the third introduction of a highly pathogenic coronavirus into the human population after the severe acute respiratory syndrome coronavirus (SARS-CoV) and the Middle East respiratory syndrome coronavirus (MERS-CoV) in the twenty-first century. In this minireview, we provide a brief introduction of the general features of SARS-CoV-2 and discuss current knowledge of molecular immune pathogenesis, diagnosis and treatment of COVID-19 on the base of the present understanding of SARS-CoV and MERS-CoV infections, which may be helpful in offering novel insights and potential therapeutic targets for combating the SARS-CoV-2 infection.", "date": "2020", "authors": ["Xiaowei Li , Manman Geng , Yizhao Peng , Liesu Meng , Shemin Lu"], "references": [3001118548, 3004280078, 3004318991, 3001195213, 3003465021, 3003573988, 3003217347, 3007940623, 3005212621, 3004906315]}, {"id": 3008928918, "title": "First case of Coronavirus Disease 2019 (COVID-19) pneumonia in Taiwan.", "abstract": "An outbreak of respiratory illness proved to be infected by a 2019 novel coronavirus, officially named Coronavirus Disease 2019 (COVID-19), was notified first in Wuhan, China, and has spread rapidly in China and to other parts of the world. Herein, we reported the first confirmed case of novel coronavirus pneumonia (NCP) imported from China in Taiwan. This case report revealed a natural course of NCP with self-recovery, which may be a good example in comparison with medical treatments.", "date": "2020", "authors": ["Shao-Chung Cheng , Yuan-Chia Chang , Yu-Long Fan Chiang , Yu-Chan Chien , Mingte Cheng , Chin-Hua Yang , Chia-Husn Huang , Yuan-Nian Hsu"], "references": [3001118548, 3005079553, 3003465021, 3004906315, 3004668429, 3003901880, 2791599184, 3004511262, 3004802901, 2156614913]}, {"id": 3008801544, "title": "Clinical and computed tomographic imaging features of novel coronavirus pneumonia caused by SARS-CoV-2.", "abstract": "Summary Purpose To investigate the clinical and imaging characteristics of computed tomography (CT) in novel coronavirus pneumonia (NCP) caused by SARS-CoV-2. Materials and methods A retrospective analysis was performed on the imaging findings of patients confirmed with COVID-19 pneumonia who had chest CT scanning and treatment after disease onset. The clinical and imaging data were analyzed. Results Fifty patients were enrolled, including mild type in nine, common in 28, severe in 10 and critically severe in the rest three. Mild patients (29 years) were significantly (P \u00b0C), 49 (98%) patients had normal or slightly reduced leukocyte count, 14 (28%) had decreased counts of lymphocytes, and 26 (52%) patients had increased C-reactive protein. Nine mild patients were negative in CT imaging. For all the other types of NCP, the lesion was in the right upper lobe in 30 cases, right middle lobe in 22, right lower lobe in 39, left upper lobe in 33 and left lower lobe in 36. The lesion was primarily located in the peripheral area under the pleura with possible extension towards the pulmonary hilum. Symmetrical lesions were seen in 26 cases and asymmetrical in 15. The density of lesion was mostly uneven with ground glass opacity as the primary presentation accompanied by partial consolidation and fibrosis. Conclusion CT imaging presentations of NCP are mostly patchy ground glass opacities in the peripheral areas under the pleura with partial consolidation which will be absorbed with formation of fibrotic stripes if improved. CT scanning provides important bases for early diagnosis and treatment of NCP.", "date": "2020", "authors": ["Yu-Huan Xu 1, Jing-Hui Dong 1, Wei-Min An 1, Xiao-Yan Lv 1, Xiao-Ping Yin 2, Jian-Zeng Zhang 1, Li Dong 3, Xi Ma 2, Hong-Jie Zhang 2, Bu-Lang Gao 1"], "references": [3001118548, 3005079553, 3004906315, 3006110666, 3003901880, 3006485704, 3006472059, 3004896587, 3004511262, 3004517278]}, {"id": 3011414603, "title": "Association of radiologic findings with mortality of patients infected with 2019 novel coronavirus in Wuhan, China.", "abstract": "Radiologic characteristics of 2019 novel coronavirus (2019-nCoV) infected pneumonia (NCIP) which had not been fully understood are especially important for diagnosing and predicting prognosis. We retrospective studied 27 consecutive patients who were confirmed NCIP, the clinical characteristics and CT image findings were collected, and the association of radiologic findings with mortality of patients was evaluated. 27 patients included 12 men and 15 women, with median age of 60 years (IQR 47-69). 17 patients discharged in recovered condition and 10 patients died in hospital. The median age of mortality group was higher compared to survival group (68 (IQR 63-73) vs 55 (IQR 35-60), P = 0.003). The comorbidity rate in mortality group was significantly higher than in survival group (80% vs 29%, P = 0.018). The predominant CT characteristics consisted of ground glass opacity (67%), bilateral sides involved (86%), both peripheral and central distribution (74%), and lower zone involvement (96%). The median CT score of mortality group was higher compared to survival group (30 (IQR 7-13) vs 12 (IQR 11-43), P = 0.021), with more frequency of consolidation (40% vs 6%, P = 0.047) and air bronchogram (60% vs 12%, P = 0.025). An optimal cutoff value of a CT score of 24.5 had a sensitivity of 85.6% and a specificity of 84.5% for the prediction of mortality. 2019-nCoV was more likely to infect elderly people with chronic comorbidities. CT findings of NCIP were featured by predominant ground glass opacities mixed with consolidations, mainly peripheral or combined peripheral and central distributions, bilateral and lower lung zones being mostly involved. A simple CT scoring method was capable to predict mortality.", "date": "2020", "authors": ["Mingli Yuan , Wen Yin , Zhaowu Tao , Weijun Tan , Yi Hu"], "references": [3001118548, 3005079553, 3003668884, 3002108456, 3003790823, 3004668429, 2766931063, 2112136274, 3004511262, 3004802901]}, {"id": 3013468450, "title": "Novel Coronavirus Infection (COVID-19) in Humans: A Scoping Review and Meta-Analysis", "abstract": "A growing body of literature on the 2019 novel coronavirus (SARS-CoV-2) is becoming available, but a synthesis of available data has not been conducted. We performed a scoping review of currently available clinical, epidemiological, laboratory, and chest imaging data related to the SARS-CoV-2 infection. We searched MEDLINE, Cochrane CENTRAL, EMBASE, Scopus and LILACS from 01 January 2019 to 24 February 2020. Study selection, data extraction and risk of bias assessment were performed by two independent reviewers. Qualitative synthesis and meta-analysis were conducted using the clinical and laboratory data, and random-effects models were applied to estimate pooled results. A total of 61 studies were included (59,254 patients). The most common disease-related symptoms were fever (82%, 95% confidence interval (CI) 56%\u201399%; n = 4410), cough (61%, 95% CI 39%\u201381%; n = 3985), muscle aches and/or fatigue (36%, 95% CI 18%\u201355%; n = 3778), dyspnea (26%, 95% CI 12%\u201341%; n = 3700), headache in 12% (95% CI 4%\u201323%, n = 3598 patients), sore throat in 10% (95% CI 5%\u201317%, n = 1387) and gastrointestinal symptoms in 9% (95% CI 3%\u201317%, n = 1744). Laboratory findings were described in a lower number of patients and revealed lymphopenia (0.93 \u00d7 109/L, 95% CI 0.83\u20131.03 \u00d7 109/L, n = 464) and abnormal C-reactive protein (33.72 mg/dL, 95% CI 21.54\u201345.91 mg/dL; n = 1637). Radiological findings varied, but mostly described ground-glass opacities and consolidation. Data on treatment options were limited. All-cause mortality was 0.3% (95% CI 0.0%\u20131.0%; n = 53,631). Epidemiological studies showed that mortality was higher in males and elderly patients. The majority of reported clinical symptoms and laboratory findings related to SARS-CoV-2 infection are non-specific. Clinical suspicion, accompanied by a relevant epidemiological history, should be followed by early imaging and virological assay.", "date": "2020", "authors": ["Israel J\u00fanior Borges do Nascimento 1, Nensi Cacic 2, Hebatullah Mohamed Abdulazeem 3, Thilo Caspar von Groote 4, Umesh Jayarajah 5, Ishanka Weerasekara 6, 7, Meisam Abdar Esfahani 8, Vinicius Tassoni Civile 9, Ana Marusic 2, Ana Jeroncic 2, Nelson Carvas Junior 10, Tina Poklepovic Pericic 2, Irena Zakarija-Grkovic 2, Silvana Mangeon Meirelles Guimar\u00e3es 1, Nicola Luigi Bragazzi 11, Maria Bjorklund 12, Ahmad Sofi-Mahmudi 8, Mohammad Altujjar 13, Maoyi Tian 14, Diana Maria Cespedes Arcani 15, D\u00f3nal P O'Math\u00fana 16, 17, Milena Soriano Marcolino 1"], "references": [3001118548, 3001897055, 3008827533, 3003668884, 3002108456, 3002539152, 3004318991, 3003465021, 3008090866, 3004239190]}, {"id": 2786098272, "title": "Serological Evidence of Bat SARS-Related Coronavirus Infection in Humans, China", "abstract": "In our previous works, we have reported genetically diverse SARS-related coronaviruses (SARSr-CoV) in a single bat cave, Yunnan province, China, and suggested that some SARSr-CoVs may have high potential to infect humans without the necessity for an intermediate host. In this report, we developed a specific ELISA based on the nucleocapsid protein of a SARSr-CoV strain and detected its antibody in humans who are highly exposed to bat populations. From 218 human serum samples, 6 were positive against the nucleocapsid protein by ELISA and further confirmed by Western blot. For the first time, we demonstrated the SARSr-CoV had spillover to humans, although did not cause clinical diseases.", "date": "2018", "authors": ["Ning Wang 1, Shi-Yue Li 2, Xing-Lou Yang 1, Hui-Min Huang 2, Yu-Ji Zhang 1, Hua Guo 1, Chu-Ming Luo 1, Maureen Miller 3, Guangjian Zhu 3, Aleksei A. Chmura 3, Emily Hagan 3, Ji-Hua Zhou 4, Yun-Zhi Zhang 5, Lin-Fa Wang 6, Peter Daszak 3, Zheng-Li Shi 1"], "references": []}, {"id": 2021442163, "title": "Organ distribution of severe acute respiratory syndrome (SARS) associated coronavirus (SARS-CoV) in SARS patients: implications for pathogenesis and virus transmission pathways.", "abstract": "We previously identified the major pathological changes in the respiratory and immune systems of patients who died of severe acute respiratory syndrome (SARS) but gained little information on the organ distribution of SARS-associated coronavirus (SARS-CoV). In the present study, we used a murine monoclonal antibody specific for SARS-CoV nucleoprotein, and probes specific for a SARS-CoV RNA polymerase gene fragment, for immunohistochemistry and in situ hybridization, respectively, to detect SARS-CoV systematically in tissues from patients who died of SARS. SARS-CoV was found in lung, trachea/bronchus, stomach, small intestine, distal convoluted renal tubule, sweat gland, parathyroid, pituitary, pancreas, adrenal gland, liver and cerebrum, but was not detected in oesophagus, spleen, lymph node, bone marrow, heart, aorta, cerebellum, thyroid, testis, ovary, uterus or muscle. These results suggest that, in addition to the respiratory system, the gastrointestinal tract and other organs with detectable SARS-CoV may also be targets of SARS-CoV infection. The pathological changes in these organs may be caused directly by the cytopathic effect mediated by local replication of the SARS-CoV; or indirectly as a result of systemic responses to respiratory failure or the harmful immune response induced by viral infection. In addition to viral spread through a respiratory route, SARS-CoV in the intestinal tract, kidney and sweat glands may be excreted via faeces, urine and sweat, thereby leading to virus transmission. This study provides important information for understanding the pathogenesis of SARS-CoV infection and sheds light on possible virus transmission pathways. This data will be useful for designing new strategies for prevention and treatment of SARS.", "date": "2004", "authors": ["Yanqing Ding 1, Li He 1, Qingling Zhang 1, Zhongxi Huang 1, Xiaoyan Che 2, Jinlin Hou 3, Huijun Wang 1, Hong Shen 1, Liwen Qiu 3, Zhuguo Li 1, Jian Geng 1, Junjie Cai 1, Huixia Han 1, Xin Li 1, Wei Kang 1, Desheng Weng 1, Ping Liang 1, Shibo Jiang 4"], "references": [2132260239, 2104548316, 2025170735, 2116586125, 2169198329, 1966238900, 2134061616, 2168446943, 1757215199, 2158118659]}, {"id": 3025232310, "title": "Humoral Immune Responses in COVID-19 Patients: A Window on the State of the Art.", "abstract": "The novel SARS-CoV-2 is a recently emerging virus causing a human pandemic. A great variety of symptoms associated with COVID-19 disease, ranging from mild to severe symptoms, eventually leading to death. Specific SARS-CoV-2 RT-PCR is the standard method to screen symptomatic people; however, asymptomatic subjects and subjects with undetectable viral load escape from the screening, contributing to viral spread. Currently, the lock down imposed by many governments is an important measure to contain the spread, as there is no specific antiviral therapy or a vaccine and the main treatments are supportive. Therefore, there is urgent need to characterize the virus and the viral-mediated responses, in order to develop specific diagnostic and therapeutic tools to prevent viral transmission and efficiently cure COVID-19 patients. Here, we review the current studies on two viral mediated-responses, specifically the cytokine storm occurring in a subset of patients and the antibody response triggered by the infection. Further studies are needed to explore both the dynamics and the mechanisms of the humoral immune response in COVID-19 patients, in order to guide future vaccine design and antibody-based therapies for the management of the disease.", "date": "2020", "authors": ["Gabriel Siracusano , Claudia Pastori , Lucia Lopalco"], "references": [3001118548, 3002108456, 3004280078, 3001465255, 3003217347, 3005679569, 3007643904, 3012756997, 3009314935, 3013985547]}, {"id": 3028321619, "title": "SARS-CoV-2 Infection and the Newborn", "abstract": "Severe Acute Respiratory Syndrome Coronavirus Type 2 (SARS-CoV-2) affects people at all ages and it may be encountered in pregnant women and newborns also. The information about its clinical features, laboratory findings and prognosis in children and newborns is scarce. All the reported cases in pregnant women were in the 2nd or 3rd trimester and only 1% of them developed severe disease. Miscarriages are rare. Materno-fetal transmission of the disease is controversial. Definitive diagnosis can be made by a history of contact with a proven case, fever, pneumonia and gastrointestinal disorder and a Polymerase chain reaction (PCR) test of nasopharyngeal swabs. Lymphopenia as well as liver and renal dysfunctions may be seen. Suspected or proven cases of newborns with symptoms should be quarantined in the neonatal intensive care unit for at least 14 days with standart and droplet isolation precautions. Asymptomatic infants may be quaratined at home. Transport of the neonates should be performed in a dedicated transport incubator and ambulance with isolation precautions. There is no specific treatment for the disease, but hemodynamic stabilization of the infant, respiratory management and other daily care are essential. Drugs against cytokine storm syndrome such as corticosteroids or tocilizumab are under investigation. Routine antibiotics are not recommended. No deaths have been reported so far in the neonatal population. Families and healthcare staff should receive pyschological support. Since the infection is quite new and knowledge is constantly accumulating, following developments and continuous updates are crucial.", "date": "2020", "authors": ["Fahri Oval\u0131"], "references": [3001118548, 3002539152, 3008028633, 3004318991, 3007940623, 3010604545, 3005212621, 3010233963, 3011610993, 3005679569]}, {"id": 3027541845, "title": "Psycho-Neuroendocrine-Immune Interactions in COVID-19: Potential Impacts on Mental Health.", "abstract": "Coronavirus disease 2019 (COVID-19) is caused by the Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2). The impacts of the disease may be beyond the respiratory system, also affecting mental health. Several factors may be involved in the association between COVID-19 and psychiatric outcomes, such as fear inherent in the pandemic, adverse effects of treatments, as well as financial stress, and social isolation. Herein we discuss the growing evidence suggesting that the relationship between SARS-CoV-2 and host may also trigger changes in brain and behavior. Based on the similarity of SARS-CoV-2 with other coronaviruses, it is conceivable that changes in endocrine and immune response in the periphery or in the central nervous system may be involved in the association between SARS-CoV-2 infection and impaired mental health. This is likely to be further enhanced, since millions of people worldwide are isolated in quarantine to minimize the transmission of SARS-CoV-2 and social isolation can also lead to neuroendocrine-immune changes. Accordingly, we highlight here the hypothesis that neuroendocrine-immune interactions may be involved in negative impacts of SARS-CoV-2 infection and social isolation on psychiatric issues.", "date": "2020", "authors": ["\u00cdcaro Raony 1, Camila Saggioro de Figueiredo 1, Pablo Pandolfo 1, Elizabeth Giestal-de-Araujo 1, 2, Priscilla Oliveira-Silva Bomfim 1, 2, 3, Wilson Savino 2, 3"], "references": [3001118548, 3005079553, 3002108456, 3012421327, 2903899730, 3006659024, 3006645647, 3009506062, 3004348779, 3009859788]}, {"id": 3003637715, "title": "Molecular Diagnosis of a Novel Coronavirus (2019-nCoV) Causing an Outbreak of Pneumonia.", "abstract": "BACKGROUND: A novel coronavirus of zoonotic origin (2019-nCoV) has recently been identified in patients with acute respiratory disease. This virus is genetically similar to SARS coronavirus and bat SARS-like coronaviruses. The outbreak was initially detected in Wuhan, a major city of China, but has subsequently been detected in other provinces of China. Travel-associated cases have also been reported in a few other countries. Outbreaks in health care workers indicate human-to-human transmission. Molecular tests for rapid detection of this virus are urgently needed for early identification of infected patients. METHODS: We developed two 1-step quantitative real-time reverse-transcription PCR assays to detect two different regions (ORF1b and N) of the viral genome. The primer and probe sets were designed to react with this novel coronavirus and its closely related viruses, such as SARS coronavirus. These assays were evaluated using a panel of positive and negative controls. In addition, respiratory specimens from two 2019-nCoV-infected patients were tested. RESULTS: Using RNA extracted from cells infected by SARS coronavirus as a positive control, these assays were shown to have a dynamic range of at least seven orders of magnitude (2x10-4-2000 TCID50/reaction). Using DNA plasmids as positive standards, the detection limits of these assays were found to be below 10 copies per reaction. All negative control samples were negative in the assays. Samples from two 2019-nCoV-infected patients were positive in the tests. CONCLUSIONS: The established assays can achieve a rapid detection of 2019n-CoV in human samples, thereby allowing early identification of patients.", "date": "2020", "authors": ["Daniel K W Chu 1, Yang Pan 2, Samuel M S Cheng 1, Kenrie P Y Hui 1, Pavithra Krishnan 1, Yingzhi Liu 1, Daisy Y M Ng 1, Carrie K C Wan 1, Peng Yang 2, Quanyi Wang 3, Malik Peiris 1, Leo L M Poon 1"], "references": [3001195213, 2799524357, 2903899730, 2470646526, 2775086803, 2134061616, 2789368753, 1998201250, 1971383779, 2954438954]}, {"id": 3009885589, "title": "Clinical course and risk factors for mortality of adult inpatients with COVID-19 in Wuhan, China: a retrospective cohort study.", "abstract": "Summary Background Since December, 2019, Wuhan, China, has experienced an outbreak of coronavirus disease 2019 (COVID-19), caused by the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). Epidemiological and clinical characteristics of patients with COVID-19 have been reported but risk factors for mortality and a detailed clinical course of illness, including viral shedding, have not been well described. Methods In this retrospective, multicentre cohort study, we included all adult inpatients (\u226518 years old) with laboratory-confirmed COVID-19 from Jinyintan Hospital and Wuhan Pulmonary Hospital (Wuhan, China) who had been discharged or had died by Jan 31, 2020. Demographic, clinical, treatment, and laboratory data, including serial samples for viral RNA detection, were extracted from electronic medical records and compared between survivors and non-survivors. We used univariable and multivariable logistic regression methods to explore the risk factors associated with in-hospital death. Findings 191 patients (135 from Jinyintan Hospital and 56 from Wuhan Pulmonary Hospital) were included in this study, of whom 137 were discharged and 54 died in hospital. 91 (48%) patients had a comorbidity, with hypertension being the most common (58 [30%] patients), followed by diabetes (36 [19%] patients) and coronary heart disease (15 [8%] patients). Multivariable regression showed increasing odds of in-hospital death associated with older age (odds ratio 1\u00b710, 95% CI 1\u00b703\u20131\u00b717, per year increase; p=0\u00b70043), higher Sequential Organ Failure Assessment (SOFA) score (5\u00b765, 2\u00b761\u201312\u00b723; p Interpretation The potential risk factors of older age, high SOFA score, and d-dimer greater than 1 \u03bcg/mL could help clinicians to identify patients with poor prognosis at an early stage. Prolonged viral shedding provides the rationale for a strategy of isolation of infected patients and optimal antiviral interventions in the future. Funding Chinese Academy of Medical Sciences Innovation Fund for Medical Sciences; National Science Grant for Distinguished Young Scholars; National Key Research and Development Program of China; The Beijing Science and Technology Project; and Major Projects of National Science and Technology on New Drug Creation and Development.", "date": "2020", "authors": ["Fei Zhou 1, Ting Yu 2, Ronghui Du 3, Guohui Fan 4, Ying Liu 2, Zhibo Liu 1, Jie Xiang 5, Yeming Wang 6, Bin Song 2, Xiaoying Gu 1, 4, Lulu Guan 3, Yuan Wei 2, Hui Li 1, Xudong Wu 7, Jiuyang Xu 8, Shengjin Tu 2, Yi Zhang 1, Hua Chen 2, Bin Cao"], "references": []}, {"id": 3006961006, "title": "SARS-CoV-2 Viral Load in Upper Respiratory Specimens of Infected Patients.", "abstract": "SARS-CoV-2 Viral Load in Upper Respiratory Specimens The authors report results of an analysis of nasal and throat swabs from 17 patients in Zhuhai, China, who had received a diagnosis of Covid-19....", "date": "2020", "authors": ["Lirong Zou 1, Feng Ruan 1, Mingxing Huang 2, Lijun Liang 1, Huitao Huang 1, Zhongsi Hong 2, Jianxiang Yu 1, Min Kang 1, Yingchao Song 1, Jinyu Xia 2, Qianfang Guo 1, Tie Song 1, Jianfeng He 1, Hui Ling Yen 3, Malik Peiris 3, Jie Wu 1"], "references": [3004239190, 2129542667, 3024919756, 2147166346, 3034411794, 3034408674, 3018339046, 2133748753]}, {"id": 3013893137, "title": "Virological assessment of hospitalized patients with COVID-2019.", "abstract": "Coronavirus disease 2019 (COVID-19) is an acute infection of the respiratory tract that emerged in late 20191,2. Initial outbreaks in China involved 13.8% of cases with severe courses, and 6.1% of cases with critical courses3. This severe presentation may result from the virus using a virus receptor that is expressed predominantly in the lung2,4; the same receptor tropism is thought to have determined the pathogenicity-but also aided in the control-of severe acute respiratory syndrome (SARS) in 20035. However, there are reports of cases of COVID-19 in which the patient shows mild upper respiratory tract symptoms, which suggests the potential for pre- or oligosymptomatic transmission6-8. There is an urgent need for information on virus replication, immunity and infectivity in specific sites of the body. Here we report a detailed virological analysis of nine cases of COVID-19 that provides proof of active virus replication in tissues of the upper respiratory tract. Pharyngeal virus shedding was very high during the first week of symptoms, with a peak at 7.11 \u00d7 108 RNA copies per throat swab on day 4. Infectious virus was readily isolated from samples derived from the throat or lung, but not from stool samples-in spite of high concentrations of virus RNA. Blood and urine samples never yielded virus. Active replication in the throat was confirmed by the presence of viral replicative RNA intermediates in the throat samples. We consistently detected sequence-distinct virus populations in throat and lung samples from one patient, proving independent replication. The shedding of viral RNA from sputum outlasted the end of symptoms. Seroconversion occurred after 7 days in 50% of patients (and by day 14 in all patients), but was not followed by a rapid decline in viral load. COVID-19 can present as a mild illness of the upper respiratory tract. The confirmation of active virus replication in the upper respiratory tract has implications for the containment of COVID-19.", "date": "2020", "authors": ["Roman W\u00f6lfel 1, Victor M. Corman 2, Wolfgang Guggemos 3, Michael Seilmaier 3, Sabine Zange 1, Marcel A. M\u00fcller 2, Daniela Niemeyer 2, Terry C. Jones 2, 4, Patrick Vollmar 1, Camilla Rothe 5, Michael Hoelscher 5, Tobias Bleicker 2, Sebastian Br\u00fcnink 2, Julia Schneider 2, Rosina Ehmann 1, Katrin Zwirglmaier 1, Christian Drosten 2, Clemens Wendtner 3"], "references": [3001897055, 3002108456, 3001195213, 3003465021, 3004239190, 3006961006, 3009912996, 3009906937, 3010338568, 2132260239]}, {"id": 3004824173, "title": "A rapid advice guideline for the diagnosis and treatment of 2019 novel coronavirus (2019-nCoV) infected pneumonia (standard version)", "abstract": "In December 2019, a new type viral pneumonia cases occurred in Wuhan, Hubei Province; and then named \u201c2019 novel coronavirus (2019-nCoV)\u201d by the World Health Organization (WHO) on 12 January 2020. For it is a never been experienced respiratory disease before and with infection ability widely and quickly, it attracted the world\u2019s attention but without treatment and control manual. For the request from frontline clinicians and public health professionals of 2019-nCoV infected pneumonia management, an evidence-based guideline urgently needs to be developed. Therefore, we drafted this guideline according to the rapid advice guidelines methodology and general rules of WHO guideline development; we also added the first-hand management data of Zhongnan Hospital of Wuhan University. This guideline includes the guideline methodology, epidemiological characteristics, disease screening and population prevention, diagnosis, treatment and control (including traditional Chinese Medicine), nosocomial infection prevention and control, and disease nursing of the 2019-nCoV. Moreover, we also provide a whole process of a successful treatment case of the severe 2019-nCoV infected pneumonia and experience and lessons of hospital rescue for 2019-nCoV infections. This rapid advice guideline is suitable for the first frontline doctors and nurses, managers of hospitals and healthcare sections, community residents, public health persons, relevant researchers, and all person who are interested in the 2019-nCoV.", "date": "2020", "authors": ["Ying-Hui Jin 1, Lin Cai 1, Zhen-Shun Cheng 1, Hong Cheng 1, Tong Deng 1, 2, Yi-Pin Fan 3, Cheng Fang 1, Di Huang 1, Lu-Qi Huang 3, Qiao Huang 1, Yong Han 1, Bo Hu 1, Fen Hu 1, Bing-Hui Li 1, 2, Yi-Rong Li 1, Ke Liang 1, Li-Kai Lin 1, Li-Sha Luo 1, Jing Ma 1, Lin-Lu Ma 1, Zhi-Yong Peng 1, Yun-Bao Pan 1, Zhen-Yu Pan 1, Xue-Qun Ren 2, Hui-Min Sun 1, Ying Wang 1, Yun-Yun Wang 1, Hong Weng 1, Chao-Jie Wei 1, Dong-Fang Wu 1, Jian Xia 1, Yong Xiong 1, Hai-Bo Xu 1, Xiao-Mei Yao 4, Yu-Feng Yuan 1, Tai-Sheng Ye 1, Xiao-Chun Zhang 1, Ying-Wen Zhang 1, Yin-Gao Zhang 1, Hua-Min Zhang 3, Yan Zhao 1, Ming-Juan Zhao 1, Hao Zi 1, 2, Xian-Tao Zeng 1, Yong-Yan Wang 3, Xing-Huan Wang 1"], "references": [3001118548, 3001897055, 3002539152, 3004280078, 2165010366, 1803784511, 3002715510, 2034462612, 2150120685, 1945961678]}, {"id": 3003464757, "title": "Genomic characterization of the 2019 novel human-pathogenic coronavirus isolated from a patient with atypical pneumonia after visiting Wuhan", "abstract": "A mysterious outbreak of atypical pneumonia in late 2019 was traced to a seafood wholesale market in Wuhan of China. Within a few weeks, a novel coronavirus tentatively named as 2019 novel coronavi...", "date": "2020", "authors": ["Jasper Fuk Woo Chan 1, Kin Hang Kok 1, 2, Zheng Zhu 2, Hin Chu 1, 2, Kelvin Kai Wang To 3, Shuofeng Yuan 1, 2, Kwok Yung Yuen 1"], "references": [3001118548, 3002539152, 2799524357, 2097706568, 2025170735, 2030966943, 2115555188, 2170933940, 1990059132, 2162496804]}, {"id": 3009834387, "title": "Evidence for gastrointestinal infection of SARS-CoV-2", "abstract": "No abstract available Keywords: ACE2; Gastrointestinal Infection; Oral-Fecal Transmission; SARS-CoV-2.", "date": "2020", "authors": ["Fei Xiao , Meiwen Tang , Xiaobin Zheng , Ye Liu , Xiaofeng Li , Hong Shan"], "references": [3001118548, 3003668884, 3004280078, 3003465021, 3010441732, 3005272159, 2131988685, 3031532178, 1974901207, 3030605366]}, {"id": 3011863580, "title": "Prolonged presence of SARS-CoV-2 viral RNA in faecal samples.", "abstract": "", "date": "2020", "authors": ["Yongjian Wu 1, Cheng Guo 2, Lantian Tang 1, Zhongsi Hong 1, Jianhui Zhou 1, Xin Dong 1, Huan Yin 1, Qiang Xiao 1, Yanping Tang 1, Xiujuan Qu 1, Liangjian Kuang 1, Xiaomin Fang 1, Nischay Mishra 2, Jiahai Lu 1, Hong Shan 1, Guanmin Jiang 1, Xi Huang 1"], "references": [3008696669, 3002533507, 3006846061, 3008352032]}, {"id": 3006846061, "title": "Enteric involvement of coronaviruses: is faecal-oral transmission of SARS-CoV-2 possible?", "abstract": "", "date": "2020", "authors": ["Charleen Yeo , Sanghvi Kaushal , Danson Yeo"], "references": [3005079553, 3002108456, 3003465021, 3004348779, 2006434809, 3003464757, 2769543984, 2144410942, 1984335993, 2064850047]}, {"id": 3010096538, "title": "Features, Evaluation and Treatment Coronavirus (COVID-19)", "abstract": "According to the World Health Organization (WHO), viral diseases continue to emerge and represent a serious issue to public health In the last twenty years, several viral epidemics such as the severe acute respiratory syndrome coronavirus (SARS-CoV) in 2002 to 2003, and H1N1 influenza in 2009, have been recorded Most recently, the Middle East respiratory syndrome coronavirus (MERS-CoV) was first identified in Saudi Arabia in 2012 In a timeline that reaches the present day, an epidemic of cases with unexplained low respiratory infections detected in Wuhan, the largest metropolitan area in China's Hubei province, was first reported to the WHO Country Office in China, on December 31, 2019 Published literature can trace the beginning of symptomatic individuals back to the beginning of December 2019 As they were unable to identify the causative agent, these first cases were classified as \"pneumonia of unknown etiology \" The Chinese Center for Disease Control and Prevention (CDC) and local CDCs organized an intensive outbreak investigation program The etiology of this illness is now attributed to a novel virus belonging to the coronavirus (CoV) family, COVID-19 On February 11, 2020, the WHO Director-General, Dr Tedros Adhanom Ghebreyesus, announced that the disease caused by this new CoV was a \"COVID-19,\" which is the acronym of \"coronavirus disease 2019\" In the past twenty years, two additional coronavirus epidemics have occurred  SARS-CoV provoked a large-scale epidemic beginning in China and involving two dozen countries with approximately 8000 cases and 800 deaths, and the MERS-CoV that began in Saudi Arabia and has approximately 2,500 cases and 800 deaths and still causes as sporadic cases This new virus seems to be very contagious and has quickly spread globally In a meeting on January 30, 2020, per the International Health Regulations (IHR, 2005), the outbreak was declared by the WHO a Public Health Emergency of International Concern (PHEIC) as it had spread to 18 countries with four countries reporting human-to-human transmission An additional landmark occurred on February 26, 2020, as the first case of the disease, not imported from China, was recorded in the United States Initially, the new virus was called 2019-nCoV Subsequently, the task of experts of the International Committee on Taxonomy of Viruses (ICTV) termed it the SARS-CoV-2 virus as it is very similar to the one that caused the SARS outbreak (SARS-CoVs) The CoVs have become the major pathogens of emerging respiratory disease outbreaks They are a large family of single-stranded RNA viruses (+ssRNA) that can be isolated in different animal species  For reasons yet to be explained, these viruses can cross species barriers and can cause, in humans, illness ranging from the common cold to more severe diseases such as MERS and SARS  Interestingly, these latter viruses have probably originated from bats and then moving into other mammalian hosts \u2014 the Himalayan palm civet for SARS-CoV, and the dromedary camel for MERS-CoV \u2014 before jumping to humans The dynamics of SARS-Cov-2 are currently unknown, but there is speculation that it also has an animal origin The potential for these viruses to grow to become a pandemic worldwide seems to be a serious public health risk Concerning COVID-19, the WHO raised the threat to the CoV epidemic to the \"very high\" level, on February 28, 2020 Probably, the effects of the epidemic caused by the new CoV has yet to emerge as the situation is quickly evolving World governments are at work to establish countermeasures to stem possible devastating effects Health organizations coordinate information flows and issues directives and guidelines to best mitigate the impact of the threat At the same time, scientists around the world work tirelessly, and information about the transmission mechanisms, the clinical spectrum of disease, new diagnostics, and prevention and therapeutic strategies are rapidly developing Many uncertainties remain with regard to both the virus-host interac ion and the evolution of the epidemic, with specific reference to the times when the epidemic will reach its peak At the moment, the therapeutic strategies to deal with the infection are only supportive, and prevention aimed at reducing transmission in the community is our best weapon Aggressive isolation measures in China have led to a progressive reduction of cases in the last few days In Italy, in geographic regions of the north of the peninsula, political and health authorities are making incredible efforts to contain a shock wave that is severely testing the health system In the midst of the crisis, the authors have chosen to use the \"Statpearls\" platform because, within the PubMed scenario, it represents a unique tool that may allow them to make updates in real-time  The aim, therefore, is to collect information and scientific evidence and to provide an overview of the topic that will be continuously updated", "date": "2020", "authors": ["Marco Cascella , Michael Rajnik , Arturo Cuomo , Scott C. Dulebohn , Raffaela Di Napoli"], "references": [3010377921, 3034963396, 3022406948, 3035275617, 3030700284, 3080981445, 3016426103, 3014941429, 3016955027, 3020956169]}, {"id": 3008443627, "title": "An interactive web-based dashboard to track COVID-19 in real time.", "abstract": "", "date": "2020", "authors": ["Ensheng Dong , Hongru Du , Lauren Gardner"], "references": [2406220407]}, {"id": 3013967887, "title": "Estimates of the severity of coronavirus disease 2019: a model-based analysis.", "abstract": "Background In the face of rapidly changing data, a range of case fatality ratio estimates for coronavirus disease 2019 (COVID-19) have been produced that differ substantially in magnitude. We aimed to provide robust estimates, accounting for censoring and ascertainment biases. Methods We collected individual-case data for patients who died from COVID-19 in Hubei, mainland China (reported by national and provincial health commissions to Feb 8, 2020), and for cases outside of mainland China (from government or ministry of health websites and media reports for 37 countries, as well as Hong Kong and Macau, until Feb 25, 2020). These individual-case data were used to estimate the time between onset of symptoms and outcome (death or discharge from hospital). We next obtained age-stratified estimates of the case fatality ratio by relating the aggregate distribution of cases to the observed cumulative deaths in China, assuming a constant attack rate by age and adjusting for demography and age-based and location-based under-ascertainment. We also estimated the case fatality ratio from individual line-list data on 1334 cases identified outside of mainland China. Using data on the prevalence of PCR-confirmed cases in international residents repatriated from China, we obtained age-stratified estimates of the infection fatality ratio. Furthermore, data on age-stratified severity in a subset of 3665 cases from China were used to estimate the proportion of infected individuals who are likely to require hospitalisation. Findings Using data on 24 deaths that occurred in mainland China and 165 recoveries outside of China, we estimated the mean duration from onset of symptoms to death to be 17\u00b78 days (95% credible interval [CrI] 16\u00b79-19\u00b72) and to hospital discharge to be 24\u00b77 days (22\u00b79-28\u00b71). In all laboratory confirmed and clinically diagnosed cases from mainland China (n=70 117), we estimated a crude case fatality ratio (adjusted for censoring) of 3\u00b767% (95% CrI 3\u00b756-3\u00b780). However, after further adjusting for demography and under-ascertainment, we obtained a best estimate of the case fatality ratio in China of 1\u00b738% (1\u00b723-1\u00b753), with substantially higher ratios in older age groups (0\u00b732% [0\u00b727-0\u00b738] in those aged Interpretation These early estimates give an indication of the fatality ratio across the spectrum of COVID-19 disease and show a strong age gradient in risk of death. Funding UK Medical Research Council.", "date": "2020", "authors": ["Robert Verity 1, Lucy C Okell 1, Ilaria Dorigatti 1, Peter Winskill 1, Charles Whittaker 1, Natsuko Imai 1, Gina Cuomo-Dannenburg 1, Hayley Thompson 1, Patrick G T Walker 1, Han Fu 1, Amy Dighe 1, Jamie T Griffin 2, Marc Baguelin 1, Sangeeta Bhatia 1, Adhiratha Boonyasiri 1, Anne Cori 1, Zulma Cucunub\u00e1 1, Rich FitzJohn 1, Katy Gaythorpe 1, Will Green 1, Arran Hamlet 1, Wes Hinsley 1, Daniel Laydon 1, Gemma Nedjati-Gilani 1, Steven Riley 1, Sabine van Elsland 1, Erik Volz 1, Haowei Wang 1, Yuanrong Wang 1, Xiaoyue Xi 1, Christl A Donnelly 1, 3, Azra C Ghani 1, Neil M Ferguson 1"], "references": [3001118548, 3008827533, 3003668884, 3002108456, 3009885589, 3002539152, 3008818676, 3007189521, 3020184843, 3001971765]}, {"id": 3015571324, "title": "Temporal dynamics in viral shedding and transmissibility of COVID-19.", "abstract": "We report temporal patterns of viral shedding in 94 patients with laboratory-confirmed COVID-19 and modeled COVID-19 infectiousness profiles from a separate sample of 77 infector-infectee transmission pairs. We observed the highest viral load in throat swabs at the time of symptom onset, and inferred that infectiousness peaked on or before symptom onset. We estimated that 44% (95% confidence interval, 25-69%) of secondary cases were infected during the index cases' presymptomatic stage, in settings with substantial household clustering, active case finding and quarantine outside the home. Disease control measures should be adjusted to account for probable substantial presymptomatic transmission.", "date": "2020", "authors": ["Xi He 1, Eric H Y Lau 2, Peng Wu 2, Xilong Deng 1, Jian Wang 1, Xinxin Hao 2, Yiu Chung Lau 2, Jessica Y Wong 2, Yujuan Guan 1, Xinghua Tan 1, Xiaoneng Mo 1, Yanqing Chen 1, Baolin Liao 1, Weilie Chen 1, Fengyu Hu 1, Qing Zhang 1, Mingqiu Zhong 1, Yanrong Wu 1, Lingzhai Zhao 1, Fuchun Zhang 1, Benjamin J Cowling 2, Fang Li 1, Gabriel M Leung 2"], "references": [3003668884, 3009885589, 3006961006, 3003573988, 3008696669, 3013893137, 3012756997, 3008294222, 2129542667, 3009983851]}, {"id": 3006642361, "title": "The reproductive number of COVID-19 is higher compared to SARS coronavirus.", "abstract": "Teaser: Our review found the average R0 for 2019-nCoV to be 3.28, which exceeds WHO estimates of 1.4 to 2.5.", "date": "2020", "authors": ["Ying Liu 1, Albert A Gayle 2, Annelies Wilder-Smith 2, Joacim Rockl\u00f6v 2"], "references": [3003668884, 3003573988, 3004397688, 3002764620, 3004026249, 3002533591, 3002747665, 3001343166, 3033251833, 3001392146]}, {"id": 3012789146, "title": "The effect of control strategies to reduce social mixing on outcomes of the COVID-19 epidemic in Wuhan, China: a modelling study.", "abstract": "BACKGROUND: In December, 2019, severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), a novel coronavirus, emerged in Wuhan, China. Since then, the city of Wuhan has taken unprecedented measures in response to the outbreak, including extended school and workplace closures. We aimed to estimate the effects of physical distancing measures on the progression of the COVID-19 epidemic, hoping to provide some insights for the rest of the world. METHODS: To examine how changes in population mixing have affected outbreak progression in Wuhan, we used synthetic location-specific contact patterns in Wuhan and adapted these in the presence of school closures, extended workplace closures, and a reduction in mixing in the general community. Using these matrices and the latest estimates of the epidemiological parameters of the Wuhan outbreak, we simulated the ongoing trajectory of an outbreak in Wuhan using an age-structured susceptible-exposed-infected-removed (SEIR) model for several physical distancing measures. We fitted the latest estimates of epidemic parameters from a transmission model to data on local and internationally exported cases from Wuhan in an age-structured epidemic framework and investigated the age distribution of cases. We also simulated lifting of the control measures by allowing people to return to work in a phased-in way and looked at the effects of returning to work at different stages of the underlying outbreak (at the beginning of March or April). FINDINGS: Our projections show that physical distancing measures were most effective if the staggered return to work was at the beginning of April; this reduced the median number of infections by more than 92% (IQR 66-97) and 24% (13-90) in mid-2020 and end-2020, respectively. There are benefits to sustaining these measures until April in terms of delaying and reducing the height of the peak, median epidemic size at end-2020, and affording health-care systems more time to expand and respond. However, the modelled effects of physical distancing measures vary by the duration of infectiousness and the role school children have in the epidemic. INTERPRETATION: Restrictions on activities in Wuhan, if maintained until April, would probably help to delay the epidemic peak. Our projections suggest that premature and sudden lifting of interventions could lead to an earlier secondary peak, which could be flattened by relaxing the interventions gradually. However, there are limitations to our analysis, including large uncertainties around estimates of R0 and the duration of infectiousness. FUNDING: Bill & Melinda Gates Foundation, National Institute for Health Research, Wellcome Trust, and Health Data Research UK.", "date": "2020", "authors": ["Kiesha Prem , Yang Liu , Timothy W Russell , Adam J Kucharski , Rosalind M Eggo , Nicholas Davies , Mark Jit , Petra Klepac"], "references": [3001897055, 3003668884, 3002539152, 3003573988, 3006659024, 3009577418, 3009468976, 3004912618, 3004026249, 3020184843]}, {"id": 3013594674, "title": "The effect of human mobility and control measures on the COVID-19 epidemic in China.", "abstract": "The ongoing coronavirus disease 2019 (COVID-19) outbreak expanded rapidly throughout China. Major behavioral, clinical, and state interventions were undertaken to mitigate the epidemic and prevent the persistence of the virus in human populations in China and worldwide. It remains unclear how these unprecedented interventions, including travel restrictions, affected COVID-19 spread in China. We used real-time mobility data from Wuhan and detailed case data including travel history to elucidate the role of case importation in transmission in cities across China and to ascertain the impact of control measures. Early on, the spatial distribution of COVID-19 cases in China was explained well by human mobility data. After the implementation of control measures, this correlation dropped and growth rates became negative in most locations, although shifts in the demographics of reported cases were still indicative of local chains of transmission outside of Wuhan. This study shows that the drastic control measures implemented in China substantially mitigated the spread of COVID-19.", "date": "2020", "authors": ["Moritz U.G. Kraemer 1, 2, Chia Hung Yang 3, Bernardo Gutierrez 1, Chieh Hsi Wu 4, Brennan Klein 3, David M. Pigott 5, Louis du Plessis 1, Nuno R. Faria 1, Ruoran Li 2, William P. Hanage 2, John S. Brownstein 2, Maylis Layan 6, Alessandro Vespignani 3, Huaiyu Tian 7, Christopher Dye 1, Oliver G. Pybus 1, Samuel V. Scarpino 3"], "references": [3001897055, 3003668884, 3008028633, 1951724000, 3003573988, 3008818676, 2097360283, 3012284084, 2122825543, 3004912618]}, {"id": 3001195213, "title": "Detection of 2019 novel coronavirus (2019-nCoV) by real-time RT-PCR.", "abstract": "Background The ongoing outbreak of the recently emerged novel coronavirus (2019-nCoV) poses a challenge for public health laboratories as virus isolates are unavailable while there is growing evidence that the outbreak is more widespread than initially thought, and international spread through travellers does already occur. Aim We aimed to develop and deploy robust diagnostic methodology for use in public health laboratory settings without having virus material available. Methods Here we present a validated diagnostic workflow for 2019-nCoV, its design relying on close genetic relatedness of 2019-nCoV with SARS coronavirus, making use of synthetic nucleic acid technology. Results The workflow reliably detects 2019-nCoV, and further discriminates 2019-nCoV from SARS-CoV. Through coordination between academic and public laboratories, we confirmed assay exclusivity based on 297 original clinical specimens containing a full spectrum of human respiratory viruses. Control material is made available through European Virus Archive \u2013 Global (EVAg), a European Union infrastructure project. Conclusion The present study demonstrates the enormous response capacity achieved through coordination of academic and public laboratories in national and European research networks.", "date": "2020", "authors": ["Victor M. Corman 1, Olfert Landt 2, Marco Kaiser 3, Richard Molenkamp 4, Adam Meijer 5, Daniel K.W. Chu 6, Tobias Bleicker 1, Sebastian Br\u00fcnink 1, Julia Schneider 1, Marie Luisa Schmidt 1, Daphne G.J.C. Mulders 4, Bart L. Haagmans 4, Bas Van Der Veer 5, Sharon Van Den Brink 5, Lisa Wijsman 5, Gabriel Goderski 5, Jean Louis Romette 7, Joanna Ellis 8, Maria Zambon 8, Malik Peiris 6, Herman Goossens 9, Chantal Reusken 5, Marion P.G. Koopmans 4, Christian Drosten 1"], "references": [2903899730, 2132260239, 1703839189, 1852588318, 2793008036, 2167080692, 2894950287, 2101063972, 2031705962, 2884280018]}, {"id": 2105275554, "title": "Loop-mediated isothermal amplification of DNA", "abstract": "We have developed a novel method, termed loop-mediated isothermal amplification (LAMP), that amplifies DNA with high specificity, efficiency and rapidity under isothermal conditions. This method employs a DNA polymerase and a set of four specially designed primers that recognize a total of six distinct sequences on the target DNA. An inner primer containing sequences of the sense and antisense strands of the target DNA initiates LAMP. The following strand displacement DNA synthesis primed by an outer primer releases a single-stranded DNA. This serves as template for DNA synthesis primed by the second inner and outer primers that hybridize to the other end of the target, which produces a stem\u2013loop DNA structure. In subsequent LAMP cycling one inner primer hybridizes to the loop on the product and initiates displacement DNA synthesis, yielding the original stem\u2013loop DNA and a new stem\u2013loop DNA with a stem twice as long. The cycling reaction continues with accumulation of 109 copies of target in less than an hour. The final products are stem\u2013loop DNAs with several inverted repeats of the target and cauliflower-like structures with multiple loops formed by annealing between alternately inverted repeats of the target in the same strand. Because LAMP recognizes the target by six distinct sequences initially and by four distinct sequences afterwards, it is expected to amplify the target sequence with high selectivity.", "date": "2000", "authors": ["Tsugunori Notomi , Hiroto Okayama , Harumi Masubuchi , Toshihiro Yonekawa , Keiko Watanabe , Nobuyuki Amino , Tetsu Hase"], "references": [2032118018, 2050717506, 2035792726, 1990689151, 2062756489, 2142539585, 2083121396, 2082277951, 1970137322, 2056636525]}, {"id": 3011969828, "title": "2019 Novel coronavirus disease (COVID-19): Paving the road for rapid detection and point-of-care diagnostics", "abstract": "We believe a point-of-care (PoC) device for the rapid detection of the 2019 novel Coronavirus (SARS-CoV-2) is crucial and urgently needed. With this perspective, we give suggestions regarding a potential candidate for the rapid detection of the coronavirus disease 2019 (COVID-19), as well as factors for the preparedness and response to the outbreak of the COVID-19.", "date": "2020", "authors": ["Trieu Nguyen , Dang Duong Bang , Anders Wolff"], "references": [3001118548, 3004280078, 3001195213, 3003465021, 3004239190, 3003573988, 3001465255, 3003951199, 3003637715, 3000771439]}, {"id": 2770752141, "title": "Loop-mediated isothermal amplification (LAMP): a versatile technique for detection of micro-organisms.", "abstract": "Summary Loop-mediated isothermal amplification (LAMP) amplifies DNA with high specificity, efficiency and rapidity under isothermal conditions by using a DNA polymerase with high displacement strand activity and a set of specifically designed primers to amplify targeted DNA strands. Following its first discovery by Notomi et al. (2000 Nucleic Acids Res 28: E63), LAMP was further developed over the years which involved the combination of this technique with other molecular approaches, such as reverse transcription and multiplex amplification for the detection of infectious diseases caused by micro-organisms in humans, livestock and plants. In this review, available types of LAMP techniques will be discussed together with their applications in detection of various micro-organisms. Up to date, there are varieties of LAMP detection methods available including colorimetric and fluorescent detection, real-time monitoring using turbidity metre and detection using lateral flow device which will also be highlighted in this review. Apart from that, commercialization of LAMP technique had also been reported such as lyophilized form of LAMP reagents kit and LAMP primer sets for detection of pathogenic micro-organisms. On top of that, advantages and limitations of this molecular detection method are also described together with its future potential as a diagnostic method for infectious disease.", "date": "2018", "authors": ["Y.\u2010P. Wong 1, S. Othman 1, Y.\u2010L. Lau 2, S. Radu 1, H.\u2010Y. Chee 1"], "references": [2105275554, 1979974453, 1975801865, 1562219715, 1997925861, 2063533401, 89741002, 2050515639, 2141633648, 2035792726]}, {"id": 2263084061, "title": "Loop-Mediated Isothermal Amplification Assay for Identification of Five Human Plasmodium Species in Malaysia", "abstract": "The lack of rapid, affordable, and accurate diagnostic tests represents the primary hurdle affecting malaria surveillance in resource- and expertise-limited areas. Loop-mediated isothermal amplification (LAMP) is a sensitive, rapid, and cheap diagnostic method. Five species-specific LAMP assays were developed based on 18S rRNA gene. Sensitivity and specificity of LAMP results were calculated as compared with microscopic examination and nested polymerase chain reaction. LAMP reactions were highly sensitive with the detection limit of one copy for Plasmodium vivax, Plasmodium falciparum, and Plasmodium malariae and 10 copies for Plasmodium knowlesi and Plasmodium ovale. LAMP positively detected all human malaria species in all positive samples (N = 134; sensitivity = 100%) within 35 minutes. All negative samples were not amplified by LAMP (N = 67; specificity = 100%). LAMP successfully detected two samples with very low parasitemia. LAMP may offer a rapid, simple, and reliable test for the diagnosis of malaria in areas where malaria is prevalent.", "date": "2016", "authors": ["Yee Ling Lau , Meng Yee Lai , Mun Yik Fong , Jenarun Jelip , Rohela Mahmud"], "references": [2105275554, 2788073857, 2133724824, 2066385972, 2149136689, 2102131955, 2043762150, 1987713777, 2110753623, 2144345536]}, {"id": 2175815746, "title": "Development of reverse-transcription loop-mediated isothermal amplification assay for rapid detection and differentiation of dengue virus serotypes 1-4.", "abstract": "Dengue virus (DENV), the most widely prevalent arbovirus, continues to be a threat to human health in the tropics and subtropics. Early and rapid detection of DENV infection during the acute phase of illness is crucial for proper clinical patient management and preventing the spread of infection. The aim of the current study was to develop a specific, sensitive, and robust reverse transcriptase loop-mediated isothermal amplification (RT-LAMP) assay for detection and differentiation of DENV1-4 serotypes. The method detection primers, which were designed to target the different DENV serotypes, were identified by inspection of multiple sequence alignments of the non-structural protein (NS) 2A of DENV1, NS4B of DENV2, NS4A of DENV3 and the 3\u2032 untranslated region of the NS protein of DENV4. No cross-reactions of the four serotypes were observed during the tests. The detection limits of the DENV1-4-specific RT-LAMP assays were approximately 10-copy templates per reaction. The RT-LAMP assays were ten-fold more sensitive than RT-PCR or real-time PCR. The diagnostic rate was 100 % for clinical strains of DENV, and 98.9 % of the DENV-infected patients whose samples were tested were detected by RT-LAMP. Importantly, no false-positives were detected with the new equipment and methodology that was used to avoid aerosol contamination of the samples. The RT-LAMP method used in our study is specific, sensitive, and suitable for further investigation as a useful alternative to the current methods used for clinical diagnosis of DENV1-4, especially in hospitals and laboratories that lack sophisticated diagnostic systems.", "date": "2015", "authors": ["Sheng-feng Hu 1, Miao Li 1, Lan-lan Zhong 1, Shi-miao Lu 1, Ze-xia Liu 1, Jie-ying Pu 1, Jin-sheng Wen 2, Xi Huang 1, 2"], "references": [2105275554, 2042479028, 2066385972, 2128110156, 2149136689, 2160892791, 2101258647, 1994850469, 2124449928, 2141627564]}, {"id": 1991420168, "title": "Utility of IgM ELISA, TaqMan real-time PCR, reverse transcription PCR, and RT-LAMP assay for the diagnosis of Chikungunya fever.", "abstract": "Chikungunya fever a re-emerging infection with expanding geographical boundaries, can mimic symptoms of other infections like dengue, malaria which makes the definitive diagnosis of the infection important. The present study compares the utility of four laboratory diagnostic methods viz. IgM capture ELISA, an in house reverse transcription PCR for the diagnosis of Chikungunya fever, TaqMan real-time PCR, and a one step reverse transcription-loop mediated isothermal amplification assay (RT-LAMP). Out of the 70 serum samples tested, 29 (41%) were positive for Chikungunya IgM antibody by ELISA and 50 (71%) samples were positive by one of the three molecular assays. CHIKV specific nucleic acid was detected in 33/70 (47%) by reverse transcription PCR, 46/70 (66%) by TaqMan real-time PCR, and 43/70 (62%) by RT-LAMP assay. A majority of the samples (62/70; 89%) were positive by at least one of the four assays used in the study. The molecular assays were more sensitive for diagnosis in the early stages of illness (2\u20135 days post onset) when antibodies were not detectable. In the later stages of illness, the IgM ELISA is a more sensitive diagnostic test. In conclusion we recommend that the IgM ELISA be used as an initial screening test followed one of the molecular assays in samples that are collected in the early phase of illness and negative for CHIKV IgM antibodies. Such as approach would enable rapid confirmation of the diagnosis and implementation of public health measures especially during outbreaks. J. Med. Virol. 84:1771\u20131778, 2012. \u00a9 2012 Wiley Periodicals, Inc.", "date": "2012", "authors": ["Vijayalakshmi Reddy 1, Vasanthapuram Ravi 1, Anita Desai 1, Manmohan Parida 2, Ann M. Powers 3, Barbara W. Johnson 3"], "references": [2141987735, 2120801593, 1977296748, 2129358311, 1969557290, 2067506266, 2052129607, 2108924397, 1966636515, 2051304065]}, {"id": 2207764089, "title": "Colorimetric Detection of Dengue by Single Tube Reverse-Transcription-Loop-Mediated Isothermal Amplification.", "abstract": "Dengue is usually diagnosed by isolation of the virus, serology or molecular diagnostic methods. Several commercial kits for the diagnosis of dengue are existing, but concerns have arisen regarding to the affordability and performance characteristics of these kits. Hence, the loop-mediated isothermal amplification (LAMP) is potentially ideal to be used especially in resource limited environments. Serum was collected from healthy donors and patients diagnosed with dengue infection. RNA extracted from the serum samples were tested by reverse-transcription-LAMP assay developed based on 3\u2032-NCR gene sequences for DENV 1\u20134. Results were interpreted by a turbidity meter in real time or visually at the end of the assay. Sensitivity and specificity of RT-LAMP results were calculated and compared to qRT-PCR and ELISA. RT-LAMP is highly sensitive with the detection limit of 10 RNA copies for all serotypes. Dengue virus RNA was detected in all positive samples using RT-LAMP and none of the negative samples within 30\u201345 minutes. With continuing efforts in the optimization of this assay, RT-LAMP may provide a simple and reliable test for detecting DENV in areas where dengue is prevalent.", "date": "2015", "authors": ["Yee-Ling Lau , Meng-Yee Lai , Boon-Teong Teoh , Juraina Abd-Jamil , Jefree Johari , Sing-Sin Sam , Kim-Kee Tan , Sazaly AbuBakar"], "references": [2105275554, 2149136689, 2019786863, 2160892791, 2115409419, 2029530793, 2038279115, 2069969977, 2096415393, 2009404254]}, {"id": 2073600962, "title": "Evaluation of a Direct Reverse Transcription Loop-Mediated Isothermal Amplification Method without RNA Extraction for the Detection of Human Enterovirus 71 Subgenotype C4 in Nasopharyngeal Swab Specimens", "abstract": "Human enterovirus 71 (EV71) is the major causative agent of hand, foot, and mouth disease (HFMD) worldwide and has been associated with neurological complications which resulted in fatalities during recent outbreak in Asia pacific region. A direct reverse transcription loop-mediated isothermal amplification (direct RT-LAMP) assay using heat-treated samples without RNA extraction was developed and evaluated for the detection of EV71 subgenotype C4 in nasopharyngeal swab specimens. The analytical sensitivity and specificity of the direct RT-LAMP assay were examined. The detection limit of the direct RT-LAMP assays was 1.6 of a 50% tissue culture infective dose (TCID50) per reaction and no cross-reaction was observed with control viruses including Cosackievirus A (CVA) viruses (CVA2,4,5,7,9,10,14,16, and 24), Coxsackievirus B (CVB) viruses (CVB1,2,3,4, and 5) or ECHO viruses (ECHO3,6,11, and 19). The direct RT-LAMP assay was evaluated and compared to both RT-LAMP and quantitative real-time PCR (qRT-PCR) in detecting EV71 infection with 145 nasopharyngeal swab specimens. The clinical performance demonstrated the sensitivity and specificity of direct RT-LAMP was reported to be 90.3% and 100% respectively, compared to RT-LAMP, and 86.83% and 100% respectively, compared to qRT-PCR. These data demonstrated that the direct RT-LAMP assay can potentially be developed for the point of care screening of EV71 infection in China.", "date": "2012", "authors": ["Kai Nie 1, Shun-xiang Qi 2, Yong Zhang 1, Le Luo 1, Yun Xie 2, Meng-jie Yang 1, Yi Zhang 1, Jin Li 1, Hongwei Shen 1, Qi Li 2, Xue-jun Ma 1"], "references": [2105275554, 2135086664, 2160892791, 2057680658, 2140363623, 2065860560, 1975134712, 1970186243, 1984421786, 2051085144]}, {"id": 3010449299, "title": "Air, Surface Environmental, and Personal Protective Equipment Contamination by Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2) From a Symptomatic Patient.", "abstract": "This study documents results of SARS-CoV-2 polymerase chain reaction (PCR) testing of environmental surfaces and personal protective equipment surrounding 3 COVID-19 patients in isolation rooms in a Singapore hospital.", "date": "2020", "authors": ["Sean Wei Xiang Ong 1, Yian Kim Tan 2, Po Ying Chia 1, Tau Hong Lee 1, Oon Tek Ng 1, Michelle Su Yen Wong 2, Kalisvar Marimuthu 1"], "references": [3005079553, 3001195213, 3010338568, 1815575713, 2250074178]}, {"id": 1981646999, "title": "Virus taxonomy : eighth report of the International Committee on Taxonomy of Viruses", "abstract": "", "date": "2004", "authors": ["Claude Fauquet , Mayo , J. Maniloff , U. Desselberger , L.A. Ball"], "references": [3005111420, 2102286438, 2113457186, 2105637133, 2516022550, 2142678031, 2036641307, 2072299899, 1552724685, 2067508599]}, {"id": 3018334611, "title": "Aerodynamic analysis of SARS-CoV-2 in two Wuhan hospitals.", "abstract": "The ongoing outbreak of coronavirus disease 2019 (COVID-19) has spread rapidly on a global scale. Although it is clear that severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) is transmitted through human respiratory droplets and direct contact, the potential for aerosol transmission is poorly understood1-3. Here we investigated the aerodynamic nature of SARS-CoV-2 by measuring viral RNA in aerosols in different areas of two Wuhan hospitals during the outbreak of COVID-19 in February and March 2020. The concentration of SARS-CoV-2 RNA in aerosols that was detected in isolation wards and ventilated patient rooms was very low, but it was higher in the toilet areas used by the patients. Levels of airborne SARS-CoV-2 RNA in the most public areas was undetectable, except in two areas that were prone to crowding; this increase was possibly due to individuals infected with SARS-CoV-2 in the crowd. We found that some medical staff areas initially had high concentrations of viral RNA with aerosol size distributions that showed peaks in the submicrometre and/or supermicrometre regions; however, these levels were reduced to undetectable levels after implementation of rigorous sanitization procedures. Although we have not established the infectivity of the virus detected in these hospital areas, we propose that SARS-CoV-2 may have the potential to be transmitted through aerosols. Our results indicate that room ventilation, open space, sanitization of protective apparel, and proper use and disinfection of toilet areas can effectively limit the concentration of SARS-CoV-2 RNA in aerosols. Future work should explore the infectivity of aerosolized virus.", "date": "2020", "authors": ["Yuan Liu 1, Zhi Ning 2, Yu Chen 1, Ming Guo 1, Yingle Liu 1, Nirmal Kumar Gali 2, Li Sun 2, Yusen Duan 3, Jing Cai 4, Dane Westerdahl 2, Xinjin Liu 1, Ke Xu 1, Kin fai Ho 5, Haidong Kan 4, Qingyan Fu 3, Ke Lan 1"], "references": [3004280078, 3012099172, 3010604545, 3009906937, 3010449299, 3005510968, 3027866910, 3018724240, 2215636165, 2147350479]}, {"id": 3015704123, "title": "Aerosol and Surface Distribution of Severe Acute Respiratory Syndrome Coronavirus 2 in Hospital Wards, Wuhan, China, 2020.", "abstract": "To determine distribution of severe acute respiratory syndrome coronavirus 2 in hospital wards in Wuhan, China, we tested air and surface samples. Contamination was greater in intensive care units than general wards. Virus was widely distributed on floors, computer mice, trash cans, and sickbed handrails and was detected in air \u22484 m from patients.", "date": "2020", "authors": ["Zhen Dong Guo 1, Zhong Yi Wang 1, Shou Feng Zhang 1, Xiao Li 1, Lin Li 1, Chao Li 2, Yan Cui 3, Rui Bin Fu 3, Yun Zhu Dong 1, Xiang Yang Chi 1, Meng Yao Zhang 1, Kun Liu 3, Cheng Cao 1, Bin Liu 1, Ke Zhang 1, Yu Wei Gao 2, Bing Lu 3, Wei Chen 4, 5"], "references": [3011242477, 3010223921, 3010449299, 3010149441, 3008429297, 3010633777, 2614711400]}, {"id": 2158121945, "title": "Guidelines for environmental infection control in health-care facilities. Recommendations of CDC and the Healthcare Infection Control Practices Advisory Committee (HICPAC).", "abstract": "The health-care facility environment is rarely implicated in disease transmission, except among patients who are immunocompromised. Nonetheless, inadvertent exposures to environmental pathogens (e.g., Aspergillus spp. and Legionella spp.) or airborne pathogens (e.g., Mycobacterium tuberculosis and varicella-zoster virus) can result in adverse patient outcomes and cause illness among health-care workers. Environmental infection-control strategies and engineering controls can effectively prevent these infections. The incidence of health-care--associated infections and pseudo-outbreaks can be minimized by 1) appropriate use of cleaners and disinfectants; 2) appropriate maintenance of medical equipment (e.g., automated endoscope reprocessors or hydrotherapy equipment); 3) adherence to water-quality standards for hemodialysis, and to ventilation standards for specialized care environments (e.g., airborne infection isolation rooms, protective environments, or operating rooms); and 4) prompt management of water intrusion into the facility. Routine environmental sampling is not usually advised, except for water quality determinations in hemodialysis settings and other situations where sampling is directed by epidemiologic principles, and results can be applied directly to infection-control decisions. This report reviews previous guidelines and strategies for preventing environment-associated infections in health-care facilities and offers recommendations. These include 1) evidence-based recommendations supported by studies; 2) requirements of federal agencies (e.g., Food and Drug Administration, U.S. Environmental Protection Agency, U.S. Department of Labor, Occupational Safety and Health Administration, and U.S. Department of Justice); 3) guidelines and standards from building and equipment professional organizations (e.g., American Institute of Architects, Association for the Advancement of Medical Instrumentation, and American Society of Heating, Refrigeration, and Air-Conditioning Engineers); 4) recommendations derived from scientific theory or rationale; and 5) experienced opinions based upon infection-control and engineering practices. The report also suggests a series of performance measurements as a means to evaluate infection-control efforts.", "date": "2003", "authors": ["Lynne Sehulster"], "references": [1856219842, 1833207062, 2093487930, 2121941820, 2905912541, 1536339477, 2465608195, 2153911335, 2315217144, 1589603082]}, {"id": 3030968929, "title": "Detection of air and surface contamination by SARS-CoV-2 in hospital rooms of infected patients.", "abstract": "Understanding the particle size distribution in the air and patterns of environmental contamination of SARS-CoV-2 is essential for infection prevention policies. Here we screen surface and air samples from hospital rooms of COVID-19 patients for SARS-CoV-2 RNA. Environmental sampling is conducted in three airborne infection isolation rooms (AIIRs) in the ICU and 27 AIIRs in the general ward. 245 surface samples are collected. 56.7% of rooms have at least one environmental surface contaminated. High touch surface contamination is shown in ten (66.7%) out of 15 patients in the first week of illness, and three (20%) beyond the first week of illness (p = 0.01, \u03c72 test). Air sampling is performed in three of the 27 AIIRs in the general ward, and detects SARS-CoV-2 PCR-positive particles of sizes >4 \u00b5m and 1-4 \u00b5m in two rooms, despite these rooms having 12 air changes per hour. This warrants further study of the airborne transmission potential of SARS-CoV-2.", "date": "2020", "authors": ["Po Ying Chia 1, 2, Kristen Kelli Coleman 3, Yian Kim Tan 4, Sean Wei Xiang Ong 2, Marcus Gum 4, Sok Kiang Lau 4, Xiao Fang Lim 4, Ai Sim Lim 4, Stephanie Sutjipto 2, Pei Hua Lee 2, Barnaby Edward Young 1, 2, Donald K Milton 5, Gregory C Gray 3, 6, Stephan Schuster 1, Timothy Barkham 2, 3, Partha Pratim De 1, 2, Shawn Vasoo 1, 2, Monica Chan 2, Brenda Sze Peng Ang 7, Boon Huan Tan 4, Yee-Sin Leo 7, Oon-Tek Ng 1, 2, Michelle Su Yen Wong 4, Kalisvar Marimuthu 2, 3"], "references": [3002539152, 3001195213, 3006961006, 3012099172, 3008696669, 3010604545, 3013893137, 3010338568, 2132260239, 3010449299]}, {"id": 2337456675, "title": "Extensive Viable Middle East Respiratory Syndrome (MERS) Coronavirus Contamination in Air and Surrounding Environment in MERS Isolation Wards", "abstract": "Background The largest outbreak of Middle East respiratory syndrome coronavirus (MERS-CoV) outside the Middle East occurred in South Korea in 2015 and resulted in 186 laboratory-confirmed infections, including 36 (19%) deaths. Some hospitals were considered epicenters of infection and voluntarily shut down most of their operations after nearly half of all transmissions occurred in hospital settings. However, the ways that MERS-CoV is transmitted in healthcare settings are not well defined. Methods We explored the possible contribution of contaminated hospital air and surfaces to MERS transmission by collecting air and swabbing environmental surfaces in 2 hospitals treating MERS-CoV patients. The samples were tested by viral culture with reverse transcription polymerase chain reaction (RT-PCR) and immunofluorescence assay (IFA) using MERS-CoV Spike antibody, and electron microscopy (EM). Results The presence of MERS-CoV was confirmed by RT-PCR of viral cultures of 4 of 7 air samples from 2 patients' rooms, 1 patient's restroom, and 1 common corridor. In addition, MERS-CoV was detected in 15 of 68 surface swabs by viral cultures. IFA on the cultures of the air and swab samples revealed the presence of MERS-CoV. EM images also revealed intact particles of MERS-CoV in viral cultures of the air and swab samples. Conclusions These data provide experimental evidence for extensive viable MERS-CoV contamination of the air and surrounding materials in MERS outbreak units. Thus, our findings call for epidemiologic investigation of the possible scenarios for contact and airborne transmission, and raise concern regarding the adequacy of current infection control procedures.", "date": "2016", "authors": ["Sung Han Kim 1, So Young Chang 2, Minki Sung 3, Ji Hoon Park 2, Hong Bin Kim 4, Heeyoung Lee 5, Jae Phil Choi 6, Won Suk Choi 7, Ji Young Min 2"], "references": [2115555188, 1993435091, 2250074178, 1984335993, 2895727403, 1985322325, 2134527559, 1947023024, 1979807576, 2116544382]}, {"id": 3008090866, "title": "Clinical course and outcomes of critically ill patients with SARS-CoV-2 pneumonia in Wuhan, China: a single-centered, retrospective, observational study.", "abstract": "Summary Background An ongoing outbreak of pneumonia associated with the severe acute respiratory coronavirus 2 (SARS-CoV-2) started in December, 2019, in Wuhan, China. Information about critically ill patients with SARS-CoV-2 infection is scarce. We aimed to describe the clinical course and outcomes of critically ill patients with SARS-CoV-2 pneumonia. Methods In this single-centered, retrospective, observational study, we enrolled 52 critically ill adult patients with SARS-CoV-2 pneumonia who were admitted to the intensive care unit (ICU) of Wuhan Jin Yin-tan hospital (Wuhan, China) between late December, 2019, and Jan 26, 2020. Demographic data, symptoms, laboratory values, comorbidities, treatments, and clinical outcomes were all collected. Data were compared between survivors and non-survivors. The primary outcome was 28-day mortality, as of Feb 9, 2020. Secondary outcomes included incidence of SARS-CoV-2-related acute respiratory distress syndrome (ARDS) and the proportion of patients requiring mechanical ventilation. Findings Of 710 patients with SARS-CoV-2 pneumonia, 52 critically ill adult patients were included. The mean age of the 52 patients was 59\u00b77 (SD 13\u00b73) years, 35 (67%) were men, 21 (40%) had chronic illness, 51 (98%) had fever. 32 (61\u00b75%) patients had died at 28 days, and the median duration from admission to the intensive care unit (ICU) to death was 7 (IQR 3\u201311) days for non-survivors. Compared with survivors, non-survivors were older (64\u00b76 years [11\u00b72] vs 51\u00b79 years [12\u00b79]), more likely to develop ARDS (26 [81%] patients vs 9 [45%] patients), and more likely to receive mechanical ventilation (30 [94%] patients vs 7 [35%] patients), either invasively or non-invasively. Most patients had organ function damage, including 35 (67%) with ARDS, 15 (29%) with acute kidney injury, 12 (23%) with cardiac injury, 15 (29%) with liver dysfunction, and one (2%) with pneumothorax. 37 (71%) patients required mechanical ventilation. Hospital-acquired infection occurred in seven (13\u00b75%) patients. Interpretation The mortality of critically ill patients with SARS-CoV-2 pneumonia is considerable. The survival time of the non-survivors is likely to be within 1\u20132 weeks after ICU admission. Older patients (>65 years) with comorbidities and ARDS are at increased risk of death. The severity of SARS-CoV-2 pneumonia poses great strain on critical care resources in hospitals, especially if they are not adequately staffed or resourced. Funding None.", "date": "2020", "authors": ["Xiaobo Yang 1, Yuan Yu 1, Jiqian Xu 1, Huaqing Shu 1, Jia'an Xia 2, Hong Liu 1, 2, Yongran Wu 1, Lu Zhang 3, Zhui Yu 4, Minghao Fang 1, Ting Yu 2, Yaxin Wang 1, Shangwen Pan 1, Xiaojing Zou 1, Shiying Yuan 1, You Shang 1, 2"], "references": [3001118548, 3001897055, 3005079553, 3002108456, 3003465021, 3004239190, 2470646526, 2026274122, 3005403371, 2286228001]}, {"id": 3011242477, "title": "COVID-19 and Italy: what next?", "abstract": "Summary The spread of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) has already taken on pandemic proportions, affecting over 100 countries in a matter of weeks. A global response to prepare health systems worldwide is imperative. Although containment measures in China have reduced new cases by more than 90%, this reduction is not the case elsewhere, and Italy has been particularly affected. There is now grave concern regarding the Italian national health system's capacity to effectively respond to the needs of patients who are infected and require intensive care for SARS-CoV-2 pneumonia. The percentage of patients in intensive care reported daily in Italy between March 1 and March 11, 2020, has consistently been between 9% and 11% of patients who are actively infected. The number of patients infected since Feb 21 in Italy closely follows an exponential trend. If this trend continues for 1 more week, there will be 30\u2008000 infected patients. Intensive care units will then be at maximum capacity; up to 4000 hospital beds will be needed by mid-April, 2020. Our analysis might help political leaders and health authorities to allocate enough resources, including personnel, beds, and intensive care facilities, to manage the situation in the next few days and weeks. If the Italian outbreak follows a similar trend as in Hubei province, China, the number of newly infected patients could start to decrease within 3\u20134 days, departing from the exponential trend. However, this cannot currently be predicted because of differences between social distancing measures and the capacity to quickly build dedicated facilities in China.", "date": "2020", "authors": ["Andrea Remuzzi 1, Giuseppe Remuzzi 2"], "references": [3003668884]}, {"id": 3008028633, "title": "Characteristics of and Important Lessons From the Coronavirus Disease 2019 (COVID-19) Outbreak in China: Summary of a Report of 72 314 Cases From the Chinese Center for Disease Control and Prevention", "abstract": "", "date": "2020", "authors": ["Zunyou Wu , Jennifer M. McGoogan"], "references": [3006533361, 3024919756, 3006044875, 3082757469, 3005409672, 3004434564, 3033263492, 3031410613, 3026023364, 3034094473]}, {"id": 3010930696, "title": "Hydroxychloroquine and azithromycin as a treatment of COVID-19: results of an open-label non-randomized clinical trial.", "abstract": "Abstract Background Chloroquine and hydroxychloroquine have been found to be efficient on SARS-CoV-2, and reported to be efficient in Chinese COV-19 patients. We evaluate the role of hydroxychloroquine on respiratory viral loads. Patients and methods French Confirmed COVID-19 patients were included in a single arm protocol from early March to March 16th, to receive 600mg of hydroxychloroquine daily and their viral load in nasopharyngeal swabs was tested daily in a hospital setting. Depending on their clinical presentation, azithromycin was added to the treatment. Untreated patients from another center and cases refusing the protocol were included as negative controls. Presence and absence of virus at Day6-post inclusion was considered the end point. Results Six patients were asymptomatic, 22 had upper respiratory tract infection symptoms and eight had lower respiratory tract infection symptoms. Twenty cases were treated in this study and showed a significant reduction of the viral carriage at D6-post inclusion compared to controls, and much lower average carrying duration than reported of untreated patients in the literature. Azithromycin added to hydroxychloroquine was significantly more efficient for virus elimination. Conclusion Despite its small sample size our survey shows that hydroxychloroquine treatment is significantly associated with viral load reduction/disappearance in COVID-19 patients and its effect is reinforced by azithromycin.", "date": "2020", "authors": ["Philippe Gautret 1, Jean-Christophe Lagier 1, Philippe Parola 1, Van Thuan Hoang 1, Line Meddeb 2, Morgane Mailhe 2, Barbara Doudier 2, Johan Courjon 3, Val\u00e9rie Giordanengo 4, Vera Esteves Vieira 2, Herv\u00e9 Tissot Dupont 1, St\u00e9phane Honor\u00e9 1, Philippe Colson 1, Eric Chabri\u00e8re 1, Bernard La Scola 1, Jean-Marc Rolain 1, Philippe Brouqui 1, Didier Raoult 1"], "references": [3009885589, 3008028633, 3005212621, 3006645647, 3009577418, 3008763357, 2302013022, 3010277308, 3024400578, 3006128040]}, {"id": 2493916176, "title": "Enriching Word Vectors with Subword Information", "abstract": "Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models to learn such representations  ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram, words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.", "date": "2017", "authors": ["Piotr Bojanowski , Edouard Grave , Armand Joulin , Tomas Mikolov"], "references": [2153579005, 1614298861, 2117130368, 2962784628, 1810943226, 2963012544, 2147152072, 2251012068, 1662133657, 1938755728]}, {"id": 2962711740, "title": "How Powerful are Graph Neural Networks", "abstract": "", "date": "2018", "authors": ["Keyulu Xu 1, Weihua Hu 2, Jure Leskovec 2, Stefanie Jegelka 1"], "references": [2907492528, 2918342466, 2916106175, 3100078588, 3007332492, 2996604169, 2963465695, 2989341556]}, {"id": 2907492528, "title": "A Comprehensive Survey on Graph Neural Networks", "abstract": "Deep learning has revolutionized many machine learning tasks in recent years, ranging from image classification and video processing to speech recognition and natural language understanding. The data in these tasks are typically represented in the Euclidean space. However, there is an increasing number of applications, where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed significant challenges on the existing machine learning algorithms. Recently, many studies on extending deep learning approaches for graph data have emerged. In this article, we provide a comprehensive overview of graph neural networks (GNNs) in data mining and machine learning fields. We propose a new taxonomy to divide the state-of-the-art GNNs into four categories, namely, recurrent GNNs, convolutional GNNs, graph autoencoders, and spatial-temporal GNNs. We further discuss the applications of GNNs across various domains and summarize the open-source codes, benchmark data sets, and model evaluation of GNNs. Finally, we propose potential research directions in this rapidly growing field.", "date": "2020", "authors": ["Zonghan Wu 1, Shirui Pan 2, Fengwen Chen 1, Guodong Long 1, Chengqi Zhang 1, Philip S. Yu 3"], "references": []}, {"id": 3100848837, "title": "Graph Convolutional Neural Networks for Web-Scale Recommender Systems", "abstract": "Recent advancements in deep neural networks for graph-structured data have led to state-of-the-art performance on recommender system benchmarks. However, making these methods practical and scalable to web-scale recommendation tasks with billions of items and hundreds of millions of users remains an unsolved challenge. Here we describe a large-scale deep recommendation engine that we developed and deployed at Pinterest. We develop a data-efficient Graph Convolutional Network (GCN) algorithm, which combines efficient random walks and graph convolutions to generate embeddings of nodes (i.e., items) that incorporate both graph structure as well as node feature information. Compared to prior GCN approaches, we develop a novel method based on highly efficient random walks to structure the convolutions and design a novel training strategy that relies on harder-and-harder training examples to improve robustness and convergence of the model. We also develop an efficient MapReduce model inference algorithm to generate embeddings using a trained model. Overall, we can train on and embed graphs that are four orders of magnitude larger than typical GCN implementations. We show how GCN embeddings can be used to make high-quality recommendations in various settings at Pinterest, which has a massive underlying graph with 3 billion nodes representing pins and boards, and 17 billion edges. According to offline metrics, user studies, as well as A/B tests, our approach generates higher-quality recommendations than comparable deep learning based systems. To our knowledge, this is by far the largest application of deep graph embeddings to date and paves the way for a new generation of web-scale recommender systems based on graph convolutional architectures.", "date": "2018", "authors": ["Rex Ying 1, Ruining He 2, Kaifeng Chen 3, Pong Eksombatchai 2, William L. Hamilton 1, Jure Leskovec 1"], "references": [2962835968, 2153579005, 2271840356, 2962756421, 2964015378, 3104097132, 2964321699, 2624431344, 2296073425, 2038276547]}, {"id": 2963224980, "title": "A Comprehensive Survey of Graph Embedding: Problems, Techniques, and Applications", "abstract": "Graph is an important data representation which appears in a wide diversity of real-world scenarios. Effective graph analytics provides users a deeper understanding of what is behind the data, and thus can benefit a lot of useful applications such as node classification, node recommendation, link prediction, etc. However, most graph analytics methods suffer the high computation and space cost. Graph embedding is an effective yet efficient way to solve the graph analytics problem. It converts the graph data into a low dimensional space in which the graph structural information and graph properties are maximumly preserved. In this survey, we conduct a comprehensive review of the literature in graph embedding. We first introduce the formal definition of graph embedding as well as the related concepts. After that, we propose two taxonomies of graph embedding which correspond to what challenges exist in different graph embedding problem settings and how the existing work addresses these challenges in their solutions. Finally, we summarize the applications that graph embedding enables and suggest four promising future research directions in terms of computation efficiency, problem settings, techniques, and application scenarios.", "date": "2018", "authors": ["Hongyun Cai 1, Vincent W. Zheng 1, Kevin Chen-Chuan Chang 2"], "references": [2153579005, 1614298861, 1880262756, 2064675550, 2163922914, 2053186076, 2962756421, 2964015378, 3104097132, 2127795553]}, {"id": 2962883549, "title": "Deep Learning in Mobile and Wireless Networking: A Survey", "abstract": "The rapid uptake of mobile devices and the rising popularity of mobile applications and services pose unprecedented demands on mobile and wireless networking infrastructure. Upcoming 5G systems are evolving to support exploding mobile traffic volumes, real-time extraction of fine-grained analytics, and agile management of network resources, so as to maximize user experience. Fulfilling these tasks is challenging, as mobile environments are increasingly complex, heterogeneous, and evolving. One potential solution is to resort to advanced machine learning techniques, in order to help manage the rise in data volumes and algorithm-driven applications. The recent success of deep learning underpins new and powerful tools that tackle problems in this space. In this paper, we bridge the gap between deep learning and mobile and wireless networking research, by presenting a comprehensive survey of the crossovers between the two areas. We first briefly introduce essential background and state-of-the-art in deep learning techniques with potential applications to networking. We then discuss several techniques and platforms that facilitate the efficient deployment of deep learning onto mobile systems. Subsequently, we provide an encyclopedic review of mobile and wireless networking research based on deep learning, which we categorize by different domains. Drawing from our experience, we discuss how to tailor deep learning to mobile environments. We complete this survey by pinpointing current challenges and open future directions for research.", "date": "2019", "authors": ["Chaoyun Zhang 1, Paul Patras 1, Hamed Haddadi 2"], "references": []}, {"id": 2963184176, "title": "Image Generation from Scene Graphs", "abstract": "To truly understand the visual world our models should be able not only to recognize images but also generate them. To this end, there has been exciting recent progress on generating images from natural language descriptions. These methods give stunning results on limited domains such as descriptions of birds or flowers, but struggle to faithfully reproduce complex sentences with many objects and relationships. To overcome this limitation we propose a method for generating images from scene graphs, enabling explicitly reasoning about objects and their relationships. Our model uses graph convolution to process input graphs, computes a scene layout by predicting bounding boxes and segmentation masks for objects, and converts the layout to an image with a cascaded refinement network. The network is trained adversarially against a pair of discriminators to ensure realistic outputs. We validate our approach on Visual Genome and COCO-Stuff, where qualitative results, ablations, and user studies demonstrate our method's ability to generate complex images with multiple objects.", "date": "2018", "authors": ["Justin Johnson , Agrim Gupta , Li Fei-Fei"], "references": [2964121744, 2097117768, 1836465849, 2117539524, 2153579005, 1861492603, 1959608418, 2963073614, 2963684088, 2963373786]}, {"id": 2796426482, "title": "FoldingNet: Point Cloud Auto-Encoder via Deep Grid Deformation", "abstract": "Recent deep networks that directly handle points in a point set, e.g., PointNet, have been state-of-the-art for supervised learning tasks on point clouds such as classification and segmentation. In this work, a novel end-to-end deep auto-encoder is proposed to address unsupervised learning challenges on point clouds. On the encoder side, a graph-based enhancement is enforced to promote local structures on top of PointNet. Then, a novel folding-based decoder deforms a canonical 2D grid onto the underlying 3D object surface of a point cloud, achieving low reconstruction errors even for objects with delicate structures. The proposed decoder only uses about 7% parameters of a decoder with fully-connected neural networks, yet leads to a more discriminative representation that achieves higher linear SVM classification accuracy than the benchmark. In addition, the proposed decoder structure is shown, in theory, to be a generic architecture that is able to reconstruct an arbitrary point cloud from a 2D grid. Our code is available at http://www.merl.com/research/license#FoldingNet", "date": "2018", "authors": ["Yaoqing Yang 1, Chen Feng 2, Yiru Shen 3, Dong Tian 4"], "references": [2187089797, 2964015378, 2560609797, 2963121255, 1920022804, 2964321699, 2211722331, 2546066744, 2558748708, 2964311892]}, {"id": 3100278010, "title": "Neural Graph Collaborative Filtering", "abstract": "Learning vector representations (aka. embeddings) of users and items lies at the core of modern recommender systems. Ranging from early matrix factorization to recently emerged deep learning based methods, existing efforts typically obtain a user's (or an item's) embedding by mapping from pre-existing features that describe the user (or the item), such as ID and attributes. We argue that an inherent drawback of such methods is that, the collaborative signal, which is latent in user-item interactions, is not encoded in the embedding process. As such, the resultant embeddings may not be sufficient to capture the collaborative filtering effect. In this work, we propose to integrate the user-item interactions - more specifically the bipartite graph structure - into the embedding process. We develop a new recommendation framework Neural Graph Collaborative Filtering (NGCF), which exploits the user-item graph structure by propagating embeddings on it. This leads to the expressive modeling of high-order connectivity in user-item graph, effectively injecting the collaborative signal into the embedding process in an explicit manner. We conduct extensive experiments on three public benchmarks, demonstrating significant improvements over several state-of-the-art models like HOP-Rec [39] and Collaborative Memory Network [5]. Further analysis verifies the importance of embedding propagation for learning better user and item representations, justifying the rationality and effectiveness of NGCF. Codes are available at https://github.com/xiangwang1223/neural_graph_collaborative_filtering.", "date": "2019", "authors": ["Xiang Wang 1, Xiangnan He 2, Meng Wang 3, Fuli Feng 1, Tat-Seng Chua 1"], "references": [2964121744, 1533861849, 2964015378, 1994389483, 2964321699, 2157881433, 3098649723, 2963323306, 2253995343, 2741249238]}, {"id": 2786016794, "title": "Modeling polypharmacy side effects with graph convolutional networks", "abstract": "Motivation The use of drug combinations, termed polypharmacy, is common to treat patients with complex diseases or co-existing conditions. However, a major consequence of polypharmacy is a much higher risk of adverse side effects for the patient. Polypharmacy side effects emerge because of drug-drug interactions, in which activity of one drug may change, favorably or unfavorably, if taken with another drug. The knowledge of drug interactions is often limited because these complex relationships are rare, and are usually not observed in relatively small clinical testing. Discovering polypharmacy side effects thus remains an important challenge with significant implications for patient mortality and morbidity. Results Here, we present Decagon, an approach for modeling polypharmacy side effects. The approach constructs a multimodal graph of protein-protein interactions, drug-protein target interactions and the polypharmacy side effects, which are represented as drug-drug interactions, where each side effect is an edge of a different type. Decagon is developed specifically to handle such multimodal graphs with a large number of edge types. Our approach develops a new graph convolutional neural network for multirelational link prediction in multimodal networks. Unlike approaches limited to predicting simple drug-drug interaction values, Decagon can predict the exact side effect, if any, through which a given drug combination manifests clinically. Decagon accurately predicts polypharmacy side effects, outperforming baselines by up to 69%. We find that it automatically learns representations of side effects indicative of co-occurrence of polypharmacy in patients. Furthermore, Decagon models particularly well polypharmacy side effects that have a strong molecular basis, while on predominantly non-molecular side effects, it achieves good performance because of effective sharing of model parameters across edge types. Decagon opens up opportunities to use large pharmacogenomic and patient population data to flag and prioritize polypharmacy side effects for follow-up analysis via formal pharmacological studies. Availability and implementation Source code and preprocessed datasets are at: http://snap.stanford.edu/decagon.", "date": "2018", "authors": ["Marinka Zitnik , Monica Agrawal , Jure Leskovec"], "references": [2964121744, 2153579005, 2095705004, 2187089797, 2537623931, 2624431344, 2604314403, 205829674, 2138862624, 2329493546]}, {"id": 2966149470, "title": "Adversarial Attacks on Neural Networks for Graph Data.", "abstract": "", "date": "2019", "authors": ["Daniel Z\u00fcgner , Amir Akbarnejad , Stephan G\u00fcnnemann"], "references": [3081300507, 3106390645, 2984488829, 3081203761, 3012737746, 3104667978, 3103409210, 3101871847]}, {"id": 2121058967, "title": "Image Super-Resolution Via Sparse Representation", "abstract": "This paper presents a new approach to single-image superresolution, based upon sparse signal representation. Research on image statistics suggests that image patches can be well-represented as a sparse linear combination of elements from an appropriately chosen over-complete dictionary. Inspired by this observation, we seek a sparse representation for each patch of the low-resolution input, and then use the coefficients of this representation to generate the high-resolution output. Theoretical results from compressed sensing suggest that under mild conditions, the sparse representation can be correctly recovered from the downsampled signals. By jointly training two dictionaries for the low- and high-resolution image patches, we can enforce the similarity of sparse representations between the low-resolution and high-resolution image patch pair with respect to their own dictionaries. Therefore, the sparse representation of a low-resolution image patch can be applied with the high-resolution image patch dictionary to generate a high-resolution image patch. The learned dictionary pair is a more compact representation of the patch pairs, compared to previous approaches, which simply sample a large amount of image patch pairs , reducing the computational cost substantially. The effectiveness of such a sparsity prior is demonstrated for both general image super-resolution (SR) and the special case of face hallucination. In both cases, our algorithm generates high-resolution images that are competitive or even superior in quality to images produced by other similar SR methods. In addition, the local sparse modeling of our approach is naturally robust to noise, and therefore the proposed algorithm can handle SR with noisy inputs in a more unified framework.", "date": "2010", "authors": ["Jianchao Yang 1, John Wright 2, Thomas S Huang 1, Yi Ma 2"], "references": [2296616510, 2053186076, 2135046866, 2160547390, 1902027874, 2153663612, 2067042811, 2295549646, 2050834445, 2113606819]}, {"id": 2056370875, "title": "Image Denoising by Sparse 3-D Transform-Domain Collaborative Filtering", "abstract": "We propose a novel image denoising strategy based on an enhanced sparse representation in transform domain. The enhancement of the sparsity is achieved by grouping similar 2D image fragments (e.g., blocks) into 3D data arrays which we call \"groups.\" Collaborative Altering is a special procedure developed to deal with these 3D groups. We realize it using the three successive steps: 3D transformation of a group, shrinkage of the transform spectrum, and inverse 3D transformation. The result is a 3D estimate that consists of the jointly filtered grouped image blocks. By attenuating the noise, the collaborative filtering reveals even the finest details shared by grouped blocks and, at the same time, it preserves the essential unique features of each individual block. The filtered blocks are then returned to their original positions. Because these blocks are overlapping, for each pixel, we obtain many different estimates which need to be combined. Aggregation is a particular averaging procedure which is exploited to take advantage of this redundancy. A significant improvement is obtained by a specially developed collaborative Wiener filtering. An algorithm based on this novel denoising strategy and its efficient implementation are presented in full detail; an extension to color-image denoising is also developed. The experimental results demonstrate that this computationally scalable algorithm achieves state-of-the-art denoising performance in terms of both peak signal-to-noise ratio and subjective visual quality.", "date": "2007", "authors": ["Kostadin Dabov , Alessandro Foi , Vladimir Katkovnik , Karen Egiazarian"], "references": [1679913846, 1992419399, 2153663612, 2136396015, 2113945798, 2170885533, 2135065661, 2144451417, 2108382860, 1941358455]}, {"id": 1895577753, "title": "Show and tell: A neural image caption generator", "abstract": "Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. For instance, while the current state-of-the-art BLEU-1 score (the higher the better) on the Pascal dataset is 25, our approach yields 59, to be compared to human performance around 69. We also show BLEU-1 score improvements on Flickr30k, from 56 to 66, and on SBU, from 19 to 28. Lastly, on the newly released COCO dataset, we achieve a BLEU-4 of 27.7, which is the current state-of-the-art.", "date": "2015", "authors": ["Oriol Vinyals , Alexander Toshev , Samy Bengio , Dumitru Erhan"], "references": [2097117768, 1836465849, 2117539524, 2964308564, 1614298861, 2130942839, 2157331557, 2963542991, 2155541015, 2064675550]}, {"id": 3104097132, "title": "DeepWalk: online learning of social representations", "abstract": "We present DeepWalk, a novel approach for learning latent representations of vertices in a network. These latent representations encode social relations in a continuous vector space, which is easily exploited by statistical models. DeepWalk generalizes recent advancements in language modeling and unsupervised feature learning (or deep learning) from sequences of words to graphs. DeepWalk uses local information obtained from truncated random walks to learn latent representations by treating walks as the equivalent of sentences. We demonstrate DeepWalk's latent representations on several multi-label network classification tasks for social networks such as BlogCatalog, Flickr, and YouTube. Our results show that DeepWalk outperforms challenging baselines which are allowed a global view of the network, especially in the presence of missing information. DeepWalk's representations can provide F1 scores up to 10% higher than competing methods when labeled data is sparse. In some experiments, DeepWalk's representations are able to outperform all baseline methods while using 60% less training data. DeepWalk is also scalable. It is an online learning algorithm which builds useful incremental results, and is trivially parallelizable. These qualities make it suitable for a broad class of real world applications such as network classification, and anomaly detection.", "date": "2014", "authors": ["Bryan Perozzi , Rami Al-Rfou , Steven Skiena"], "references": []}, {"id": 1888005072, "title": "LINE: Large-scale Information Network Embedding", "abstract": "This paper studies the problem of embedding very large information networks into low-dimensional vector spaces, which is useful in many tasks such as visualization, node classification, and link prediction. Most existing graph embedding methods do not scale for real world information networks which usually contain millions of nodes. In this paper, we propose a novel network embedding method called the ``LINE,'' which is suitable for arbitrary types of information networks: undirected, directed, and/or weighted. The method optimizes a carefully designed objective function that preserves both the local and global network structures. An edge-sampling algorithm is proposed that addresses the limitation of the classical stochastic gradient descent and improves both the effectiveness and the efficiency of the inference. Empirical experiments prove the effectiveness of the LINE on a variety of real-world information networks, including language networks, social networks, and citation networks. The algorithm is very efficient, which is able to learn the embedding of a network with millions of vertices and billions of edges in a few hours on a typical single machine. The source code of the LINE is available online\\footnote{\\url{https://github.com/tangjianpku/LINE}}.", "date": "2015", "authors": ["Jian Tang 1, Meng Qu 2, Mingzhe Wang 2, Ming Zhang 2, Jun Yan 1, Qiaozhu Mei 3"], "references": [2153579005, 1614298861, 2187089797, 1532325895, 2131744502, 2053186076, 2001141328, 3104097132, 1854214752, 2156718197]}, {"id": 2964321699, "title": "Convolutional neural networks on graphs with fast localized spectral filtering", "abstract": "In this work, we are interested in generalizing convolutional neural networks (CNNs) from low-dimensional regular grids, where image, video and speech are represented, to high-dimensional irregular domains, such as social networks, brain connectomes or words' embedding, represented by graphs. We present a formulation of CNNs in the context of spectral graph theory, which provides the necessary mathematical background and efficient numerical schemes to design fast localized convolutional filters on graphs. Importantly, the proposed technique offers the same linear computational complexity and constant learning complexity as classical CNNs, while being universal to any graph structure. Experiments on MNIST and 20NEWS demonstrate the ability of this novel deep learning system to learn local, stationary, and compositional features on graphs.", "date": "2016", "authors": ["Micha\u00ebl Defferrard , Xavier Bresson , Pierre Vandergheynst"], "references": [2964121744, 1614298861, 2310919327, 2121947440, 603908379, 2132914434, 2101491865, 2070232376, 2158787690, 2096152098]}, {"id": 2100664567, "title": "On Using Very Large Target Vocabulary for Neural Machine Translation", "abstract": "Neural machine translation, a recently proposed approach to machine translation based purely on neural networks, has shown promising results compared to the existing approaches such as phrase-based statistical machine translation. Despite its recent success, neural machine translation has its limitation in handling a larger vocabulary, as training complexity as well as decoding complexity increase proportionally to the number of target words. In this paper, we propose a method based on importance sampling that allows us to use a very large target vocabulary without increasing training complexity. We show that decoding can be efficiently done even with the model having a very large target vocabulary by selecting only a small subset of the whole target vocabulary. The models trained by the proposed approach are empirically found to outperform the baseline models with a small vocabulary as well as the LSTM-based neural machine translation models. Furthermore, when we use the ensemble of a few models with very large target vocabularies, we achieve the state-of-the-art translation performance (measured by BLEU) on the English!German translation and almost as high performance as state-of-the-art English!French translation system.", "date": "2015", "authors": ["S\u00e9bastien Jean , Kyunghyun Cho , Roland Memisevic , Yoshua Bengio"], "references": [2964308564, 1614298861, 2130942839, 2157331557, 2101105183, 1753482797, 2964199361, 2153653739, 1606347560, 2118434577]}, {"id": 2123024445, "title": "DeViSE: A Deep Visual-Semantic Embedding Model", "abstract": "Modern visual recognition systems are often limited in their ability to scale to large numbers of object categories. This limitation is in part due to the increasing difficulty of acquiring sufficient training data in the form of labeled images as the number of object categories grows. One remedy is to leverage data from other sources - such as text data - both to train visual models and to constrain their predictions. In this paper we present a new deep visual-semantic embedding model trained to identify visual objects using both labeled image data as well as semantic information gleaned from unannotated text. We demonstrate that this model matches state-of-the-art performance on the 1000-class ImageNet object recognition challenge while making more semantically reasonable errors, and also show that the semantic information can be exploited to make predictions about tens of thousands of image labels not observed during training. Semantic knowledge improves such zero-shot predictions achieving hit rates of up to 18% across thousands of novel labels never seen by the visual model.", "date": "2013", "authors": ["Andrea Frome , Greg S Corrado , Jon Shlens , Samy Bengio , Jeff Dean , Marc'Aurelio Ranzato , Tomas Mikolov"], "references": [2618530766, 2117539524, 2153579005, 1614298861, 2108598243, 2146502635, 1904365287, 2187089797, 2168231600, 2132339004]}, {"id": 2120419212, "title": "A discriminatively trained, multiscale, deformable part model", "abstract": "This paper describes a discriminatively trained, multiscale, deformable part model for object detection. Our system achieves a two-fold improvement in average precision over the best performance in the 2006 PASCAL person detection challenge. It also outperforms the best results in the 2007 challenge in ten out of twenty categories. The system relies heavily on deformable parts. While deformable part models have become quite popular, their value had not been demonstrated on difficult benchmarks such as the PASCAL challenge. Our system also relies heavily on new methods for discriminative training. We combine a margin-sensitive approach for data mining hard negative examples with a formalism we call latent SVM. A latent SVM, like a hidden CRF, leads to a non-convex training problem. However, a latent SVM is semi-convex and the training problem becomes convex once latent information is specified for the positive examples. We believe that our training methods will eventually make possible the effective use of more latent information such as hierarchical (grammar) models and models involving latent three dimensional pose.", "date": "2008", "authors": ["P. Felzenszwalb 1, D. McAllester 2, D. Ramanan 3"], "references": [2161969291, 2154422044, 3021469268, 2030536784, 2112020727, 1576520375, 2186094539, 2101534792, 1518641734, 1970255615]}, {"id": 3111950349, "title": "Active Appearance Models", "abstract": "", "date": "2001", "authors": ["Timothy F. Cootes , Gareth J. Edwards , Christopher J. Taylor"], "references": []}, {"id": 3021469268, "title": "Making large-scale svm learining practical", "abstract": "", "date": "1998", "authors": ["T. Joachims"], "references": [2153635508, 2161969291, 1880262756, 2168356304, 2166706824, 1964357740, 2108646579, 2120419212, 2172000360]}, {"id": 2145072179, "title": "PCA-SIFT: a more distinctive representation for local image descriptors", "abstract": "Stable local feature detection and representation is a fundamental component of many image registration and object recognition algorithms. Mikolajczyk and Schmid (June 2003) recently evaluated a variety of approaches and identified the SIFT [D. G. Lowe, 1999] algorithm as being the most resistant to common image deformations. This paper examines (and improves upon) the local image descriptor used by SIFT. Like SIFT, our descriptors encode the salient aspects of the image gradient in the feature point's neighborhood; however, instead of using SIFT's smoothed weighted histograms, we apply principal components analysis (PCA) to the normalized gradient patch. Our experiments demonstrate that the PCA-based local descriptors are more distinctive, more robust to image deformations, and more compact than the standard SIFT representation. We also present results showing that using these descriptors in an image retrieval application results in increased accuracy and faster matching.", "date": "2004", "authors": ["Yan Ke , R. Sukthankar"], "references": [2151103935, 2148694408, 2124386111, 2177274842, 1902027874, 2154422044, 2119747362, 2111308925, 2098693229, 1541642243]}, {"id": 2030536784, "title": "Pictorial Structures for Object Recognition", "abstract": "In this paper we present a computationally efficient framework for part-based modeling and recognition of objects. Our work is motivated by the pictorial structure models introduced by Fischler and Elschlager. The basic idea is to represent an object by a collection of parts arranged in a deformable configuration. The appearance of each part is modeled separately, and the deformable configuration is represented by spring-like connections between pairs of parts. These models allow for qualitative descriptions of visual appearance, and are suitable for generic recognition problems. We address the problem of using pictorial structure models to find instances of an object in an image as well as the problem of learning an object model from training examples, presenting efficient algorithms in both cases. We demonstrate the techniques by learning models that represent faces and human bodies and using the resulting models to locate the corresponding objects in novel images.", "date": "2004", "authors": ["Pedro F. Felzenszwalb 1, Daniel P. Huttenlocher 2"], "references": [2752885492, 2138451337, 2143516773, 2159080219, 1560013842, 1997063559, 301824129, 2123977795, 2085261163, 1988520084]}, {"id": 2115763357, "title": "A general framework for object detection", "abstract": "This paper presents a general trainable framework for object detection in static images of cluttered scenes. The detection technique we develop is based on a wavelet representation of an object class derived from a statistical analysis of the class instances. By learning an object class in terms of a subset of an overcomplete dictionary of wavelet basis functions, we derive a compact representation of an object class which is used as an input to a support vector machine classifier. This representation overcomes both the problem of in-class variability and provides a low false detection rate in unconstrained environments. We demonstrate the capabilities of the technique in two domains whose inherent information content differs significantly. The first system is face detection and the second is the domain of people which, in contrast to faces, vary greatly in color, texture, and patterns. Unlike previous approaches, this system learns from examples and does not rely on any a priori (hand-crafted) models or motion-based segmentation. The paper also presents a motion-based extension to enhance the performance of the detection algorithm over video sequences. The results presented here suggest that this architecture may well be quite general.", "date": "1998", "authors": ["C.P. Papageorgiou , M. Oren , T. Poggio"], "references": [2132984323, 2087347434, 2124351082, 2104671481, 3113292254, 2159173611, 2137346077, 2056695679, 1676612073, 2030989822]}, {"id": 2097018403, "title": "Linear spatial pyramid matching using sparse coding for image classification", "abstract": "Recently SVMs using spatial pyramid matching (SPM) kernel have been highly successful in image classification. Despite its popularity, these nonlinear SVMs have a complexity O(n2 ~ n3) in training and O(n) in testing, where n is the training size, implying that it is nontrivial to scaleup the algorithms to handle more than thousands of training images. In this paper we develop an extension of the SPM method, by generalizing vector quantization to sparse coding followed by multi-scale spatial max pooling, and propose a linear SPM kernel based on SIFT sparse codes. This new approach remarkably reduces the complexity of SVMs to O(n) in training and a constant in testing. In a number of image categorization experiments, we find that, in terms of classification accuracy, the suggested linear SPM based on sparse coding of SIFT descriptors always significantly outperforms the linear SPM kernel on histograms, and is even better than the nonlinear SPM kernels, leading to state-of-the-art performance on several benchmarks by using a single type of descriptors.", "date": "2009", "authors": ["Jianchao Yang 1, Kai Yu 2, Yihong Gong 2, Thomas Huang 1"], "references": [2153635508, 2162915993, 1576445103, 2153663612, 2107034620, 1566135517, 2166049352, 2113606819, 2161516371, 2166742463]}, {"id": 2159269332, "title": "A universal image quality index", "abstract": "We propose a new universal objective image quality index, which is easy to calculate and applicable to various image processing applications. Instead of using traditional error summation methods, the proposed index is designed by modeling any image distortion as a combination of three factors: loss of correlation, luminance distortion, and contrast distortion. Although the new index is mathematically defined and no human visual system model is explicitly employed, our experiments on various image distortion types indicate that it performs significantly better than the widely used distortion metric mean squared error. Demonstrative images and an efficient MATLAB implementation of the algorithm are available online at http://anchovy.ece.utexas.edu//spl sim/zwang/research/quality_index/demo.html.", "date": "2002", "authors": ["Zhou Wang 1, A.C. Bovik 2"], "references": [2153777140, 2912116903, 1543242897]}, {"id": 2142276208, "title": "A new, fast, and efficient image codec based on set partitioning in hierarchical trees", "abstract": "Embedded zerotree wavelet (EZW) coding, introduced by Shapiro (see IEEE Trans. Signal Processing, vol.41, no.12, p.3445, 1993), is a very effective and computationally simple technique for image compression. We offer an alternative explanation of the principles of its operation, so that the reasons for its excellent performance can be better understood. These principles are partial ordering by magnitude with a set partitioning sorting algorithm, ordered bit plane transmission, and exploitation of self-similarity across different scales of an image wavelet transform. Moreover, we present a new and different implementation based on set partitioning in hierarchical trees (SPIHT), which provides even better performance than our previously reported extension of EZW that surpassed the performance of the original EZW. The image coding results, calculated from actual file sizes and images reconstructed by the decoding algorithm, are either comparable to or surpass previous results obtained through much more sophisticated and computationally complex methods. In addition, the new coding and decoding procedures are extremely fast, and they can be made even faster, with only small loss in performance, by omitting entropy coding of the bit stream by the arithmetic code.", "date": "1996", "authors": ["A. Said 1, W.A. Pearlman 2"], "references": [2053691921, 2148593155, 2103504761, 2129652681, 1931641413, 2166087152, 2058719583, 1592970628, 2117465325, 2147399030]}, {"id": 2053691921, "title": "Embedded image coding using zerotrees of wavelet coefficients", "abstract": "The embedded zerotree wavelet algorithm (EZW) is a simple, yet remarkably effective, image compression algorithm, having the property that the bits in the bit stream are generated in order of importance, yielding a fully embedded code. The embedded code represents a sequence of binary decisions that distinguish an image from the \"null\" image. Using an embedded coding algorithm, an encoder can terminate the encoding at any point thereby allowing a target rate or target distortion metric to be met exactly. Also, given a bit stream, the decoder can cease decoding at any point in the bit stream and still produce exactly the same image that would have been encoded at the bit rate corresponding to the truncated bit stream. In addition to producing a fully embedded bit stream, the EZW consistently produces compression results that are competitive with virtually all known compression algorithms on standard test images. Yet this performance is achieved with a technique that requires absolutely no training, no pre-stored tables or codebooks, and requires no prior knowledge of the image source. The EZW algorithm is based on four key concepts: (1) a discrete wavelet transform or hierarchical subband decomposition, (2) prediction of the absence of significant information across scales by exploiting the self-similarity inherent in images, (3) entropy-coded successive-approximation quantization, and (4) universal lossless data compression which is achieved via adaptive arithmetic coding. >", "date": "1993", "authors": ["J.M. Shapiro"], "references": [2132984323, 2098914003, 2140196014, 1996021349, 2156447271, 1970352604, 2103504761, 2129652681, 2166982406, 2186435531]}, {"id": 2118217749, "title": "JPEG2000 Image Compression Fundamentals, Standards and Practice", "abstract": "This is nothing less than a totally essential reference for engineers and researchers in any field of work that involves the use of compressed imagery. Beginning with a thorough and up-to-date overview of the fundamentals of image compression, the authors move on to provide a complete description of the JPEG2000 standard. They then devote space to the implementation and exploitation of that standard. The final section describes other key image compression systems. This work has specific applications for those involved in the development of software and hardware solutions for multimedia, internet, and medical imaging applications.", "date": "2013", "authors": ["David S. Taubman , Michael W. Marcellin"], "references": [2133665775, 2119667497, 2101675075, 2129768577, 1976709621, 2161907179, 2046119925, 2158787690]}, {"id": 2153777140, "title": "Image quality measures and their performance", "abstract": "A number of quality measures are evaluated for gray scale image compression. They are all bivariate, exploiting the differences between corresponding pixels in the original and degraded images. It is shown that although some numerical measures correlate well with the observers' response for a given compression technique, they are not reliable for an evaluation across different techniques. A graphical measure called Hosaka plots, however, can be used to appropriately specify not only the amount, but also the type of degradation in reconstructed images.", "date": "1995", "authors": ["A.M. Eskicioglu , P.S. Fisher"], "references": [3021180913, 2170745401, 1487065163]}, {"id": 2912116903, "title": "Image dissimilarity", "abstract": "", "date": "1998", "authors": ["Jean-Bernard Martens , Lydia Meesters"], "references": [2103504761, 2103232506, 2134774992, 42232744, 1990873664, 2108657140, 1551978325, 2118491738, 2056930330, 2101728695]}, {"id": 2107790757, "title": "Shiftable multiscale transforms", "abstract": "One of the major drawbacks of orthogonal wavelet transforms is their lack of translation invariance: the content of wavelet subbands is unstable under translations of the input signal. Wavelet transforms are also unstable with respect to dilations of the input signal and, in two dimensions, rotations of the input signal. The authors formalize these problems by defining a type of translation invariance called shiftability. In the spatial domain, shiftability corresponds to a lack of aliasing; thus, the conditions under which the property holds are specified by the sampling theorem. Shiftability may also be applied in the context of other domains, particularly orientation and scale. Jointly shiftable transforms that are simultaneously shiftable in more than one domain are explored. Two examples of jointly shiftable transforms are designed and implemented: a 1-D transform that is jointly shiftable in position and scale, and a 2-D transform that is jointly shiftable in position and orientation. The usefulness of these image representations for scale-space analysis, stereo disparity measurement, and image enhancement is demonstrated. >", "date": "1992", "authors": ["E.P. Simoncelli 1, W.T. Freeman 1, E.H. Adelson 1, D.J. Heeger 2"], "references": [2170120409, 2132984323, 2098914003, 1996021349, 2103504761, 2118877769, 1991605728, 2109863423, 2166982406, 1627054999]}, {"id": 2158564760, "title": "Why is image quality assessment so difficult", "abstract": "Image quality assessment plays an important role in various image processing applications. A great deal of effort has been made in recent years to develop objective image quality metrics that correlate with perceived quality measurement. Unfortunately, only limited success has been achieved. In this paper, we provide some insights on why image quality assessment is so difficult by pointing out the weaknesses of the error sensitivity based framework, which has been used by most image quality assessment approaches in the literature. Furthermore, we propose a new philosophy in designing image quality metrics: The main function of the human eyes is to extract structural information from the viewing field, and the human visual system is highly adapted for this purpose. Therefore, a measurement of structural distortion should be a good approximation of perceived image distortion. Based on the new philosophy, we implemented a simple but effective image quality indexing algorithm, which is very promising as shown by our current results.", "date": "2002", "authors": ["Zhou Wang , Alan C. Bovik , Ligang Lu"], "references": [2159269332, 2153777140, 2912116903, 2145792107, 1543242897, 2015937190, 1527289589, 42232744, 2029826041, 1990873664]}, {"id": 2124731682, "title": "Image compression via joint statistical characterization in the wavelet domain", "abstract": "We develop a probability model for natural images, based on empirical observation of their statistics in the wavelet transform domain. Pairs of wavelet coefficients, corresponding to basis functions at adjacent spatial locations, orientations, and scales, are found to be non-Gaussian in both their marginal and joint statistical properties. Specifically, their marginals are heavy-tailed, and although they are typically decorrelated, their magnitudes are highly correlated. We propose a Markov model that explains these dependencies using a linear predictor for magnitude coupled with both multiplicative and additive uncertainties, and show that it accounts for the statistics of a wide variety of images including photographic images, graphical images, and medical images. In order to directly demonstrate the power of the model, we construct an image coder called EPWIC (embedded predictive wavelet image coder), in which subband coefficients are encoded one bitplane at a time using a nonadaptive arithmetic encoder that utilizes conditional probabilities calculated from the model. Bitplanes are ordered using a greedy algorithm that considers the MSE reduction per encoded bit. The decoder uses the statistical model to predict coefficient values based on the bits it has received. Despite the simplicity of the model, the rate-distortion performance of the coder is roughly comparable to the best image coders in the literature.", "date": "1999", "authors": ["R.W. Buccigrossi 1, E.P. Simoncelli 2"], "references": [2132984323, 2151693816, 2053691921, 2408227189, 2148593155, 2156447271, 2103504761, 1490632837, 2107790757, 2180838288]}, {"id": 2115838129, "title": "Linear transform for simultaneous diagonalization of covariance and perceptual metric matrix in image coding", "abstract": "Two types ofredundancies are contained in images: statistical redundancy and psychovisual redundancy. Image representation techniques for image coding should remove both redundancies in order to obtain good results. In order to establish an appropriate representation, the standard approach to transform coding only considers the statistical redundancy, whereas the psychovisual factors are introduced after the selection ofthe representation as a simple scalar weighting in the transform domain. In this work, we take into account the psychovisual factors in the de8nition of the representation together with the statistical factors, by means of the perceptual metric and the covariance matrix, respectively. In general the ellipsoids described by these matrices are not aligned. Therefore, the optimal basis for image representation should simultaneously diagonalize both matrices. This approach to the basis selection problem has several advantages in the particular application ofimage coding. As the transform domain is Euclidean (by de8nition), the quantizer design is highly simpli8ed and at the same time, the use ofscalar quantizers is truly justi8ed. The proposed representation is compared to covariance-based representations such as the DCT and the KLT or PCA using standard JPEG-like and Max-Lloyd quantizers. ? 2003 Pattern Recognition Society. Published by Elsevier Science Ltd. All rights reserved.", "date": "2003", "authors": ["Irene Epifanio 1, Jaime Gutierrez 2, Jesus Malo 2"], "references": [1548802052, 2798909945, 1634005169, 2140196014, 2145889472, 3017143921, 2137234026, 2134383396, 98769269, 1500256440]}, {"id": 2481240925, "title": "Deep Visual-Semantic Alignments for Generating Image Descriptions", "abstract": "We present a model that generates natural language descriptions of images and their regions. Our approach leverages datasets of images and their sentence descriptions to learn about the inter-modal correspondences between language and visual data. Our alignment model is based on a novel combination of Convolutional Neural Networks over image regions, bidirectional Recurrent Neural Networks (RNN) over sentences, and a structured objective that aligns the two modalities through a multimodal embedding. We then describe a Multimodal Recurrent Neural Network architecture that uses the inferred alignments to learn to generate novel descriptions of image regions. We demonstrate that our alignment model produces state of the art results in retrieval experiments on Flickr8K, Flickr30K and MSCOCO datasets. We then show that the generated descriptions outperform retrieval baselines on both full images and on a new dataset of region-level annotations. Finally, we conduct large-scale analysis of our RNN language model on the Visual Genome dataset of 4.1 million captions and highlight the differences between image and region-level caption statistics.", "date": "2017", "authors": ["Andrej Karpathy , Li Fei-Fei"], "references": [2618530766, 2962835968, 2097117768, 2102605133, 2117539524, 2153579005, 2250539671, 2108598243, 2310919327, 2031489346]}, {"id": 1947481528, "title": "Long-term recurrent convolutional networks for visual recognition and description", "abstract": "Models based on deep convolutional networks have dominated recent image interpretation tasks; we investigate whether models which are also recurrent, or \u201ctemporally deep\u201d, are effective for tasks involving sequences, visual and otherwise. We develop a novel recurrent convolutional architecture suitable for large-scale visual learning which is end-to-end trainable, and demonstrate the value of these models on benchmark video recognition tasks, image description and retrieval problems, and video narration challenges. In contrast to current models which assume a fixed spatio-temporal receptive field or simple temporal averaging for sequential processing, recurrent convolutional models are \u201cdoubly deep\u201d in that they can be compositional in spatial and temporal \u201clayers\u201d. Such models may have advantages when target concepts are complex and/or training data are limited. Learning long-term dependencies is possible when nonlinearities are incorporated into the network state updates. Long-term RNN models are appealing in that they directly can map variable-length inputs (e.g., video frames) to variable length outputs (e.g., natural language text) and can model complex temporal dynamics; yet they can be optimized with backpropagation. Our recurrent long-term models are directly connected to modern visual convnet models and can be jointly trained to simultaneously learn temporal dynamics and convolutional perceptual representations. Our results show such models have distinct advantages over state-of-the-art models for recognition or generation which are separately defined and/or optimized.", "date": "2015", "authors": ["Jeff Donahue 1, Lisa Anne Hendricks 1, Sergio Guadarrama 1, Marcus Rohrbach 1, Subhashini Venugopalan 2, Trevor Darrell 1, Kate Saenko 3"], "references": [2618530766, 2962835968, 2097117768, 2117539524, 2130942839, 2108598243, 2155893237, 1849277567, 1861492603, 2064675550]}, {"id": 1959608418, "title": "Auto-Encoding Variational Bayes", "abstract": "Abstract: How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.", "date": "2013", "authors": ["Diederik P Kingma , Max Welling"], "references": [2146502635, 2163922914, 2145094598, 2166851633, 2097268041, 2963173382, 2951493172, 2171490498, 2119196781, 3104819538]}, {"id": 3003301247, "title": "A Style-Based Generator Architecture for Generative Adversarial Networks.", "abstract": "We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces.", "date": "2020", "authors": ["Tero Karras , Samuli Laine , Timo Aila"], "references": [3109317361, 3102761173, 3085152052, 3118365541, 3040894825, 3046884811, 3021368648]}, {"id": 2893749619, "title": "Large Scale GAN Training for High Fidelity Natural Image Synthesis", "abstract": "", "date": "2018", "authors": ["Andrew Brock 1, Jeff Donahue 2, Karen Simonyan 2"], "references": [3003301247, 2962974533, 2804078698, 2962883549, 3035574324, 2996035354, 2890139949, 2965095304]}, {"id": 2962974533, "title": "Semantic Image Synthesis With Spatially-Adaptive Normalization", "abstract": "We propose spatially-adaptive normalization, a simple but effective layer for synthesizing photorealistic images given an input semantic layout. Previous methods directly feed the semantic layout as input to the network, forcing the network to memorize the information throughout all the layers. Instead, we propose using the input layout for modulating the activations in normalization layers through a spatially-adaptive, learned affine transformation. Experiments on several challenging datasets demonstrate the superiority of our method compared to existing approaches, regarding both visual fidelity and alignment with input layouts. Finally, our model allows users to easily control the style and content of image synthesis results as well as create multi-modal results. Code is available upon publication.", "date": "2019", "authors": ["Taesung Park 1, Ming-Yu Liu 2, Ting-Chun Wang 2, Jun-Yan Zhu 3"], "references": [2194775991, 2618530766, 2964121744, 1836465849, 2099471712, 1959608418, 2340897893, 2405756170, 3003301247, 2963981733]}, {"id": 2804078698, "title": "Self-Attention Generative Adversarial Networks", "abstract": "", "date": "2018", "authors": ["Han Zhang 1, Ian J. Goodfellow 1, Dimitris N. Metaxas 2, Augustus Odena 1"], "references": [2883583109, 2990452356, 3034431451, 3099495704, 3034577585, 2996286887, 2997337685]}, {"id": 3035574324, "title": "Analyzing and Improving the Image Quality of StyleGAN", "abstract": "The style-based GAN architecture (StyleGAN) yields state-of-the-art results in data-driven unconditional generative image modeling. We expose and analyze several of its characteristic artifacts, and propose changes in both model architecture and training methods to address them. In particular, we redesign the generator normalization, revisit progressive growing, and regularize the generator to encourage good conditioning in the mapping from latent codes to images. In addition to improving image quality, this path length regularizer yields the additional benefit that the generator becomes significantly easier to invert. This makes it possible to reliably attribute a generated image to a particular network. We furthermore visualize how well the generator utilizes its output resolution, and identify a capacity problem, motivating us to train larger models for additional quality improvements. Overall, our improved model redefines the state of the art in unconditional image modeling, both in terms of existing distribution quality metrics as well as perceived image quality.", "date": "2020", "authors": ["Tero Karras 1, Samuli Laine 1, Miika Aittala 1, Janne Hellsten 1, Jaakko Lehtinen 2, Timo Aila 1"], "references": [2194775991, 2962835968, 2117539524, 1901129140, 1677182931, 1533861849, 2962879692, 2962760235, 648143168, 3003301247]}, {"id": 2982763192, "title": "Free-Form Image Inpainting With Gated Convolution", "abstract": "We present a generative image inpainting system to complete images with free-form mask and guidance. The system is based on gated convolutions learned from millions of images without additional labelling efforts. The proposed gated convolution solves the issue of vanilla convolution that treats all input pixels as valid ones, generalizes partial convolution by providing a learnable dynamic feature selection mechanism for each channel at each spatial location across all layers. Moreover, as free-form masks may appear anywhere in images with any shape, global and local GANs designed for a single rectangular mask are not applicable. Thus, we also present a patch-based GAN loss, named SN-PatchGAN, by applying spectral-normalized discriminator on dense image patches. SN-PatchGAN is simple in formulation, fast and stable in training. Results on automatic image inpainting and user-guided extension demonstrate that our system generates higher-quality and more flexible results than previous methods. Our system helps user quickly remove distracting objects, modify image layouts, clear watermarks and edit faces. Code, demo and models are available at: \\url{https://github.com/JiahuiYu/generative_inpainting}.", "date": "2019", "authors": ["Jiahui Yu 1, Zhe Lin 2, Jimei Yang 2, Xiaohui Shen 3, Xin Lu 2, Thomas Huang 1"], "references": [2117539524, 1901129140, 2963073614, 2331128040, 2963420686, 2963420272, 2519091744, 2607333215, 2962760235, 2105038642]}, {"id": 2962754210, "title": "cGANs with Projection Discriminator", "abstract": "", "date": "2018", "authors": ["Takeru Miyato 1, Masanori Koyama 2"], "references": [3003301247, 2893749619, 2962974533, 2804078698, 2993158499, 2731516742, 2953327099, 2965289598]}, {"id": 2963841322, "title": "Video-to-Video Synthesis", "abstract": "We study the problem of video-to-video synthesis, whose goal is to learn a mapping function from an input source video (e.g., a sequence of semantic segmentation masks) to an output photorealistic video that precisely depicts the content of the source video. While its image counterpart, the image-to-image translation problem, is a popular topic, the video-to-video synthesis problem is less explored in the literature. Without modeling temporal dynamics, directly applying existing image synthesis approaches to an input video often results in temporally incoherent videos of low visual quality. In this paper, we propose a video-to-video synthesis approach under the generative adversarial learning framework. Through carefully-designed generators and discriminators, coupled with a spatio-temporal adversarial objective, we achieve high-resolution, photorealistic, temporally coherent video results on a diverse set of input formats including segmentation masks, sketches, and poses. Experiments on multiple benchmarks show the advantage of our method compared to strong baselines. In particular, our model is capable of synthesizing 2K resolution videos of street scenes up to 30 seconds long, which significantly advances the state-of-the-art of video synthesis. Finally, we apply our method to future video prediction, outperforming several competing systems. Code, models, and more results are available at our website: https://github.com/NVIDIA/vid2vid. (Please use Adobe Reader to see the embedded videos in the paper.)", "date": "2018", "authors": ["Ting-Chun Wang 1, Ming-Yu Liu 1, Jun-Yan Zhu 2, Guilin Liu 3, Andrew Tao 1, Jan Kautz 1, Bryan Catanzaro 1"], "references": [2962974533, 2942074357, 2908541468, 2970415880, 2913399670, 2949825757, 3048510980, 3100398946]}, {"id": 2933374552, "title": "Adversarially Regularized Autoencoders", "abstract": "", "date": "2017", "authors": ["Junbo Jake Zhao 1, 2, Yoon Kim 3, Kelly Zhang 1, Alexander M. Rush 3, Yann LeCun 1"], "references": [2963667126, 2954730351, 2962820504, 2889009749, 2927085091, 2963330684, 3034782902]}, {"id": 2963840672, "title": "Multi-Scale Context Aggregation by Dilated Convolutions", "abstract": "Abstract: State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction and image classification are structurally different. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy.", "date": "2016", "authors": ["Fisher Yu 1, Vladlen Koltun 2"], "references": [2412782625, 2963881378, 2340897893, 2560023338, 2395611524, 2326925005, 2600383743, 2601564443]}, {"id": 2795155917, "title": "Conditional Adversarial Domain Adaptation", "abstract": "Adversarial learning has been embedded into deep networks to learn disentangled and transferable representations for domain adaptation. Existing adversarial domain adaptation methods may struggle to align different domains of multimodal distributions that are native in classification problems. In this paper, we present conditional adversarial domain adaptation, a principled framework that conditions the adversarial adaptation models on discriminative information conveyed in the classifier predictions. Conditional domain adversarial networks (CDANs) are designed with two novel conditioning strategies: multilinear conditioning that captures the cross-covariance between feature representations and classifier predictions to improve the discriminability, and entropy conditioning that controls the uncertainty of classifier predictions to guarantee the transferability. Experiments testify that the proposed approach exceeds the state-of-the-art results on five benchmark datasets.", "date": "2017", "authors": ["Mingsheng Long 1, Zhangjie Cao 1, Jianmin Wang 1, Michael I. Jordan 2"], "references": [2986381065, 2968634921, 2963094258, 3105518655, 2985406498, 3097003645]}, {"id": 2984529706, "title": "Everybody Dance Now", "abstract": "This paper presents a simple method for \u201cdo as I do\u201d motion transfer: given a source video of a person dancing, we can transfer that performance to a novel (amateur) target after only a few minutes of the target subject performing standard moves. We approach this problem as video-to-video translation using pose as an intermediate representation. To transfer the motion, we extract poses from the source subject and apply the learned pose-to-appearance mapping to generate the target subject. We predict two consecutive frames for temporally coherent video results and introduce a separate pipeline for realistic face synthesis. Although our method is quite simple, it produces surprisingly compelling results (see video). This motivates us to also provide a forensics tool for reliable synthetic content detection, which is able to distinguish videos synthesized by our system from real data. In addition, we release a first-of-its-kind open-source dataset of videos that can be legally used for training and motion transfer.", "date": "2019", "authors": ["Caroline Chan , Shiry Ginosar , Tinghui Zhou , Alexei Efros"], "references": [2962835968, 2133665775, 2963073614, 2962793481, 2331128040, 2559085405, 2964304707, 2963800363, 2962947361, 2963784072]}, {"id": 2963626105, "title": "AttGAN: Facial Attribute Editing by Only Changing What You Want", "abstract": "Facial attribute editing aims to manipulate single or multiple attributes on a given face image, i.e., to generate a new face image with desired attributes while preserving other details. Recently, the generative adversarial net (GAN) and encoder\u2013decoder architecture are usually incorporated to handle this task with promising results. Based on the encoder\u2013decoder architecture, facial attribute editing is achieved by decoding the latent representation of a given face conditioned on the desired attributes. Some existing methods attempt to establish an attribute-independent latent representation for further attribute editing. However, such attribute-independent constraint on the latent representation is excessive because it restricts the capacity of the latent representation and may result in information loss, leading to over-smooth or distorted generation. Instead of imposing constraints on the latent representation, in this work, we propose to apply an attribute classification constraint to the generated image to just guarantee the correct change of desired attributes, i.e., to change what you want. Meanwhile, the reconstruction learning is introduced to preserve attribute-excluding details, in other words, to only change what you want. Besides, the adversarial learning is employed for visually realistic editing. These three components cooperate with each other forming an effective framework for high quality facial attribute editing, referred as AttGAN . Furthermore, the proposed method is extended for attribute style manipulation in an unsupervised manner. Experiments on two wild datasets, CelebA and LFW, show that the proposed method outperforms the state-of-the-art on realistic attribute editing with other facial details well preserved.", "date": "2019", "authors": ["Zhenliang He 1, Wangmeng Zuo 2, Meina Kan 1, Shiguang Shan 1, Xilin Chen 1"], "references": [2964121744, 1836465849, 1901129140, 2099471712, 1959608418, 2963073614, 2962793481, 2963684088, 2096733369, 2964153729]}, {"id": 3098418424, "title": "The Perception-Distortion Tradeoff", "abstract": "Image restoration algorithms are typically evaluated by some distortion measure (e.g. PSNR, SSIM, IFC, VIF) or by human opinion scores that quantify perceived perceptual quality. In this paper, we prove mathematically that distortion and perceptual quality are at odds with each other. Specifically, we study the optimal probability for correctly discriminating the outputs of an image restoration algorithm from real images. We show that as the mean distortion decreases, this probability must increase (indicating worse perceptual quality). As opposed to the common belief, this result holds true for any distortion measure, and is not only a problem of the PSNR or SSIM criteria. However, as we show experimentally, for some measures it is less severe (e.g. distance between VGG features). We also show that generative-adversarial-nets (GANs) provide a principled way to approach the perception-distortion bound. This constitutes theoretical support to their observed success in low-level vision tasks. Based on our analysis, we propose a new methodology for evaluating image restoration methods, and use it to perform an extensive comparison between recent super-resolution algorithms.", "date": "2018", "authors": ["Yochai Blau , Tomer Michaeli"], "references": [2133665775, 2963073614, 2310919327, 2962793481, 2099111195, 2963373786, 2331128040, 2963470893, 2962879692, 54257720]}, {"id": 2989855043, "title": "Few-Shot Unsupervised Image-to-Image Translation", "abstract": "Unsupervised image-to-image translation methods learn to map images in a given class to an analogous image in a different class, drawing on unstructured (non-registered) datasets of images. While remarkably successful, current methods require access to many images in both source and destination classes at training time. We argue this greatly limits their use. Drawing inspiration from the human capability of picking up the essence of a novel object from a small number of examples and generalizing from there, we seek a few-shot, unsupervised image-to-image translation algorithm that works on previously unseen target classes that are specified, at test time, only by a few example images. Our model achieves this few-shot generation capability by coupling an adversarial training scheme with a novel network design. Through extensive experimental validation and comparisons to several baseline methods on benchmark datasets, we verify the effectiveness of the proposed framework. Our implementation and datasets are available at https://github.com/NVlabs/FUNIT", "date": "2019", "authors": ["Ming-Yu Liu 1, Xun Huang 2, Arun Mallya 1, Tero Karras 1, Timo Aila 1, Jaakko Lehtinen 3, Jan Kautz 1"], "references": [2194775991, 2962835968, 2099471712, 2108598243, 1536680647, 2183341477, 2962793481, 2963373786, 2331128040, 2963341924]}, {"id": 2981988113, "title": "Learning Fixed Points in Generative Adversarial Networks: From Image-to-Image Translation to Disease Detection and Localization", "abstract": "Generative adversarial networks (GANs) have ushered in a revolution in image-to-image translation. The development and proliferation of GANs raises an interesting question: can we train a GAN to remove an object, if present, from an image while otherwise preserving the image? Specifically, can a GAN ``virtually heal'' anyone by turning his medical image, with an unknown health status (diseased or healthy), into a healthy one, so that diseased regions could be revealed by subtracting those two images? Such a task requires a GAN to identify a minimal subset of target pixels for domain translation, an ability that we call fixed-point translation, which no GAN is equipped with yet. Therefore, we propose a new GAN, called Fixed-Point GAN, trained by (1) supervising same-domain translation through a conditional identity loss, and (2) regularizing cross-domain translation through revised adversarial, domain classification, and cycle consistency loss. Based on fixed-point translation, we further derive a novel framework for disease detection and localization using only image-level annotation. Qualitative and quantitative evaluations demonstrate that the proposed method outperforms the state of the art in multi-domain image-to-image translation and that it surpasses predominant weakly-supervised localization methods in both disease detection and localization. Implementation is available at https://github.com/jlianglab/Fixed-Point-GAN.", "date": "2019", "authors": ["Mahfuzur Rahman Siddiquee 1, Zongwei Zhou 1, Nima Tajbakhsh 1, Ruibin Feng 1, Michael Gotway 2, Yoshua Bengio 3, Jianming Liang 1"], "references": [2194775991, 2099471712, 2963073614, 2962793481, 2962879692, 2295107390, 2962851944, 1834627138, 3102564565, 1641498739]}, {"id": 2206858481, "title": "Visualizing and Understanding Convolutional Neural Networks", "abstract": "", "date": "2013", "authors": ["Matthew D Zeiler , Rob Fergus"], "references": [2194775991, 639708223, 1677182931, 2109255472, 2964153729, 2016053056]}, {"id": 2120480077, "title": "Building high-level features using large scale unsupervised learning", "abstract": "We consider the problem of building high-level, class-specific feature detectors from only unlabeled data. For example, is it possible to learn a face detector using only unlabeled images? To answer this, we train a deep sparse autoencoder on a large dataset of images (the model has 1 billion connections, the dataset has 10 million 200\u00d7200 pixel images downloaded from the Internet). We train this network using model parallelism and asynchronous SGD on a cluster with 1,000 machines (16,000 cores) for three days. Contrary to what appears to be a widely-held intuition, our experimental results reveal that it is possible to train a face detector without having to label images as containing a face or not. Control experiments show that this feature detector is robust not only to translation but also to scaling and out-of-plane rotation. We also find that the same network is sensitive to other high-level concepts such as cat faces and human bodies. Starting from these learned features, we trained our network to recognize 22,000 object categories from ImageNet and achieve a leap of 70% relative improvement over the previous state-of-the-art.", "date": "2013", "authors": ["Quoc V. Le"], "references": [2108598243, 2136922672, 3118608800, 2100495367, 2310919327, 2168231600, 2546302380, 2110798204, 1782590233, 2130325614]}, {"id": 2150165932, "title": "How to Explain Individual Classification Decisions", "abstract": "After building a classifier with modern tools of machine learning we typically have a black box at hand that is able to predict well for unseen data. Thus, we get an answer to the question what is the most likely label of a given unseen data point. However, most methods will provide no answer why the model predicted a particular label for a single instance and what features were most influential for that particular instance. The only method that is currently able to provide such explanations are decision trees. This paper proposes a procedure which (based on a set of assumptions) allows to explain the decisions of any classification method.", "date": "2010", "authors": ["David Baehrens 1, Timon Schroeter 1, Stefan Harmeling 2, Motoaki Kawanabe 3, Katja Hansen 1, Klaus-Robert M\u00fcller 1"], "references": [2156909104, 1746819321, 1554663460, 1480376833, 2119479037, 3023786531, 740415, 2108995755, 1618905105, 1564947197]}, {"id": 2131975293, "title": "Resilient distributed datasets: a fault-tolerant abstraction for in-memory cluster computing", "abstract": "We present Resilient Distributed Datasets (RDDs), a distributed memory abstraction that lets programmers perform in-memory computations on large clusters in a fault-tolerant manner. RDDs are motivated by two types of applications that current computing frameworks handle inefficiently: iterative algorithms and interactive data mining tools. In both cases, keeping data in memory can improve performance by an order of magnitude. To achieve fault tolerance efficiently, RDDs provide a restricted form of shared memory, based on coarse-grained transformations rather than fine-grained updates to shared state. However, we show that RDDs are expressive enough to capture a wide class of computations, including recent specialized programming models for iterative jobs, such as Pregel, and new applications that these models do not capture. We have implemented RDDs in a system called Spark, which we evaluate through a variety of user applications and benchmarks.", "date": "2012", "authors": ["Matei Zaharia , Mosharaf Chowdhury , Tathagata Das , Ankur Dave , Justin Ma , Murphy McCauley , Michael J. Franklin , Scott Shenker , Ion Stoica"], "references": [2173213060, 1554944419, 3013264884, 2170616854, 2100830825, 2098935637, 2096125134, 2163961697, 2060204338, 2109722477]}, {"id": 2125389028, "title": "Conditional Generative Adversarial Nets", "abstract": "Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.", "date": "2014", "authors": ["Mehdi Mirza , Simon Osindero"], "references": [2097117768, 2099471712, 1614298861, 1904365287, 2546302380, 2294059674, 2123024445, 2951446714, 154472438, 1496559305]}, {"id": 2963207607, "title": "Explaining and Harnessing Adversarial Examples", "abstract": "Abstract: Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.", "date": "2014", "authors": ["Ian J. Goodfellow , Jonathon Shlens , Christian Szegedy"], "references": [2963857521, 2964253222, 2604763608, 3102564565, 2180612164, 2243397390, 2964082701]}, {"id": 2963382180, "title": "Striving for Simplicity: The All Convolutional Net", "abstract": "", "date": "2014", "authors": ["Jost Tobias Springenberg , Alexey Dosovitskiy , Thomas Brox , Martin A. Riedmiller"], "references": [1026270304, 3102564565, 2963399829, 2963685250, 2963564844, 2964159205]}, {"id": 2048679005, "title": "Combining labeled and unlabeled data with co-training", "abstract": "We consider the problem of using a large unlabeled sample to boost performance of a learning algorit,hrn when only a small set of labeled examples is available. In particular, we consider a problem setting motivated by the task of learning to classify web pages, in which the description of each example can be partitioned into two distinct views. For example, the description of a web page can be partitioned into the words occurring on that page, and the words occurring in hyperlinks t,hat point to that page. We assume that either view of the example would be sufficient for learning if we had enough labeled data, but our goal is to use both views together to allow inexpensive unlabeled data to augment, a much smaller set of labeled examples. Specifically, the presence of two distinct views of each example suggests strategies in which two learning algorithms are trained separately on each view, and then each algorithm\u2019s predictions on new unlabeled examples are used to enlarge the training set of the other. Our goal in this paper is to provide a PAC-style analysis for this setting, and, more broadly, a PAC-style framework for the general problem of learning from both labeled and unlabeled data. We also provide empirical results on real web-page data indicating that this use of unlabeled examples can lead to significant improvement of hypotheses in practice. *This research was supported in part by the DARPA HPKB program under contract F30602-97-1-0215 and by NSF National Young investigator grant CCR-9357793. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. TO copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. COLT 98 Madison WI USA Copyright ACM 1998 l-58113-057--0/98/ 7...%5.00 92 Tom Mitchell School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213-3891 mitchell+@cs.cmu.edu", "date": "1998", "authors": ["Avrim Blum , Tom Mitchell"], "references": [2049633694, 3017143921, 2101210369, 2167044614, 2128221272, 1995897489, 2103555337, 2150516767, 2020764470, 1550302919]}, {"id": 2962897886, "title": "Stochastic Backpropagation and Approximate Inference in Deep Generative Models", "abstract": "We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition model to represent an approximate posterior distribution and uses this for optimisation of a variational lower bound. We develop stochastic backpropagation - rules for gradient backpropagation through stochastic variables - and derive an algorithm that allows for joint optimisation of the parameters of both the generative and recognition models. We demonstrate on several real-world data sets that by using stochastic backpropagation and variational inference, we obtain models that are able to generate realistic samples of data, allow for accurate imputations of missing data, and provide a useful tool for high-dimensional data visualisation.", "date": "2014", "authors": ["Danilo Jimenez Rezende , Shakir Mohamed , Daan Wierstra"], "references": [1959608418, 2145094598, 2335728318, 2166851633, 2044758663, 2951446714, 2108677974, 2097268041, 2963173382, 2167433878]}, {"id": 2136504847, "title": "Semi-Supervised Learning Literature Survey", "abstract": "", "date": "2004", "authors": ["Xiaojin Zhu"], "references": [1880262756, 2148603752, 2053186076, 2121947440, 2001141328, 2125838338, 2097308346, 2165874743, 2114524997, 1479807131]}, {"id": 1676820704, "title": "Solving multiclass learning problems via error-correcting output codes", "abstract": "Multiclass learning problems involve finding a definition for an unknown function f(x) whose range is a discrete set containing k > 2 values (i.e., k \"classes\"). The definition is acquired by studying collections of training examples of the form (xi, f(xi)). Existing approaches to multiclass learning problems include direct application of multiclass algorithms such as the decision-tree algorithms C4.5 and CART, application of binary concept learning algorithms to learn individual binary functions for each of the k classes, and application of binary concept learning algorithms with distributed output representations. This paper compares these three approaches to a new technique in which error-correcting codes are employed as a distributed output representation. We show that these output representations improve the generalization performance of both C4.5 and backpropagation on a wide range of multiclass learning tasks. We also demonstrate that this approach is robust with respect to changes in the size of the training sample, the assignment of distributed representations to particular classes, and the application of overfitting avoidance techniques such as decision-tree pruning. Finally, we show that--like the other methods--the error-correcting code technique can provide reliable class probability estimates. Taken together, these results demonstrate that error-correcting output codes provide a general-purpose method for improving the performance of inductive learning programs on multiclass problems.", "date": "1994", "authors": ["Thomas G. Dietterich 1, Ghulum Bakiri 2"], "references": [2154642048, 3085162807, 2147800946, 2798643531, 2173629880, 2019363670, 2093717447, 3036751298, 2176028050, 1667614912]}, {"id": 2407712691, "title": "Deep Learning via Semi-Supervised Embedding", "abstract": "We show how nonlinear embedding algorithms popular for use with \"shallow\" semi-supervised learning techniques such as kernel methods can be easily applied to deep multi-layer architectures, either as a regularizer at the output layer, or on each layer of the architecture. This trick provides a simple alternative to existing approaches to deep learning whilst yielding competitive error rates compared to those methods, and existing shallow semi-supervised techniques.", "date": "2011", "authors": ["Jason Weston , Fr\u00e9d\u00e9ric Ratle , Hossein Mobahi , Ronan Collobert"], "references": [2136922672, 2310919327, 2001141328, 2097308346, 1479807131, 2139427956, 2914746235, 2159291644, 2145038566, 2148029428]}, {"id": 2158049734, "title": "Semi-Supervised Learning for Natural Language", "abstract": "", "date": "2004", "authors": ["Percy Liang"], "references": [2147880316, 2139212933, 2121947440, 2048679005, 2008652694, 2097089247, 2138745909, 2156515921, 2107008379, 2144578941]}, {"id": 2122457239, "title": "Semi-Supervised Learning in Gigantic Image Collections", "abstract": "With the advent of the Internet it is now possible to collect hundreds of millions of images. These images come with varying degrees of label information. \"Clean labels\" can be manually obtained on a small fraction, \"noisy labels\" may be extracted automatically from surrounding text, while for most images there are no labels at all. Semi-supervised learning is a principled framework for combining these different label sources. However, it scales polynomially with the number of images, making it impractical for use on gigantic collections with hundreds of millions of images and thousands of classes. In this paper we show how to utilize recent results in machine learning to obtain highly efficient approximations for semi-supervised learning that are linear in the number of images. Specifically, we use the convergence of the eigenvectors of the normalized graph Laplacian to eigenfunctions of weighted Laplace-Beltrami operators. Our algorithm enables us to apply semi-supervised learning to a database of 80 million images gathered from the Internet.", "date": "2009", "authors": ["Rob Fergus 1, Yair Weiss 2, Antonio Torralba 3"], "references": [3118608800, 2110764733, 2145607950, 1560724230, 1566135517, 2293597654, 2111993661, 1479807131, 2154455818, 2104290444]}, {"id": 2112076978, "title": "Experiments with a new boosting algorithm", "abstract": "In an earlier paper, we introduced a new \"boosting\" algorithm called AdaBoost which, theoretically, can be used to significantly reduce the error of any learning algorithm that con- sistently generates classifiers whose performance is a little better than random guessing. We also introduced the related notion of a \"pseudo-loss\" which is a method for forcing a learning algorithm of multi-label concepts to concentrate on the labels that are hardest to discriminate. In this paper, we describe experiments we carried out to assess how well AdaBoost with and without pseudo-loss, performs on real learning problems. We performed two sets of experiments. The first set compared boosting to Breiman's \"bagging\" method when used to aggregate various classifiers (including decision trees and single attribute- value tests). We compared the performance of the two methods on a collection of machine-learning benchmarks. In the second set of experiments, we studied in more detail the performance of boosting using a nearest-neighbor classifier on an OCR problem.", "date": "1996", "authors": ["Yoav Freund , Robert E. Schapire"], "references": [1988790447, 2912934387, 2125055259, 1504694836, 1670263352, 1966280301, 2093717447, 2132166479, 2070534370, 2137291015]}, {"id": 1975846642, "title": "Boosting the margin: a new explanation for the effectiveness of voting methods", "abstract": "One of the surprising recurring phenomena observed in experiments with boosting is that the test error of the generated classifier usually does not increase as its size becomes very large, and often is observed to decrease even after the training error reaches zero. In this paper, we show that this phenomenon is related to the distribution of margins of the training examples with respect to the generated voting classification rule, where the margin of an example is simply the difference between the number of correct votes and the maximum number of votes received by any incorrect label. We show that techniques used in the analysis of Vapnik's support vector classifiers and of neural networks with small weights can be applied to voting methods to relate the margin distribution to the test error. We also show theoretically and experimentally that boosting is especially effective at increasing the margins of the training examples. Finally, we compare our explanation to those based on the bias-variance decomposition.", "date": "1998", "authors": ["Robert E. Schapire , Yoav Freund 1, Peter Bartlett 2, Wee Sun Lee 3"], "references": [2156909104, 2119821739, 1988790447, 2912934387, 2112076978, 2087347434, 1594031697, 1605688901, 2982720039, 2032210760]}, {"id": 2152761983, "title": "An Empirical Comparison of Voting Classification Algorithms: Bagging, Boosting, and Variants", "abstract": "Methods for voting classification algorithms, such as Bagging and AdaBoost, have been shown to be very successful in improving the accuracy of certain classifiers for artificial and real-world datasets. We review these algorithms and describe a large empirical study comparing several variants in conjunction with a decision tree inducer (three variants) and a Naive-Bayes inducer. The purpose of the study is to improve our understanding of why and when these algorithms, which use perturbation, reweighting, and combination techniques, affect classification error. We provide a bias and variance decomposition of the error to show how different methods and variants influence these two terms. This allowed us to determine that Bagging reduced variance of unstable methods, while boosting methods (AdaBoost and Arc-x4) reduced both the bias and variance of unstable methods but increased the variance for Naive-Bayes, which was very stable. We observed that Arc-x4 behaves differently than AdaBoost if reweighting is used instead of resampling, indicating a fundamental difference. Voting variants, some of which are introduced in this paper, include: pruning versus no pruning, use of probabilistic estimates, weight perturbations (Wagging), and backfitting of data. We found that Bagging improves when probabilistic estimates in conjunction with no-pruning are used, as well as when the data was backfit. We measure tree sizes and show an interesting positive correlation between the increase in the average tree size in AdaBoost trials and its success in reducing the error. We compare the mean-squared error of voting methods to non-voting methods and show that the voting methods lead to large and significant reductions in the mean-squared errors. Practical problems that arise in implementing boosting algorithms are explored, including numerical instabilities and underflows. We use scatterplots that graphically show how AdaBoost reweights instances, emphasizing not only \u201chard\u201d areas but also outliers and noise.", "date": "1999", "authors": ["Eric Bauer 1, Ron Kohavi 2"], "references": [2331432542, 1988790447, 2912934387, 2084812512, 2125055259, 2112076978, 1975846642, 1680392829, 2140785063, 3017143921]}, {"id": 2113242816, "title": "The random subspace method for constructing decision forests", "abstract": "Much of previous attention on decision trees focuses on the splitting criteria and optimization of tree sizes. The dilemma between overfitting and achieving maximum accuracy is seldom resolved. A method to construct a decision tree based classifier is proposed that maintains highest accuracy on training data and improves on generalization accuracy as it grows in complexity. The classifier consists of multiple trees constructed systematically by pseudorandomly selecting subsets of components of the feature vector, that is, trees constructed in randomly chosen subspaces. The subspace method is compared to single-tree classifiers and other forest construction methods by experiments on publicly available datasets, where the method's superiority is demonstrated. We also discuss independence between trees in a forest and relate that to the combined classification accuracy.", "date": "1998", "authors": ["Tin Kam Ho"], "references": [2156909104, 2912934387, 2125055259, 3085162807, 2112076978, 2149706766, 1594031697, 2101522199, 1966280301, 2102734279]}, {"id": 1605688901, "title": "An Experimental Comparison of Three Methods for Constructing Ensembles of Decision Trees: Bagging, Boosting, and Randomization", "abstract": "Bagging and boosting are methods that generate a diverse ensemble of classifiers by manipulating the training data given to a \u201cbase\u201d learning algorithm. Breiman has pointed out that they rely for their effectiveness on the instability of the base learning algorithm. An alternative approach to generating an ensemble is to randomize the internal decisions made by the base algorithm. This general approach has been studied previously by Ali and Pazzani and by Dietterich and Kong. This paper compares the effectiveness of randomization, bagging, and boosting for improving the performance of the decision-tree algorithm C4.5. The experiments show that in situations with little or no classification noise, randomization is competitive with (and perhaps slightly superior to) bagging but not as accurate as boosting. In situations with substantial classification noise, bagging is much better than boosting, and sometimes better than randomization.", "date": "2000", "authors": ["Thomas G. Dietterich"], "references": [2912934387, 2112076978, 2152761983, 2982720039, 1966280301, 2167277498, 2073738917, 1562197959, 2976840617, 1850527962]}, {"id": 2120240539, "title": "Shape quantization and recognition with randomized trees", "abstract": "We explore a new approach to shape recognition based on a virtually infinite family of binary features (queries) of the image data, designed to accommodate prior information about shape invariance and regularity. Each query corresponds to a spatial arrangement of several local topographic codes (or tags), which are in themselves too primitive and common to be informative about shape. All the discriminating power derives from relative angles and distances among the tags. The important attributes of the queries are a natural partial ordering corresponding to increasing structure and complexity; semi-invariance, meaning that most shapes of a given class will answer the same way to two queries that are successive in the ordering; and stability, since the queries are not based on distinguished points and substructures. No classifier based on the full feature set can be evaluated, and it is impossible to determine a priori which arrangements are informative. Our approach is to select informative features and build tree classifiers at the same time by inductive learning. In effect, each tree provides an approximation to the full posterior where the features chosen depend on the branch that is traversed. Due to the number and nature of the queries, standard decision tree construction based on a fixed-length feature vector is not feasible. Instead we entertain only a small random sample of queries at each node, constrain their complexity to increase with tree depth, and grow multiple trees. The terminal nodes are labeled by estimates of the corresponding posterior distribution over shape classes. An image is classified by sending it down every tree and aggregating the resulting distributions. The method is applied to classifying handwritten digits and synthetic linear and nonlinear deformations of three hundred L AT E X symbols. Stateof-the-art error rates are achieved on the National Institute of Standards and Technology database of digits. The principal goal of the experiments on L AT E X symbols is to analyze invariance, generalization error and related issues, and a comparison with artificial neural networks methods is presented in this context.", "date": "1997", "authors": ["Yali Amit 1, Donald Geman 2"], "references": [2099111195, 2912934387, 3085162807, 2149706766, 1594031697, 2101522199, 1676820704, 2165758113, 2154579312, 2076118331]}, {"id": 2099968818, "title": "Boosting in the limit: maximizing the margin of learned ensembles", "abstract": "The \"minimum margin\" of an ensemble classifier on a given training set is, roughly speaking, the smallest vote it gives to any correct training label. Recent work has shown that the Adaboost algorithm is particularly effective at producing ensembles with large minimum margins, and theory suggests that this may account for its success at reducing generalization error. We note, however, that the problem of finding good margins is closely related to linear programming, and we use this connection to derive and test new \"LPboosting\" algorithms that achieve better minimum margins than Adaboost.However, these algorithms do not always yield better generalization performance. In fact, more often the opposite is true. We report on a series of controlled experiments which show that no simple version of the minimum-margin story can be complete. We conclude that the crucial question as to why boosting works so well in practice, and how to further improve upon it, remains mostly open.Some of our experiments are interesting for another reason: we show that Adaboost sometimes does overfit--eventually. This may take a very long time to occur, however, which is perhaps why this phenomenon has gone largely unnoticed.", "date": "1998", "authors": ["Adam J. Grove , Dale Schuurmans"], "references": [2119821739, 1988790447, 2125055259, 2112076978, 2152761983, 1504694836, 2982720039, 1966280301, 1553313034, 2172195373]}, {"id": 2067885219, "title": "Arcing classifier (with discussion and a rejoinder by the author)", "abstract": "Recent work has shown that combining multiple versions of unstable classifiers such as trees or neural nets results in reduced test set error. One of the more effective is bagging. Here, modified training sets are formed by resampling from the original training set, classifiers constructed using these training sets and then combined by voting. Freund and Schapire propose an algorithm the basis of which is to adaptively resample and combine (hence the acronym arcing) so that the weights in the resampling are increased for those cases most often misclassified and the combining is done by weighted voting. Arcing is more successful than bagging in test set error reduction. We explore two arcing algorithms, compare them to each other and to bagging, and try to understand how arcing works. We introduce the definitions of bias and variance for a classifier as components of the test set error. Unstable classifiers can have low bias on a large range of data sets. Their problem is high variance. Combining multiple versions either through bagging or arcing reduces variance significantly.", "date": "1998", "authors": ["Leo Breiman"], "references": [2911964244, 2053463056, 1966701961, 2075647286, 3104887532, 2167917621, 2032210760, 1540007258, 2155806188, 2168020168]}, {"id": 1580948147, "title": "Randomizing Outputs to Increase Prediction Accuracy", "abstract": "Bagging and boosting reduce error by changing both the inputs and outputs to form perturbed training sets, growing predictors on these perturbed training sets and combining them. An interesting question is whether it is possible to get comparable performance by perturbing the outputs alone. Two methods of randomizing outputs are experimented with. One is called output smearing and the other output flipping. Both are shown to consistently do better than bagging.", "date": "2000", "authors": ["Leo Breiman"], "references": [1988790447, 2912934387, 3085162807, 2112076978, 1594031697, 1605688901, 2102201073, 2067885219, 2076118331, 139959648]}, {"id": 4919037, "title": "Regularization of Neural Networks using DropConnect", "abstract": "We introduce DropConnect, a generalization of Dropout (Hinton et al., 2012), for regularizing large fully-connected layers within neural networks. When training with Dropout, a randomly selected subset of activations are set to zero within each layer. DropConnect instead sets a randomly selected subset of weights within the network to zero. Each unit thus receives input from a random subset of units in the previous layer. We derive a bound on the generalization performance of both Dropout and DropConnect. We then evaluate DropConnect on a range of datasets, comparing to Dropout, and show state-of-the-art results on several image recognition benchmarks by aggregating multiple DropConnect-trained models.", "date": "2013", "authors": ["Li Wan , Matthew Zeiler , Sixin Zhang , Yann Le Cun , Rob Fergus"], "references": [3118608800, 2310919327, 1904365287, 1665214252, 2131241448, 2141125852, 2335728318, 2134557905, 2963574257, 188867022]}, {"id": 2145889472, "title": "Emergence of simple-cell receptive field properties by learning a sparse code for natural images", "abstract": "The receptive fields of simple cells in mammalian primary visual cortex can be characterized as being spatially localized, oriented and bandpass (selective to structure at different spatial scales), comparable to the basis functions of wavelet transforms. One approach to understanding such response properties of visual neurons has been to consider their relationship to the statistical structure of natural images in terms of efficient coding. Along these lines, a number of studies have attempted to train unsupervised learning algorithms on natural images in the hope of developing receptive fields with similar properties, but none has succeeded in producing a full set that spans the image space and contains all three of the above properties. Here we investigate the proposal that a coding strategy that maximizes sparseness is sufficient to account for these properties. We show that a learning algorithm that attempts to find sparse linear codes for natural scenes will develop a complete family of localized, oriented, bandpass receptive fields, similar to those found in the primary visual cortex. The resulting sparse image code provides a more efficient representation for later stages of processing because it possesses a higher degree of statistical independence among its outputs.", "date": "1996", "authors": ["Bruno A. Olshausen 1, 2, David J. Field 2"], "references": [1993845689, 2180838288, 2167034998, 2120838001, 2122925692, 1914401667, 2106884367, 2911607583, 2042912927, 2117731089]}, {"id": 2122922389, "title": "Self-taught learning: transfer learning from unlabeled data", "abstract": "We present a new machine learning framework called \"self-taught learning\" for using unlabeled data in supervised classification tasks. We do not assume that the unlabeled data follows the same class labels or generative distribution as the labeled data. Thus, we would like to use a large number of unlabeled images (or audio samples, or text documents) randomly downloaded from the Internet to improve performance on a given image (or audio, or text) classification task. Such unlabeled data is significantly easier to obtain than in typical semi-supervised or transfer learning settings, making self-taught learning widely applicable to many practical learning problems. We describe an approach to self-taught learning that uses sparse coding to construct higher-level features using the unlabeled data. These features form a succinct input representation and significantly improve classification performance. When using an SVM for classification, we further show how a Fisher kernel can be learned for this representation.", "date": "2007", "authors": ["Rajat Raina , Alexis Battle , Honglak Lee , Benjamin Packer , Andrew Y. Ng"], "references": [1880262756, 2100495367, 2162915993, 2053186076, 2135046866, 2001141328, 2063978378, 2147152072, 2166049352, 2113606819]}, {"id": 2963037989, "title": "You Only Look Once: Unified, Real-Time Object Detection", "abstract": "We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.", "date": "2016", "authors": ["Joseph Redmon 1, Santosh Divvala 2, Ross Girshick 3, Ali Farhadi 2"], "references": [2097117768, 639708223, 2102605133, 2117539524, 2161969291, 1536680647, 2168356304, 2963542991, 2109255472, 3097096317]}, {"id": 2302255633, "title": "Identity Mappings in Deep Residual Networks", "abstract": "Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62 % error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: https://github.com/KaimingHe/resnet-1k-layers.", "date": "2016", "authors": ["Kaiming He , Xiangyu Zhang , Shaoqing Ren , Jian Sun"], "references": [2194775991, 2962835968, 2097117768, 1836465849, 2117539524, 1677182931, 2183341477, 1861492603, 3118608800, 1904365287]}, {"id": 2130306094, "title": "Deep Neural Networks for Object Detection", "abstract": "Deep Neural Networks (DNNs) have recently shown outstanding performance on image classification tasks [14]. In this paper we go one step further and address the problem of object detection using DNNs, that is not only classifying but also precisely localizing objects of various classes. We present a simple and yet powerful formulation of object detection as a regression problem to object bounding box masks. We define a multi-scale inference procedure which is able to produce high-resolution object detections at a low cost by a few network applications. State-of-the-art performance of the approach is shown on Pascal VOC.", "date": "2013", "authors": ["Christian Szegedy , Alexander Toshev , Dumitru Erhan"], "references": [2618530766, 2161969291, 2108598243, 2168356304, 2146502635, 2100495367, 2031489346, 2072128103, 2022508996, 2167510172]}, {"id": 2129305389, "title": "Segmentation as selective search for object recognition", "abstract": "For object recognition, the current state-of-the-art is based on exhaustive search. However, to enable the use of more expensive features and classifiers and thereby progress beyond the state-of-the-art, a selective search strategy is needed. Therefore, we adapt segmentation as a selective search by reconsidering segmentation: We propose to generate many approximate locations over few and precise object delineations because (1) an object whose location is never generated can not be recognised and (2) appearance and immediate nearby context are most effective for object recognition. Our method is class-independent and is shown to cover 96.7% of all objects in the Pascal VOC 2007 test set using only 1,536 locations per image. Our selective search enables the use of the more expensive bag-of-words method which we use to substantially improve the state-of-the-art by up to 8.5% for 8 out of 20 classes on the Pascal VOC 2010 detection challenge.", "date": "2011", "authors": ["Koen E. A. van de Sande 1, Jasper R. R. Uijlings 2, Theo Gevers 1, Arnold W. M. Smeulders 1"], "references": [2151103935, 2161969291, 2168356304, 2031489346, 3097096317, 2162915993, 2110158442, 2131846894, 1999478155, 1625255723]}, {"id": 2017691720, "title": "Constrained parametric min-cuts for automatic object segmentation", "abstract": "We present a novel framework for generating and ranking plausible objects hypotheses in an image using bottom-up processes and mid-level cues. The object hypotheses are represented as figure-ground segmentations, and are extracted automatically, without prior knowledge about properties of individual object classes, by solving a sequence of constrained parametric min-cut problems (CPMC) on a regular image grid. We then learn to rank the object hypotheses by training a continuous model to predict how plausible the segments are, given their mid-level region properties. We show that this algorithm significantly outperforms the state of the art for low-level segmentation in the VOC09 segmentation dataset. It achieves the same average best segmentation covering as the best performing technique to date [2], 0.61 when using just the top 7 ranked segments, instead of the full hierarchy in [2]. Our method achieves 0.78 average best covering using 154 segments. In a companion paper [18], we also show that the algorithm achieves state-of-the art results when used in a segmentation-based recognition pipeline.", "date": "2010", "authors": ["Joao Carreira , Cristian Sminchisescu"], "references": []}, {"id": 2128715914, "title": "What is an object", "abstract": "We present a generic objectness measure, quantifying how likely it is for an image window to contain an object of any class. We explicitly train it to distinguish objects with a well-defined boundary in space, such as cows and telephones, from amorphous background elements, such as grass and road. The measure combines in a Bayesian framework several image cues measuring characteristics of objects, such as appearing different from their surroundings and having a closed boundary. This includes an innovative cue measuring the closed boundary characteristic. In experiments on the challenging PASCAL VOC 07 dataset, we show this new cue to outperform a state-of-the-art saliency measure [17], and the combined measure to perform better than any cue alone. Finally, we show how to sample windows from an image according to their objectness distribution and give an algorithm to employ them as location priors for modern class-specific object detectors. In experiments on PASCAL VOC 07 we show this greatly reduces the number of windows evaluated by class-specific object detectors.", "date": "2010", "authors": ["Bogdan Alexe , Thomas Deselaers , Vittorio Ferrari"], "references": [2161969291, 2168356304, 2124386111, 2128272608, 2154422044, 1999478155, 2166049352, 2146103513, 2172188317, 2135957164]}, {"id": 2113201641, "title": "Beyond sliding windows: Object localization by efficient subwindow search", "abstract": "Most successful object recognition systems rely on binary classification, deciding only if an object is present or not, but not providing information on the actual object location. To perform localization, one can take a sliding window approach, but this strongly increases the computational cost, because the classifier function has to be evaluated over a large set of candidate subwindows. In this paper, we propose a simple yet powerful branch-and-bound scheme that allows efficient maximization of a large class of classifier functions over all possible subimages. It converges to a globally optimal solution typically in sublinear time. We show how our method is applicable to different object detection and retrieval scenarios. The achieved speedup allows the use of classifiers for localization that formerly were considered too slow for this task, such as SVMs with a spatial pyramid kernel or nearest neighbor classifiers based on the chi2-distance. We demonstrate state-of-the-art performance of the resulting systems on the UIUC Cars dataset, the PASCAL VOC 2006 dataset and in the PASCAL VOC 2007 competition.", "date": "2008", "authors": ["C.H. Lampert 1, M.B. Blaschko 1, T. Hofmann 2"], "references": [2151103935, 2161969291, 1677409904, 2162915993, 2131846894, 3023786531, 2154422044, 2107034620, 2112020727, 2154683974]}, {"id": 2152473410, "title": "Example-based object detection in images by components", "abstract": "We present a general example-based framework for detecting objects in static images by components. The technique is demonstrated by developing a system that locates people in cluttered scenes. The system is structured with four distinct example-based detectors that are trained to separately find the four components of the human body: the head, legs, left arm, and right arm. After ensuring that these components are present in the proper geometric configuration, a second example-based classifier combines the results of the component detectors to classify a pattern as either a \"person\" or a \"nonperson.\" We call this type of hierarchical architecture, in which learning occurs at multiple stages, an adaptive combination of classifiers (ACC). We present results that show that this system performs significantly better than a similar full-body person detector. This suggests that the improvement in performance is due to the component-based approach and the ACC data classification architecture. The algorithm is also more robust than the full-body person detection method in that it is capable of locating partially occluded views of people and people whose body parts have little contrast with the background.", "date": "2001", "authors": ["A. Mohan 1, C. Papageorgiou 2, T. Poggio 3"], "references": [2156909104, 2139212933, 2912934387, 2132984323, 2217896605, 2149684865, 2112076978, 2115763357, 2152761983, 2159686933]}, {"id": 1576520375, "title": "Making large scale SVM learning practical", "abstract": "Training a support vector machine SVM leads to a quadratic optimization problem with bound constraints and one linear equality constraint. Despite the fact that this type of problem is well understood, there are many issues to be considered in designing an SVM learner. In particular, for large learning tasks with many training examples on the shelf optimization techniques for general quadratic programs quickly become intractable in their memory and time requirements. SVM light is an implementation of an SVM learner which addresses the problem of large tasks. This chapter presents algorithmic and computational results developed for SVM light V 2.0, which make large-scale SVM training more practical. The results give guidelines for the application of SVMs to large domains.", "date": "1997", "authors": ["Thorsten Joachims"], "references": [2153635508, 2161969291, 1964357740, 2108646579, 2120419212, 2172000360, 2047221353, 2132870739, 2035720976, 2108995755]}, {"id": 1608462934, "title": "A Trainable System for Object Detection", "abstract": "This paper presents a general, trainable system for object detection in unconstrained, cluttered scenes. The system derives much of its power from a representation that describes an object class in terms of an overcomplete dictionary of local, oriented, multiscale intensity differences between adjacent regions, efficiently computable as a Haar wavelet transform. This example-based learning approach implicitly derives a model of an object class by training a support vector machine classifier using a large set of positive and negative examples. We present results on face, people, and car detection tasks using the same architecture. In addition, we quantify how the representation affects detection performance by considering several alternate representations including pixels and principal components. We also describe a real-time application of our person detection system as part of a driver assistance system.", "date": "2000", "authors": ["Constantine Papageorgiou , Tomaso Poggio"], "references": [2156909104, 2148603752, 2139212933, 2132984323, 2128272608, 2217896605, 2140235142, 2124351082, 2159686933, 26816478]}, {"id": 1992825118, "title": "Detecting pedestrians using patterns of motion and appearance", "abstract": "This paper describes a pedestrian detection system that integrates image intensity information with motion information. We use a detection style algorithm that scans a detector over two consecutive frames of a video sequence. The detector is trained (using AdaBoost) to take advantage of both motion and appearance information to detect a walking person. Past approaches have built detectors based on appearance information, but ours is the first to combine both sources of information in a single detector. The implementation described runs at about 4 frames/second, detects pedestrians at very small scales (as small as 20/spl times/15 pixels), and has a very low false positive rate. Our approach builds on the detection work of Viola and Jones. Novel contributions of this paper include: i) development of a representation of image motion which is extremely efficient, and ii) implementation of a state of the art pedestrian detection system which operates on low resolution images under difficult conditions (such as rain and snow).", "date": "2003", "authors": ["Viola 1, Jones 2"], "references": [2164598857, 1988790447, 2217896605, 2115763357, 2155511848, 2798643531, 2032210760, 2145073242, 2162919312, 2089181482]}, {"id": 2295106276, "title": "Matching shapes", "abstract": "We present a novel approach to measuring similarity between shapes and exploit it for object recognition. In our framework, the measurement of similarity is preceded by (1) solving for correspondences between points on the two shapes, (2) using the correspondences to estimate an aligning transform. In order to solve the correspondence problem, we attach a descriptor, the shape context, to each point. The shape context at a reference point captures the distribution of the remaining points relative to it, thus offering a globally discriminative characterization. Corresponding points on two similar shapes will have similar shape contexts, enabling us to solve for correspondences as an optimal assignment problem. Given the point correspondences, we estimate the transformation that best aligns the two shapes; regularized thin-plate splines provide a flexible class of transformation maps for this purpose. Dis-similarity between two shapes is computed as a sum of matching errors between corresponding points, together with a term measuring the magnitude of the aligning transform. We treat recognition in a nearest-neighbor classification framework. Results are presented for silhouettes, trademarks, handwritten digits and the COIL dataset.", "date": "2000", "authors": ["S. Belongie , J. Malik , J. Puzicha"], "references": [2310919327, 2057175746, 2038952578, 2123977795, 2101522199, 2146766088, 2095757522, 2089181482, 2062104878, 1571578421]}, {"id": 2104974755, "title": "A taxonomy and evaluation of dense two-frame stereo correspondence algorithms", "abstract": "Stereo matching is one of the most active research areas in computer vision. While a large number of algorithms for stereo correspondence have been developed, relatively little work has been done on characterizing their performance. In this paper, we present a taxonomy of dense, two-frame stereo methods designed to assess the different components and design decisions made in individual stereo algorithms. Using this taxonomy, we compare existing stereo methods and present experiments evaluating the performance of many different variants. In order to establish a common software platform and a collection of data sets for easy evaluation, we have designed a stand-alone, flexible C++ implementation that enables the evaluation of individual components and that can be easily extended to include new algorithms. We have also produced several new multiframe stereo data sets with ground truth, and are making both the code and data sets available on the Web.", "date": "2001", "authors": ["D. Scharstein 1, R. Szeliski 2, R. Zabih 3"], "references": [2033819227, 2167667767, 2143516773, 2145023731, 1997063559, 2113137767, 3003662786, 1963623641, 2103504761, 2121781154]}, {"id": 1565746575, "title": "Statistical Comparisons of Classifiers over Multiple Data Sets", "abstract": "While methods for comparing two learning algorithms on a single data set have been scrutinized for quite some time already, the issue of statistical tests for comparisons of more algorithms on multiple data sets, which is even more essential to typical machine learning studies, has been all but ignored. This article reviews the current practice and then theoretically and empirically examines several suitable tests. Based on that, we recommend a set of simple, yet safe and robust non-parametric tests for statistical comparisons of classifiers: the Wilcoxon signed ranks test for comparison of two classifiers and the Friedman test with the corresponding post-hoc tests for comparison of more classifiers over multiple data sets. Results of the latter can also be neatly presented with the newly introduced CD (critical difference) diagrams.", "date": "2006", "authors": ["Janez Dem\u0161ar"], "references": [2084812512, 2115012618, 2009543464, 1966280301, 2167277498, 2121044470, 1585743408, 2030360178, 2024081693, 1524761913]}, {"id": 2163352848, "title": "Multiresolution gray-scale and rotation invariant texture classification with local binary patterns", "abstract": "Presents a theoretically very simple, yet efficient, multiresolution approach to gray-scale and rotation invariant texture classification based on local binary patterns and nonparametric discrimination of sample and prototype distributions. The method is based on recognizing that certain local binary patterns, termed \"uniform,\" are fundamental properties of local image texture and their occurrence histogram is proven to be a very powerful texture feature. We derive a generalized gray-scale and rotation invariant operator presentation that allows for detecting the \"uniform\" patterns for any quantization of the angular space and for any spatial resolution and presents a method for combining multiple operators for multiresolution analysis. The proposed approach is very robust in terms of gray-scale variations since the operator is, by definition, invariant against any monotonic transformation of the gray scale. Another advantage is computational simplicity as the operator can be realized with a few operations in a small neighborhood and a lookup table. Experimental results demonstrate that good discrimination can be achieved with the occurrence statistics of simple rotation invariant local binary patterns.", "date": "2002", "authors": ["T. Ojala , M. Pietikainen , T. Maenpaa"], "references": [2039051707, 3017143921, 2098347925, 2106798282, 2132047332, 2159988601, 2021751319, 1993655741, 2136343973, 2124353687]}, {"id": 2067191022, "title": "Mean shift: a robust approach toward feature space analysis", "abstract": "A general non-parametric technique is proposed for the analysis of a complex multimodal feature space and to delineate arbitrarily shaped clusters in it. The basic computational module of the technique is an old pattern recognition procedure: the mean shift. For discrete data, we prove the convergence of a recursive mean shift procedure to the nearest stationary point of the underlying density function and, thus, its utility in detecting the modes of the density. The relation of the mean shift procedure to the Nadaraya-Watson estimator from kernel regression and the robust M-estimators; of location is also established. Algorithms for two low-level vision tasks discontinuity-preserving smoothing and image segmentation - are described as applications. In these algorithms, the only user-set parameter is the resolution of the analysis, and either gray-level or color images are accepted as input. Extensive experimental results illustrate their excellent performance.", "date": "2002", "authors": ["D. Comaniciu , P. Meer"], "references": [2798766386, 2140235142, 2099244020, 2132549764, 2150134853, 2159128898, 1971784203, 2999729612, 2129905273, 1499877760]}, {"id": 2121947440, "title": "Normalized cuts and image segmentation", "abstract": "We propose a novel approach for solving the perceptual grouping problem in vision. Rather than focusing on local features and their consistencies in the image data, our approach aims at extracting the global impression of an image. We treat image segmentation as a graph partitioning problem and propose a novel global criterion, the normalized cut, for segmenting the graph. The normalized cut criterion measures both the total dissimilarity between the different groups as well as the total similarity within the groups. We show that an efficient computational technique based on a generalized eigenvalue problem can be used to optimize this criterion. We applied this approach to segmenting static images, as well as motion sequences, and found the results to be very encouraging.", "date": "2000", "authors": ["Jianbo Shi 1, J. Malik 2"], "references": [2121947440, 2798909945, 1578099820, 1997063559, 1971784203, 2114487471, 2913192828, 2114030927, 2132603077, 100944330]}, {"id": 2117812871, "title": "Pattern recognition and neural networks", "abstract": "From the Publisher: Pattern recognition has long been studied in relation to many different (and mainly unrelated) applications, such as remote sensing, computer vision, space research, and medical imaging. In this book Professor Ripley brings together two crucial ideas in pattern recognition; statistical methods and machine learning via neural networks. Unifying principles are brought to the fore, and the author gives an overview of the state of the subject. Many examples are included to illustrate real problems in pattern recognition and how to overcome them.This is a self-contained account, ideal both as an introduction for non-specialists readers, and also as a handbook for the more expert reader.", "date": "1995", "authors": ["Brian D. Ripley , N. L. Hjort"], "references": [2156909104, 1554663460, 2119821739, 1988790447, 2912934387, 1679913846, 2112076978, 1971784203, 2046079134, 2147800946]}, {"id": 1496357020, "title": "All of Statistics: A Concise Course in Statistical Inference", "abstract": "WINNER OF THE 2005 DEGROOT PRIZE! This book is for people who want to learn probability and statistics quickly. It brings together many of the main ideas in modern statistics in one place. The book is suitable for students and researchers in statistics, computer science, data mining and machine learning. This book covers a much wider range of topics than a typical introductory text on mathematical statistics. It includes modern topics like nonparametric curve estimation, bootstrapping and classification, topics that are usually relegated to follow-up courses. The reader is assumed to know calculus and a little linear algebra. No previous knowledge of probability and statistics is required. The text can be used at the advanced undergraduate and graduate level.", "date": "2004", "authors": ["Larry Wasserman"], "references": [1663973292, 2000042664, 1521626219, 2530395818, 1951289974, 255556494, 1607663648, 2552194003]}, {"id": 2606594511, "title": "Sn: A simulator for connectionist models", "abstract": "", "date": "1987", "authors": ["Leon Bottou 1, 2, Yann Lecun"], "references": [753012316, 2147800946, 2962727772, 2971225054, 2141504882, 103531544, 3120582545, 2982377460, 2170026903, 2023791596]}, {"id": 2141362318, "title": "Object retrieval with large vocabularies and fast spatial matching", "abstract": "In this paper, we present a large-scale object retrieval system. The user supplies a query object by selecting a region of a query image, and the system returns a ranked list of images that contain the same object, retrieved from a large corpus. We demonstrate the scalability and performance of our system on a dataset of over 1 million images crawled from the photo-sharing site, Flickr [3], using Oxford landmarks as queries. Building an image-feature vocabulary is a major time and performance bottleneck, due to the size of our dataset. To address this problem we compare different scalable methods for building a vocabulary and introduce a novel quantization method based on randomized trees which we show outperforms the current state-of-the-art on an extensive ground-truth. Our experiments show that the quantization has a major effect on retrieval quality. To further improve query performance, we add an efficient spatial verification stage to re-rank the results returned from our bag-of-words model and show that this consistently improves search quality, though by less of a margin when the visual vocabulary is large. We view this work as a promising step towards much larger, \"web-scale \" image corpora.", "date": "2007", "authors": ["J. Philbin 1, O. Chum 1, M. Isard 2, J. Sivic 1, A. Zisserman 1"], "references": [2151103935, 2033819227, 1660390307, 2131846894, 2128017662, 2172188317, 1634005169, 2073965851, 2085261163, 2427881153]}, {"id": 2094728533, "title": "Freebase: a collaboratively created graph database for structuring human knowledge", "abstract": "Freebase is a practical, scalable tuple database used to structure general human knowledge. The data in Freebase is collaboratively created, structured, and maintained. Freebase currently contains more than 125,000,000 tuples, more than 4000 types, and more than 7000 properties. Public read/write access to Freebase is allowed through an HTTP-based graph-query API using the Metaweb Query Language (MQL) as a data query and manipulation language. MQL provides an easy-to-use object-oriented interface to the tuple data in Freebase and is designed to facilitate the creation of collaborative, Web-based data-oriented applications.", "date": "2008", "authors": ["Kurt Bollacker , Colin Evans , Praveen Paritosh , Tim Sturge , Jamie Taylor"], "references": [23685451]}, {"id": 21006490, "title": "WSABIE: scaling up to large vocabulary image annotation", "abstract": "Image annotation datasets are becoming larger and larger, with tens of millions of images and tens of thousands of possible annotations. We propose a strongly performing method that scales to such datasets by simultaneously learning to optimize precision at the top of the ranked list of annotations for a given image and learning a low-dimensional joint embedding space for both images and annotations. Our method, called WSABIE, both outperforms several baseline methods and is faster and consumes less memory.", "date": "2011", "authors": ["Jason Weston 1, Samy Bengio 1, Nicolas Usunier 2"], "references": [2108598243, 2135046866, 2038721957, 1576445103, 2145607950, 1604938182, 2160218441, 2154956324, 28412257, 2102765684]}, {"id": 1897761818, "title": "Every picture tells a story: generating sentences from images", "abstract": "Humans can prepare concise descriptions of pictures, focusing on what they find important. We demonstrate that automatic methods can do so too. We describe a system that can compute a score linking an image to a sentence. This score can be used to attach a descriptive sentence to a given image, or to obtain images that illustrate a given sentence. The score is obtained by comparing an estimate of meaning obtained from the image to one obtained from the sentence. Each estimate of meaning comes from a discriminative procedure that is learned us-ingdata. We evaluate on a novel dataset consisting of human-annotated images. While our underlying estimate of meaning is impoverished, it is sufficient to produce very good quantitative results, evaluated with a novel score that can account for synecdoche.", "date": "2010", "authors": ["Ali Farhadi 1, Mohsen Hejrati 2, Mohammad Amin Sadeghi 2, Peter Young 1, Cyrus Rashtchian 1, Julia Hockenmaier 1, David Forsyth 1"], "references": [2142194269, 2120419212, 1666447063, 1647729745, 2337319582, 2502277634, 1532257412, 2119775030, 2106624428, 2147625498]}, {"id": 2066134726, "title": "Baby talk: Understanding and generating simple image descriptions", "abstract": "We posit that visually descriptive language offers computer vision researchers both information about the world, and information about how people describe the world. The potential benefit from this source is made more significant due to the enormous amount of language data easily available today. We present a system to automatically generate natural language descriptions from images that exploits both statistics gleaned from parsing large quantities of text data and recognition algorithms from computer vision. The system is very effective at producing relevant sentences for images. It also generates descriptions that are notably more true to the specific image content than previous work.", "date": "2011", "authors": ["Girish Kulkarni , Visruth Premraj , Sagnik Dhar , Siming Li , Yejin Choi , Alexander C Berg , Tamara L Berg"], "references": [2108598243, 2101105183, 2145607950, 2536626143, 2134270519, 2098411764, 2054279472, 2137471889, 1897761818, 2109586012]}, {"id": 2134270519, "title": "Learning to detect unseen object classes by between-class attribute transfer", "abstract": "We study the problem of object classification when training and test classes are disjoint, i.e. no training examples of the target classes are available. This setup has hardly been studied in computer vision research, but it is the rule rather than the exception, because the world contains tens of thousands of different object classes and for only a very few of them image, collections have been formed and annotated with suitable class labels. In this paper, we tackle the problem by introducing attribute-based classification. It performs object detection based on a human-specified high-level description of the target objects instead of training images. The description consists of arbitrary semantic attributes, like shape, color or even geographic information. Because such properties transcend the specific learning task at hand, they can be pre-learned, e.g. from image datasets unrelated to the current task. Afterwards, new classes can be detected based on their attribute representation, without the need for a new training phase. In order to evaluate our method and to facilitate research in this area, we have assembled a new large-scale dataset, \u201cAnimals with Attributes\u201d, of over 30,000 animal images that match the 50 classes in Osherson's classic table of how strongly humans associate 85 semantic attributes with animal classes. Our experiments show that by using an attribute layer it is indeed possible to build a learning object detection system that does not require any training images of the target classes.", "date": "2009", "authors": ["Christoph H Lampert , Hannes Nickisch , Stefan Harmeling"], "references": [2151103935, 2161969291, 2119605622, 1988790447, 1576445103, 2154422044, 2154642048, 3085162807, 2120419212, 2098411764]}, {"id": 2098411764, "title": "Describing objects by their attributes", "abstract": "We propose to shift the goal of recognition from naming to describing. Doing so allows us not only to name familiar objects, but also: to report unusual aspects of a familiar object (\u201cspotty dog\u201d, not just \u201cdog\u201d); to say something about unfamiliar objects (\u201chairy and four-legged\u201d, not just \u201cunknown\u201d); and to learn how to recognize new objects with few or no visual examples. Rather than focusing on identity assignment, we make inferring attributes the core problem of recognition. These attributes can be semantic (\u201cspotty\u201d) or discriminative (\u201cdogs have it but sheep do not\u201d). Learning attributes presents a major new challenge: generalization across object categories, not just across instances within a category. In this paper, we also introduce a novel feature selection method for learning attributes that generalize well across categories. We support our claims by thorough evaluation that provides insights into the limitations of the standard recognition paradigm of naming and demonstrates the new abilities provided by our attribute-based framework.", "date": "2009", "authors": ["Ali Farhadi , Ian Endres , Derek Hoiem , David Forsyth"], "references": [2161969291, 2131846894, 2154422044, 2120419212, 2134270519, 2108082645, 2149489787, 2914746235, 2171188998, 2152444902]}, {"id": 2101234009, "title": "Scikit-learn: Machine Learning in Python", "abstract": "Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.", "date": "2011", "authors": ["Fabian Pedregosa 1, Ga\u00ebl Varoquaux 1, Alexandre Gramfort 1, Vincent Michel 1, Bertrand Thirion 1, Olivier Grisel 2, Mathieu Blondel 3, Peter Prettenhofer 4, Ron Weiss 5, Vincent Dubourg 6, Jake Vanderplas 7, Alexandre Passos 8, David Cournapeau 9, Matthieu Brucher 10, Matthieu Perrot 11, \u00c9douard Duchesnay 12"], "references": [2153635508, 2097360283, 2118585731, 2063978378, 2994804831, 2047804403, 2097850441, 2040387238, 1571024744, 2024933578]}, {"id": 2135957164, "title": "Graph-Based Visual Saliency", "abstract": "A new bottom-up visual saliency model, Graph-Based Visual Saliency (GBVS), is proposed. It consists of two steps: first forming activation maps on certain feature channels, and then normalizing them in a way which highlights conspicuity and admits combination with other maps. The model is simple, and biologically plausible insofar as it is naturally parallelized. This model powerfully predicts human fixations on 749 variations of 108 natural images, achieving 98% of the ROC area of a human-based control, whereas the classical algorithms of Itti & Koch ([2], [3], [4]) achieve only 84%.", "date": "2006", "authors": ["Jonathan Harel , Christof Koch , Pietro Perona"], "references": [2128272608, 2139047169, 2054802006, 2156595177, 1969258103, 2119228922, 2115738369, 2117301471, 2014558261, 1533198222]}, {"id": 1511924373, "title": "Studying aesthetics in photographic images using a computational approach", "abstract": "Aesthetics, in the world of art and photography, refers to the principles of the nature and appreciation of beauty. Judging beauty and other aesthetic qualities of photographs is a highly subjective task. Hence, there is no unanimously agreed standard for measuring aesthetic value. In spite of the lack of firm rules, certain features in photographic images are believed, by many, to please humans more than certain others. In this paper, we treat the challenge of automatically inferring aesthetic quality of pictures using their visual content as a machine learning problem, with a peer-rated online photo sharing Website as data source. We extract certain visual features based on the intuition that they can discriminate between aesthetically pleasing and displeasing images. Automated classifiers are built using support vector machines and classification trees. Linear regression on polynomial terms of the features is also applied to infer numerical aesthetics ratings. The work attempts to explore the relationship between emotions which pictures arouse in people, and their low-level content. Potential applications include content-based image retrieval and digital photography.", "date": "2006", "authors": ["Ritendra Datta , Dhiraj Joshi , Jia Li , James Z. Wang"], "references": [2156909104, 2062024414, 3085162807, 2130660124, 740415, 1594031697, 2125148312, 2137471889, 2143668817, 2109868644]}, {"id": 2075456404, "title": "Large-scale visual sentiment ontology and detectors using adjective noun pairs", "abstract": "We address the challenge of sentiment analysis from visual content. In contrast to existing methods which infer sentiment or emotion directly from visual low-level features, we propose a novel approach based on understanding of the visual concepts that are strongly related to sentiments. Our key contribution is two-fold: first, we present a method built upon psychological theories and web mining to automatically construct a large-scale Visual Sentiment Ontology (VSO) consisting of more than 3,000 Adjective Noun Pairs (ANP). Second, we propose SentiBank, a novel visual concept detector library that can be used to detect the presence of 1,200 ANPs in an image. The VSO and SentiBank are distinct from existing work and will open a gate towards various applications enabled by automatic sentiment analysis. Experiments on detecting sentiment of image tweets demonstrate significant improvement in detection accuracy when comparing the proposed SentiBank based predictors with the text-based approaches. The effort also leads to a large publicly available resource consisting of a visual sentiment ontology, a large detector library, and the training/testing benchmark for visual sentiment analysis.", "date": "2013", "authors": ["Damian Borth 1, Rongrong Ji 2, Tao Chen 2, Thomas Breuel 1, Shih-Fu Chang 2"], "references": [2108598243, 2031489346, 2097726431, 2171468534, 1566135517, 1590495275, 2022204871, 2122369144, 38739846, 2139043937]}, {"id": 2078807908, "title": "AVA: A large-scale database for aesthetic visual analysis", "abstract": "With the ever-expanding volume of visual content available, the ability to organize and navigate such content by aesthetic preference is becoming increasingly important. While still in its nascent stage, research into computational models of aesthetic preference already shows great potential. However, to advance research, realistic, diverse and challenging databases are needed. To this end, we introduce a new large-scale database for conducting Aesthetic Visual Analysis: AVA. It contains over 250,000 images along with a rich variety of meta-data including a large number of aesthetic scores for each image, semantic labels for over 60 categories as well as labels related to photographic style. We show the advantages of AVA with respect to existing databases in terms of scale, diversity, and heterogeneity of annotations. We then describe several key insights into aesthetic preference afforded by AVA. Finally, we demonstrate, through three applications, how the large scale of AVA can be leveraged to improve performance on existing preference tasks.", "date": "2012", "authors": ["Naila Murray 1, Luca Marchesotti 2, Florent Perronnin 2"], "references": [2151103935, 2108598243, 2031489346, 1576445103, 1606858007, 2166049352, 2147238549, 1511924373, 2170658603, 2104915826]}, {"id": 2157462866, "title": "A reliable effective terascale linear learning system", "abstract": "We present a system and a set of techniques for learning linear predictors with convex losses on terascale data sets, with trillions of features, billions of training examples and millions of parameters in an hour using a cluster of 1000 machines. Individually none of the component techniques are new, but the careful synthesis required to obtain an efficient implementation is. The result is, up to our knowledge, the most scalable and efficient linear learning system reported in the literature. We describe and thoroughly evaluate the components of the system, showing the importance of the various design choices.", "date": "2013", "authors": ["Alekh Agarwal 1, Olivier Chapelle 2, Miroslav Dud\u00edk 1, John Langford 1"], "references": [2173213060, 2164278908, 2146502635, 2168231600, 2131975293, 2109722477, 1564947197, 2166706236, 2148087609, 1603765807]}, {"id": 2128272608, "title": "A model of saliency-based visual attention for rapid scene analysis", "abstract": "A visual attention system, inspired by the behavior and the neuronal architecture of the early primate visual system, is presented. Multiscale image features are combined into a single topographical saliency map. A dynamical neural network then selects attended locations in order of decreasing saliency. The system breaks down the complex problem of scene understanding by rapidly selecting, in a computationally efficient manner, conspicuous locations to be analyzed in detail.", "date": "1998", "authors": ["L. Itti 1, C. Koch 1, E. Niebur 2"], "references": [2093353037, 2149095485, 2160903697, 1486735428, 2089597841, 1497599070, 2115441154, 2152752164, 1502980253, 2142768220]}, {"id": 2124351082, "title": "Training support vector machines: an application to face detection", "abstract": "We investigate the application of Support Vector Machines (SVMs) in computer vision. SVM is a learning technique developed by V. Vapnik and his team (AT&T Bell Labs., 1985) that can be seen as a new method for training polynomial, neural network, or Radial Basis Functions classifiers. The decision surfaces are found by solving a linearly constrained quadratic programming problem. This optimization problem is challenging because the quadratic form is completely dense and the memory requirements grow with the square of the number of data points. We present a decomposition algorithm that guarantees global optimality, and can be used to train SVM's over very large data sets. The main idea behind the decomposition is the iterative solution of sub-problems and the evaluation of optimality conditions which are used both to generate improved iterative values, and also establish the stopping criteria for the algorithm. We present experimental results of our implementation of SVM, and demonstrate the feasibility of our approach on a face detection problem that involves a data set of 50,000 data points.", "date": "1997", "authors": ["E. Osuna , R. Freund , F. Girosit"], "references": [2156909104, 2119821739, 2087347434, 2159686933, 26816478, 2159173611, 2137346077, 2084844503, 2056695679, 2125713050]}, {"id": 2159686933, "title": "Example-based learning for view-based human face detection", "abstract": "We present an example-based learning approach for locating vertical frontal views of human faces in complex scenes. The technique models the distribution of human face patterns by means of a few view-based \"face\" and \"nonface\" model clusters. At each image location, a difference feature vector is computed between the local image pattern and the distribution-based model. A trained classifier determines, based on the difference feature vector measurements, whether or not a human face exists at the current image location. We show empirically that the distance metric we adopt for computing difference feature vectors, and the \"nonface\" clusters we include in our distribution-based model, are both critical for the success of our system.", "date": "1997", "authors": ["K.-K. Sung 1, T. Poggio 2"], "references": [2138451337, 3017143921, 2113341759, 2098947662, 2135463994, 2104671481, 3113292254, 1554705102, 1532977286, 1571461735]}, {"id": 2155511848, "title": "A statistical method for 3D object detection applied to faces and cars", "abstract": "In this paper, we describe a statistical method for 3D object detection. We represent the statistics of both object appearance and \"non-object\" appearance using a product of histograms. Each histogram represents the joint statistics of a subset of wavelet coefficients and their position on the object. Our approach is to use many such histograms representing a wide variety of visual attributes. Using this method, we have developed the first algorithm that can reliably detect human faces with out-of-plane rotation and the first algorithm that can reliably detect passenger cars over a wide range of viewpoints.", "date": "2000", "authors": ["H. Schneiderman , T. Kanade"], "references": [2156909104, 1988790447, 2217896605, 2117812871, 1658679052, 2159686933, 2140785063, 3003716168, 2166713160, 2138560582]}, {"id": 2101522199, "title": "Joint induction of shape features and tree classifiers", "abstract": "We introduce a very large family of binary features for two-dimensional shapes. The salient ones for separating particular shapes are determined by inductive learning during the construction of classification trees. There is a feature for every possible geometric arrangement of local topographic codes. The arrangements express coarse constraints on relative angles and distances among the code locations and are nearly invariant to substantial affine and nonlinear deformations. They are also partially ordered, which makes it possible to narrow the search for informative ones at each node of the tree. Different trees correspond to different aspects of shape. They are statistically and weakly dependent due to randomization and are aggregated in a simple way. Adapting the algorithm to a shape family is then fully automatic once training samples are provided. As an illustration, we classified handwritten digits from the NIST database; the error rate was 0.7 percent.", "date": "1997", "authors": ["Y. Amit 1, D. Geman 2, K. Wilder 2"], "references": [2912934387, 3085162807, 2087347434, 1594031697, 2120240539, 1676820704, 2154579312, 2102734279, 2168228682, 2100659887]}, {"id": 1588351438, "title": "Statistical Pattern Recognition", "abstract": "Introduction to statistical pattern recognition * Estimation * Density estimation * Linear discriminant analysis * Nonlinear discriminant analysis - neural networks * Nonlinear discriminant analysis - statistical methods * Classification trees * Feature selection and extraction * Clustering * Additional topics * Measures of dissimilarity * Parameter estimation * Linear algebra * Data * Probability theory.", "date": "1999", "authors": ["Andrew R. Webb"], "references": [2164598857, 3097096317, 2154053567, 1499991161, 2045186954, 2075647286, 2096754397, 2098347925, 2150757437]}, {"id": 2110158442, "title": "Contour Detection and Hierarchical Image Segmentation", "abstract": "This paper investigates two fundamental problems in computer vision: contour detection and image segmentation. We present state-of-the-art algorithms for both of these tasks. Our contour detector combines multiple local cues into a globalization framework based on spectral clustering. Our segmentation algorithm consists of generic machinery for transforming the output of any contour detector into a hierarchical region tree. In this manner, we reduce the problem of image segmentation to that of contour detection. Extensive experimental evaluation demonstrates that both our contour detection and segmentation methods significantly outperform competing algorithms. The automatically generated hierarchical segmentations can be interactively refined by user-specified annotations. Computation at multiple image resolutions provides a means of coupling our system to recognition applications.", "date": "2011", "authors": ["P Arbela\u0301ez 1, M Maire 2, C Fowlkes 3, J Malik 1"], "references": [2067191022, 2121947440, 2116040950, 2124351162, 1999478155, 2145023731, 1578099820, 2169551590, 2121927366, 2109200236]}, {"id": 2118020653, "title": "Machine learning in automated text categorization", "abstract": "The automated categorization (or classification) of texts into predefined categories has witnessed a booming interest in the last 10 years, due to the increased availability of documents in digital form and the ensuing need to organize them. In the research community the dominant approach to this problem is based on machine learning techniques: a general inductive process automatically builds a classifier by learning, from a set of preclassified documents, the characteristics of the categories. The advantages of this approach over the knowledge engineering approach (consisting in the manual definition of a classifier by domain experts) are a very good effectiveness, considerable savings in terms of expert labor power, and straightforward portability to different domains. This survey discusses the main approaches to text categorization that fall within the machine learning paradigm. We will discuss in detail issues pertaining to three different problems, namely, document representation, classifier construction, and classifier evaluation.", "date": "2002", "authors": ["Fabrizio Sebastiani"], "references": [1574901103, 2149684865, 2147152072, 2435251607, 2097089247, 2114535528, 2005422315, 2053463056, 2140785063, 1978394996]}, {"id": 2113459411, "title": "Learning Word Vectors for Sentiment Analysis", "abstract": "Unsupervised vector-based approaches to semantics can model rich lexical meanings, but they largely fail to capture sentiment information that is central to many word meanings and important for a wide range of NLP tasks. We present a model that uses a mix of unsupervised and supervised techniques to learn word vectors capturing semantic term--document information as well as rich sentiment content. The proposed model can leverage both continuous and multi-dimensional sentiment information as well as non-sentiment annotations. We instantiate the model to utilize the document-level sentiment polarity annotations present in many online documents (e.g. star ratings). We evaluate the model using small, widely used sentiment and subjectivity corpora and find it out-performs several previously introduced methods for sentiment classification. We also introduce a large dataset of movie reviews to serve as a more robust benchmark for work in this area.", "date": "2011", "authors": ["Andrew L. Maas , Raymond E. Daly , Peter T. Pham , Dan Huang , Andrew Y. Ng , Christopher Potts"], "references": [1880262756, 2118585731, 2117130368, 2166706824, 2132339004, 2158139315, 2147152072, 2114524997, 1662133657, 2163455955]}, {"id": 71795751, "title": "Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions", "abstract": "We introduce a novel machine learning framework based on recursive autoencoders for sentence-level prediction of sentiment label distributions. Our method learns vector space representations for multi-word phrases. In sentiment prediction tasks these representations outperform other state-of-the-art approaches on commonly used datasets, such as movie reviews, without using any pre-defined sentiment lexica or polarity shifting rules. We also evaluate the model's ability to predict sentiment distributions on a new dataset based on confessions from the experience project. The dataset consists of personal user stories annotated with multiple labels which, when aggregated, form a multinomial distribution that captures emotional reactions. Our algorithm can more accurately predict distributions over such labels compared to several competitive baselines.", "date": "2011", "authors": ["Richard Socher , Jeffrey Pennington , Eric H. Huang , Andrew Y. Ng , Christopher D. Manning"], "references": [1880262756, 2097726431, 2117130368, 2166706824, 2132339004, 2155328222, 2158139315, 1423339008, 2114524997, 2022204871]}, {"id": 2164019165, "title": "Improving Word Representations via Global Context and Multiple Word Prototypes", "abstract": "Unsupervised word representations are very useful in NLP tasks both as inputs to learning algorithms and as extra word features in NLP systems. However, most of these models are built with only local context and one representation per word. This is problematic because words are often polysemous and global context can also provide useful information for learning word meanings. We present a new neural network architecture which 1) learns word embeddings that better capture the semantics of words by incorporating both local and global document context, and 2) accounts for homonymy and polysemy by learning multiple embeddings per word. We introduce a new dataset with human judgments on pairs of words in sentential context, and evaluate our model on it, showing that our model outperforms competitive baselines and other neural language models.", "date": "2012", "authors": ["Eric Huang , Richard Socher , Christopher Manning , Andrew Ng"], "references": [1532325895, 2117130368, 2118020653, 2132339004, 2158139315, 1423339008, 71795751, 2081580037, 1970381522, 2120779048]}, {"id": 2097606805, "title": "Accurate Unlexicalized Parsing", "abstract": "We demonstrate that an unlexicalized PCFG can parse much more accurately than previously shown, by making use of simple, linguistically motivated state splits, which break down false independence assumptions latent in a vanilla treebank grammar. Indeed, its performance of 86.36% (LP/LR F1) is better than that of early lexicalized PCFG models, and surprisingly close to the current state-of-the-art. This result has potential uses beyond establishing a strong lower bound on the maximum possible accuracy of unlexicalized models: an unlexicalized PCFG is much more compact, easier to replicate, and easier to interpret than more complex lexical models, and the parsing algorithms are simpler, more widely understood, of lower asymptotic complexity, and easier to optimize.", "date": "2003", "authors": ["Dan Klein , Christopher D. Manning"], "references": [1535015163, 2092654472, 2110882317, 1567570606, 2153439141, 1551104980, 2155693943, 2161204834, 1859173823, 199541590]}, {"id": 1544827683, "title": "Teaching machines to read and comprehend", "abstract": "Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.", "date": "2015", "authors": ["Karl Moritz Hermann 1, Tom\u00e1\u0161 Ko\u010disk\u00fd 2, Edward Grefenstette 1, Lasse Espeholt 1, Will Kay 1, Mustafa Suleyman 1, Phil Blunsom 2"], "references": [2964308564, 2130942839, 2158899491, 2064675550, 2120615054, 1793121960, 2962741254, 2147527908, 2584341106, 2144499799]}, {"id": 2125436846, "title": "MCTest: A Challenge Dataset for the Open-Domain Machine Comprehension of Text", "abstract": "We present MCTest, a freely available set of stories and associated questions intended for research on the machine comprehension of text. Previous work on machine comprehension (e.g., semantic modeling) has made great strides, but primarily focuses either on limited-domain datasets, or on solving a more restricted goal (e.g., open-domain relation extraction). In contrast, MCTest requires machines to answer multiple-choice reading comprehension questions about fictional stories, directly tackling the high-level goal of open-domain machine comprehension. Reading comprehension can test advanced abilities such as causal reasoning and understanding the world, yet, by being multiple-choice, still provide a clear metric. By being fictional, the answer typically can be found only in the story itself. The stories and questions are also carefully limited to those a young child would understand, reducing the world knowledge that is required for the task. We present the scalable crowd-sourcing methods that allow us to cheaply construct a dataset of 500 stories and 2000 questions. By screening workers (with grammar tests) and stories (with grading), we have ensured that the data is the same quality as another set that we manually edited, but at one tenth the editing cost. By being open-domain, yet carefully restricted, we hope MCTest will serve to encourage research and provide a clear metric for advancement on the machine comprehension of text. 1 Reading Comprehension A major goal for NLP is for machines to be able to understand text as well as people. Several research disciplines are focused on this problem: for example, information extraction, relation extraction, semantic role labeling, and recognizing textual entailment. Yet these techniques are necessarily evaluated individually, rather than by how much they advance us towards the end goal. On the other hand, the goal of semantic parsing is the machine comprehension of text (MCT), yet its evaluation requires adherence to a specific knowledge representation, and it is currently unclear what the best representation is, for open-domain text. We believe that it is useful to directly tackle the top-level task of MCT. For this, we need a way to measure progress. One common method for evaluating someone\u2019s understanding of text is by giving them a multiple-choice reading comprehension test. This has the advantage that it is objectively gradable (vs. essays) yet may test a range of abilities such as causal or counterfactual reasoning, inference among relations, or just basic understanding of the world in which the passage is set. Therefore, we propose a multiple-choice reading comprehension task as a way to evaluate progress on MCT. We have built a reading comprehension dataset containing 500 fictional stories, with 4 multiple choice questions per story. It was built using methods which can easily scale to at least 5000 stories, since the stories were created, and the curation was done, using crowd sourcing almost entirely, at a total of $4.00 per story. We plan to periodically update the dataset to ensure that methods are not overfitting to the existing data. The dataset is open-domain, yet restricted to concepts and words that a 7 year old is expected to understand. This task is still beyond the capability of today\u2019s computers and algorithms.", "date": "2013", "authors": ["Matthew Richardson , Christopher J.C. Burges , Erin Renshaw"], "references": [2106568252, 2525127255, 2126631960, 2167090521, 2103001613, 1979532929, 2989499211, 2096979215, 2121300346, 2142898321]}, {"id": 2964267515, "title": "The Goldilocks Principle: Reading Children's Books with Explicit Memory Representations", "abstract": "Abstract: We introduce a new test of how well language models capture meaning in children's books. Unlike standard language modelling benchmarks, it distinguishes the task of predicting syntactic function words from that of predicting lower-frequency words, which carry greater semantic content. We compare a range of state-of-the-art models, each with a different way of encoding what has been previously read. We show that models which store explicit representations of long-term contexts outperform state-of-the-art neural language models at predicting semantic content words, although this advantage is not observed for syntactic function words. Interestingly, we find that the amount of text encoded in a single memory representation is highly influential to the performance: there is a sweet-spot, not too big and not too small, between single words and full sentences that allows the most meaningful information in a text to be effectively retained and recalled. Further, the attention over such window-based memories can be trained effectively through self-supervision. We then assess the generality of this principle by applying it to the CNN QA benchmark, which involves identifying named entities in paraphrased summaries of news articles, and achieve state-of-the-art performance.", "date": "2015", "authors": ["Felix Hill , Antoine Bordes , Sumit Chopra , Jason Weston"], "references": [2963748441, 2551396370, 2963339397, 2962809918, 2740747242, 2962985038, 2964223283]}, {"id": 2962809918, "title": "A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task", "abstract": "Enabling a computer to understand a document so that it can answer comprehension questions is a central, yet unsolved goal of NLP. A key factor impeding its solution by machine learned systems is the limited availability of human-annotated data. Hermann et al. (2015) seek to solve this problem by creating over a million training examples by pairing CNN and Daily Mail news articles with their summarized bullet points, and show that a neural network can then be trained to give good performance on this task. In this paper, we conduct a thorough examination of this new reading comprehension task. Our primary aim is to understand what depth of language understanding is required to do well on this task. We approach this from one side by doing a careful hand-analysis of a small subset of the problems and from the other by showing that simple, carefully designed systems can obtain accuracies of 72.4% and 75.8% on these two datasets, exceeding current state-of-the-art results by over 5% and approaching what we believe is the ceiling for performance on this task.1", "date": "2015", "authors": ["Danqi Chen 1, Jason Bolton 2, Christopher D. Manning 2"], "references": [2250539671, 1902237438, 1544827683, 1793121960, 2250861254, 2584341106, 2125436846, 2964267515, 2962790689, 2964091467]}, {"id": 2171278097, "title": "Building Watson: An Overview of the DeepQA Project", "abstract": "IBM Research undertook a challenge to build a computer system that could compete at the human champion level in real time on the American TV Quiz show, Jeopardy! The extent of the challenge includes fielding a real-time automatic contestant on the show, not merely a laboratory exercise. The Jeopardy! Challenge helped us address requirements that led to the design of the DeepQA architecture and the implementation of Watson. After 3 years of intense research and development by a core team of about 20 researches, Watson is performing at human expert-levels in terms of precision, confidence and speed at the Jeopardy! Quiz show. Our results strongly suggest that DeepQA is an effective and extensible architecture that may be used as a foundation for combining, deploying, evaluating and advancing a wide range of algorithmic techniques to rapidly advance the field of QA.", "date": "2010", "authors": ["David A. Ferrucci 1, Eric W. Brown 1, Jennifer Chu-Carroll 1, James Fan 1, David Gondek 1, Aditya Kalyanpur 1, Adam Lally 1, J. William Murdock 1, Eric Nyberg 2, John M. Prager 1, Nico Schlaefer 2, Christopher A. Welty 1"], "references": [2047221353, 2081580037, 2096797897, 2150884987, 2087064593, 2988119488, 2107658650, 2158823144, 2122537498, 2080278171]}, {"id": 2962790689, "title": "Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks", "abstract": "Abstract: One long-term goal of machine learning research is to produce methods that are applicable to reasoning and natural language, in particular building an intelligent dialogue agent. To measure progress towards that goal, we argue for the usefulness of a set of proxy tasks that evaluate reading comprehension via question answering. Our tasks measure understanding in several ways: whether a system is able to answer questions via chaining facts, simple induction, deduction and many more. The tasks are designed to be prerequisites for any system that aims to be capable of conversing with a human. We believe many existing learning systems can currently not solve them, and hence our aim is to classify these tasks into skill sets, so that researchers can identify (and then rectify) the failings of their systems. We also extend and improve the recently introduced Memory Networks model, and show it is able to solve some, but not all, of the tasks.", "date": "2015", "authors": ["Jason Weston , Antoine Bordes , Sumit Chopra , Alexander M. Rush , Bart van Merri\u00ebnboer , Armand Joulin , Tomas Mikolov"], "references": [1933349210, 2962809918, 2561715562, 2964091467, 2768661419, 2963448850, 2962717047]}, {"id": 2251818205, "title": "WikiQA: A Challenge Dataset for Open-Domain Question Answering", "abstract": "We describe the WIKIQA dataset, a new publicly available set of question and sentence pairs, collected and annotated for research on open-domain question answering. Most previous work on answer sentence selection focuses on a dataset created using the TREC-QA data, which includes editor-generated questions and candidate answer sentences selected by matching content words in the question. WIKIQA is constructed using a more natural process and is more than an order of magnitude larger than the previous dataset. In addition, the WIKIQA dataset also includes questions for which there are no correct sentences, enabling researchers to work on answer triggering, a critical component in any QA system. We compare several systems on the task of answer sentence selection on both datasets and also describe the performance of a system on the problem of answer triggering using the WIKIQA dataset.", "date": "2015", "authors": ["Yi Yang 1, Wen-tau Yih 2, Christopher Meek 2"], "references": [2153579005, 2131744502, 2070246124, 2118091490, 1591825359, 1514986335, 2125313055, 2120735855, 2989499211, 2251921768]}, {"id": 2251349042, "title": "Learning to Automatically Solve Algebra Word Problems", "abstract": "We present an approach for automatically learning to solve algebra word problems. Our algorithm reasons across sentence boundaries to construct and solve a system of linear equations, while simultaneously recovering an alignment of the variables and numbers in these equations to the problem text. The learning algorithm uses varied supervision, including either full equations or just the final answers. We evaluate performance on a newly gathered corpus of algebra word problems, demonstrating that the system can correctly answer almost 70% of the questions in the dataset. This is, to our knowledge, the first learning result for this task.", "date": "2014", "authors": ["Nate Kushman 1, Yoav Artzi 2, Luke Zettlemoyer 2, Regina Barzilay 1"], "references": [2252136820, 1508977358, 2163561827, 2123661878, 1496189301, 2251673953, 2189089430, 2118781169, 1923162067, 1559723967]}, {"id": 2754835109, "title": "Modeling, Inference and Optimization with Composable Differentiable Procedures", "abstract": "", "date": "2016", "authors": ["Dougal Maclaurin"], "references": [1836465849, 1959608418, 3118608800, 1503398984, 104184427, 2165363188, 4919037, 2963355447, 1738124305, 2147800946]}, {"id": 1585773866, "title": "Evaluating Derivatives: Principles and Techniques of Algorithmic Differentiation", "abstract": "Algorithmic, or automatic, differentiation (AD) is a growing area of theoretical research and software development concerned with the accurate and efficient evaluation of derivatives for function evaluations given as computer programs. The resulting derivative values are useful for all scientific computations that are based on linear, quadratic, or higher order approximations to nonlinear scalar or vector functions. AD has been applied in particular to optimization, parameter identification, nonlinear equation solving, the numerical integration of differential equations, and combinations of these. Apart from quantifying sensitivities numerically, AD also yields structural dependence information, such as the sparsity pattern and generic rank of Jacobian matrices. The field opens up an exciting opportunity to develop new algorithms that reflect the true cost of accurate derivatives and to use them for improvements in speed and reliability. This second edition has been updated and expanded to cover recent developments in applications and theory, including an elegant NP completeness argument by Uwe Naumann and a brief introduction to scarcity, a generalization of sparsity. There is also added material on checkpointing and iterative differentiation. To improve readability the more detailed analysis of memory and complexity bounds has been relegated to separate, optional chapters.The book consists of three parts: a stand-alone introduction to the fundamentals of AD and its software; a thorough treatment of methods for sparse problems; and final chapters on program-reversal schedules, higher derivatives, nonsmooth problems and iterative processes. Each of the 15 chapters concludes with examples and exercises. Audience: This volume will be valuable to designers of algorithms and software for nonlinear computational problems. Current numerical software users should gain the insight necessary to choose and deploy existing AD software tools to the best advantage. Contents: Rules; Preface; Prologue; Mathematical Symbols; Chapter 1: Introduction; Chapter 2: A Framework for Evaluating Functions; Chapter 3: Fundamentals of Forward and Reverse; Chapter 4: Memory Issues and Complexity Bounds; Chapter 5: Repeating and Extending Reverse; Chapter 6: Implementation and Software; Chapter 7: Sparse Forward and Reverse; Chapter 8: Exploiting Sparsity by Compression; Chapter 9: Going beyond Forward and Reverse; Chapter 10: Jacobian and Hessian Accumulation; Chapter 11: Observations on Efficiency; Chapter 12: Reversal Schedules and Checkpointing; Chapter 13: Taylor and Tensor Coefficients; Chapter 14: Differentiation without Differentiability; Chapter 15: Implicit and Iterative Differentiation; Epilogue; List of Figures; List of Tables; Assumptions and Definitions; Propositions, Corollaries, and Lemmas; Bibliography; Index", "date": "1986", "authors": ["Andreas Griewank , Andrea Walther"], "references": [2899771611, 2963977107, 2144898279, 2749069611, 2093229042, 1973333099, 2093603746, 1977164425, 2963820353, 2568283272]}, {"id": 1579027943, "title": "Compiling fast partial derivatives of functions given by algorithms", "abstract": "If the gradient of the function y = f(x/sub 1/,..., x/sub n/) is desired, where f is given by an algoritym Af(x, n, y), most numerical analysts will use numerical differencing. This is a sampling scheme that approximates derivatives by the slope of secants in closely spaced points. Symbolic methods that make full use of the program text of Af should be able to come up with a better way to evaluate the gradient of F. The system Jake described produces gradients significantly faster than numerical differencing. Jake can handle algorithms Af with arbitrary flow of control. Measurements performed on one particular machine suggest that Jake is faster than numerical differencing for n > 8. Somewhat weaker results were obtained for the problem of computing Jacobians of arbitrary shape.", "date": "1979", "authors": ["Bert Speelpenning"], "references": [2899771611, 2962727772, 2963851840, 1994309501, 3022251767, 2046253206, 2080233464, 2188233853, 2020527870]}, {"id": 1902237438, "title": "Effective Approaches to Attention-based Neural Machine Translation", "abstract": "An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches on the WMT translation tasks between English and German in both directions. With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems that already incorporate known techniques such as dropout. Our ensemble model using different attention architectures yields a new state-of-the-art result in the WMT\u201915 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker. 1", "date": "2015", "authors": ["Minh-Thang Luong , Hieu Pham , Christopher D. Manning"], "references": [2964308564, 2130942839, 2157331557, 2101105183, 1514535095, 1753482797, 2153653739, 2100664567, 2962741254, 2147527908]}, {"id": 2124807415, "title": "Moses: Open Source Toolkit for Statistical Machine Translation", "abstract": "We describe an open-source toolkit for statistical machine translation whose novel contributions are (a) support for linguistically motivated factors, (b) confusion network decoding, and (c) efficient data formats for translation models and language models. In addition to the SMT decoder, the toolkit also includes a wide variety of tools for training, tuning and applying the system to many translation tasks.", "date": "2007", "authors": ["Philipp Koehn 1, Hieu Hoang 1, Alexandra Birch 1, Chris Callison-Burch 1, Marcello Federico 2, Nicola Bertoldi 2, Brooke Cowan 3, Wade Shen 3, Christine Moran 3, Richard Zens 4, Chris Dyer 5, Ondrej Bojar 6, Alexandra Constantin 7, Evan Herbst 8"], "references": [2101105183, 2153653739, 2156985047, 1631260214, 2146574666, 1498238796, 2113788796, 2105891181, 2056250865, 2130450156]}, {"id": 2251012068, "title": "Better Word Representations with Recursive Neural Networks for Morphology", "abstract": "Vector-space word representations have been very successful in recent years at improving performance across a variety of NLP tasks. However, common to most existing work, words are regarded as independent entities without any explicit relationship among morphologically related words being modeled. As a result, rare and complex words are often poorly estimated, and all unknown words are represented in a rather crude way using only one or a few vectors. This paper addresses this shortcoming by proposing a novel model that is capable of building representations for morphologically complex words from their morphemes. We combine recursive neural networks (RNNs), where each morpheme is a basic unit, with neural language models (NLMs) to consider contextual information in learning morphologicallyaware word representations. Our learned models outperform existing word representations by a good margin on word similarity tasks across many datasets, including a new dataset we introduce focused on rare words to complement existing ones in an interesting way.", "date": "2013", "authors": ["Thang Luong , Richard Socher , Christopher Manning"], "references": [2158899491, 2141599568, 2117130368, 179875071, 2132339004, 2158139315, 1423339008, 71795751, 2081580037, 1889268436]}, {"id": 2185175083, "title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "abstract": "We propose to use the visual denotations of linguistic expressions (i.e. the set of images they describe) to define novel denotational similarity metrics, which we show to be at least as beneficial as distributional similarities for two tasks that require semantic inference. To compute these denotational similarities, we construct a denotation graph, i.e. a subsumption hierarchy over constituents and their denotations, based on a large corpus of 30K images and 150K descriptive captions.", "date": "2014", "authors": ["Peter Young , Alice Lai , Micah Hodosh , Julia Hockenmaier"], "references": [2120779048, 2248026759, 1647729745, 2525127255, 1897761818, 2066134726, 2109586012, 1984052055, 2117805756, 2251861449]}, {"id": 2154359981, "title": "Baselines and Bigrams: Simple, Good Sentiment and Topic Classification", "abstract": "Variants of Naive Bayes (NB) and Support Vector Machines (SVM) are often used as baseline methods for text classification, but their performance varies greatly depending on the model variant, features used and task/dataset. We show that: (i) the inclusion of word bigram features gives consistent gains on sentiment analysis tasks; (ii) for short snippet sentiment tasks, NB actually does better than SVMs (while for longer documents the opposite result holds); (iii) a simple but novel SVM variant using NB log-count ratios as feature values consistently performs well across tasks and datasets. Based on these observations, we identify simple NB and SVM variants which outperform most published results on sentiment analysis datasets, sometimes providing a new state-of-the-art performance level.", "date": "2012", "authors": ["Sida Wang , Christopher Manning"], "references": [2118585731, 2117130368, 2160660844, 2113459411, 2114524997, 71795751, 1550206324, 2014902591, 2163455955, 2132166724]}, {"id": 3112605745, "title": "A Survey on Transfer Learning", "abstract": "", "date": "2010", "authors": ["Sinno Jialin Pan , Qiang Yang"], "references": [3014257857, 2996928369, 2520351113, 2404336876, 191607574, 3016409162, 2970575747, 3024355889, 3011505102, 2999888171]}, {"id": 2062118960, "title": "CNN Features Off-the-Shelf: An Astounding Baseline for Recognition", "abstract": "Recent results indicate that the generic descriptors extracted from the convolutional neural networks are very powerful. This paper adds to the mounting evidence that this is indeed the case. We report on a series of experiments conducted for different recognition tasks using the publicly available code and model of the OverFeat network which was trained to perform object classification on ILSVRC13. We use features extracted from the OverFeat network as a generic image representation to tackle the diverse range of recognition tasks of object image classification, scene recognition, fine grained recognition, attribute detection and image retrieval applied to a diverse set of datasets. We selected these tasks and datasets as they gradually move further away from the original task and data the OverFeat network was trained to solve. Astonishingly, we report consistent superior results compared to the highly tuned state-of-the-art systems in all the visual classification tasks on various datasets. For instance retrieval it consistently outperforms low memory footprint methods except for sculptures dataset. The results are achieved using a linear SVM classifier (or L2 distance in case of retrieval) applied to a feature representation of size 4096 extracted from a layer in the net. The representations are further modified using simple augmentation techniques e.g. jittering. The results strongly suggest that features obtained from deep learning with convolutional nets should be the primary candidate in most visual recognition tasks.", "date": "2014", "authors": ["Ali Sharif Razavian , Hossein Azizpour , Josephine Sullivan , Stefan Carlsson"], "references": []}, {"id": 2963012544, "title": "Character-level convolutional networks for text classification", "abstract": "This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classification. We constructed several large-scale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results. Comparisons are offered against traditional models such as bag of words, n-grams and their TFIDF variants, and deep learning models such as word-based ConvNets and recurrent neural networks.", "date": "2015", "authors": ["Xiang Zhang , Junbo Zhao , Yann LeCun"], "references": [2153579005, 2310919327, 1832693441, 1904365287, 2158899491, 2064675550, 1665214252, 104184427, 2149684865, 1689711448]}, {"id": 3098903812, "title": "Language Models are Few-Shot Learners", "abstract": "", "date": "2020", "authors": ["Tom B. Brown 1, Benjamin Mann , Nick Ryder 2, Melanie Subbiah , Jared Kaplan 3, Prafulla Dhariwal , Arvind Neelakantan 4, Pranav Shyam , Girish Sastry 1, Amanda Askell 1, Sandhini Agarwal , Ariel Herbert-Voss 1, Gretchen Krueger 1, Tom Henighan 1, Rewon Child 1, Aditya Ramesh 1, Daniel M. Ziegler 5, Jeffrey Wu 1, Clemens Winter , Christopher Hesse , Mark Chen 1, Eric Sigler , Mateusz Litwin , Scott Gray 1, Benjamin Chess 1, Jack Clark 1, Christopher Berner , Sam McCandlish 1, Alec Radford 1, Ilya Sutskever 1, Dario Amodei 6"], "references": [3100307207, 3100859887, 3118485687, 3120105373, 3120793869, 3098053103, 3108981297, 3088418428, 3101864923]}, {"id": 2980282514, "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing.", "abstract": "Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. \\textit{Transformers} is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. \\textit{Transformers} is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at \\url{this https URL}.", "date": "2019", "authors": ["Thomas Wolf , Lysandre Debut , Victor Sanh , Julien Chaumond , Clement Delangue , Anthony Moi , Pierric Cistac , Tim Rault , R\u00e9mi Louf , Morgan Funtowicz , Jamie Brew"], "references": [2964121744, 2963403868, 2963341956, 2962739339, 2251939518, 2899771611, 2963748441, 2123442489, 2965373594, 2970597249]}, {"id": 2970986510, "title": "Knowledge Enhanced Contextual Word Representations", "abstract": "Contextual word representations, typically trained on unstructured, unlabeled text, do not contain any explicit grounding to real world entities and are often unable to remember facts about those entities. We propose a general method to embed multiple knowledge bases (KBs) into large scale models, and thereby enhance their representations with structured, human-curated knowledge. For each KB, we first use an integrated entity linker to retrieve relevant entity embeddings, then update contextual word representations via a form of word-to-entity attention. In contrast to previous approaches, the entity linkers and self-supervised language modeling objective are jointly trained end-to-end in a multitask setting that combines a small amount of entity linking supervision with a large amount of raw text. After integrating WordNet and a subset of Wikipedia into BERT, the knowledge enhanced BERT (KnowBert) demonstrates improved perplexity, ability to recall facts as measured in a probing task and downstream performance on relationship extraction, entity typing, and word sense disambiguation. KnowBert\u2019s runtime is comparable to BERT\u2019s and it scales to large KBs.", "date": "2019", "authors": ["Matthew E. Peters 1, Mark Neumann 1, Robert L. Logan 2, Roy Schwartz 1, Vidur Joshi , Sameer Singh 2, Noah A. Smith 1"], "references": [2963403868, 2963341956, 2153579005, 1614298861, 2250539671, 2962739339, 2117130368, 2962784628, 2525778437, 2081580037]}, {"id": 3007332492, "title": "Benchmarking Graph Neural Networks", "abstract": "Graph neural networks (GNNs) have become the standard toolkit for analyzing and learning from data on graphs. As the field grows, it becomes critical to identify key architectures and validate new ideas that generalize to larger, more complex datasets. Unfortunately, it has been increasingly difficult to gauge the effectiveness of new models in the absence of a standardized benchmark with consistent experimental settings. In this paper, we introduce a reproducible GNN benchmarking framework, with the facility for researchers to add new models conveniently for arbitrary datasets. We demonstrate the usefulness of our framework by presenting a principled investigation into the recent Weisfeiler-Lehman GNNs (WL-GNNs) compared to message passing-based graph convolutional networks (GCNs) for a variety of graph tasks, i.e. graph regression/classification and node/link prediction, with medium-scale datasets.", "date": "2020", "authors": ["Vijay Prakash Dwivedi 1, Chaitanya K. Joshi 1, Thomas Laurent 2, Yoshua Bengio , Xavier Bresson 1"], "references": [2194775991, 2618530766, 2964121744, 2963403868, 1836465849, 2102605133, 2117539524, 1903029394, 2964308564, 2108598243]}, {"id": 3035503910, "title": "ERASER: A Benchmark to Evaluate Rationalized NLP Models.", "abstract": "State-of-the-art models in NLP are now predominantly based on deep neural networks that are opaque in terms of how they come to make predictions. This limitation has increased interest in designing more interpretable deep models for NLP that reveal the \u2018reasoning\u2019 behind model outputs. But work in this direction has been conducted on different datasets and tasks with correspondingly unique aims and metrics; this makes it difficult to track progress. We propose the Evaluating Rationales And Simple English Reasoning (ERASER a benchmark to advance research on interpretable models in NLP. This benchmark comprises multiple datasets and tasks for which human annotations of \u201crationales\u201d (supporting evidence) have been collected. We propose several metrics that aim to capture how well the rationales provided by models align with human rationales, and also how faithful these rationales are (i.e., the degree to which provided rationales influenced the corresponding predictions). Our hope is that releasing this benchmark facilitates progress on designing more interpretable NLP systems. The benchmark, code, and documentation are available at https://www.eraserbenchmark.com/", "date": "2020", "authors": ["Jay DeYoung 1, Sarthak Jain 1, Nazneen Fatema Rajani 2, Eric Lehman 1, Caiming Xiong 2, Richard Socher 2, Byron C. Wallace 1"], "references": [2964121744, 2963341956, 2964308564, 2095705004, 2250539671, 2157331557, 2031489346, 2064675550, 2516809705, 1840435438]}, {"id": 1880262756, "title": "Latent dirichlet allocation", "abstract": "We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.", "date": "2003", "authors": ["David M. Blei 1, Andrew Y. Ng 2, Michael I. Jordan 1"], "references": [2045656233, 2147152072, 2107743791, 2097089247, 1956559956, 1516111018, 1508165687, 1746680969, 2020842694, 2063392856]}, {"id": 3101045333, "title": "MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers", "abstract": "", "date": "2019", "authors": ["Wenhui Wang 1, Furu Wei 1, Li Dong 2, Hangbo Bao 3, Nan Yang 1, Ming Zhou 1"], "references": []}, {"id": 3100985894, "title": "GOBO: Quantizing Attention-Based NLP Models for Low Latency and Energy Efficient Inference", "abstract": "Attention-based models have demonstrated remarkable success in various natural language understanding tasks. However, efficient execution remains a challenge for these models which are memory-bound due to their massive number of parameters. We present GOBO, a model quantization technique that compresses the vast majority (typically 99.9%) of the 32-bit floating-point parameters of state-of-the-art BERT models and their variants to 3 bits while maintaining their accuracy. Unlike other quantization methods, GOBO does not require fine-tuning nor retraining to compensate for the quantization error. We present two practical hardware applications of GOBO. In the first GOBO reduces memory storage and traffic and as a result inference latency and energy consumption. This GOBO memory compression mechanism is plug-in compatible with many architectures; we demonstrate it with the TPU, Eyeriss, and an architecture using Tensor Cores-like units. Second, we present a co-designed hardware architecture that also reduces computation. Uniquely, the GOBO architecture maintains most of the weights in 3b even during computation, a property that: (i) makes the processing elements area efficient, allowing us to pack more compute power per unit area, (ii) replaces most multiply-accumulations with additions, and (iii) reduces the off-chip traffic by amplifying on-chip memory capacity.", "date": "2020", "authors": ["Ali Hadi Zadeh , Isak Edo , Omar Mohamed Awad , Andreas Moshovos"], "references": [2963341956, 2963748441, 2964299589, 2965373594, 2285660444, 2923014074, 2625745261, 2978017171, 3011411500, 2162639668]}, {"id": 3022810465, "title": "GOBO: Quantizing Attention-Based NLP Models for Low Latency and Energy Efficient Inference", "abstract": "Attention-based models have demonstrated remarkable success in various natural language understanding tasks. However, efficient execution remains a challenge for these models which are memory-bound due to their massive number of parameters. We present a model quantization technique that compresses the vast majority (typically 99.9%) of the 32-bit floating-point parameters of state-of-the-art BERT models and its variants to 3 bits while maintaining their accuracy. Unlike other quantization methods, our technique does not require fine-tuning nor retraining to compensate for the quantization error.", "date": "2020", "authors": ["Ali Hadi Zadeh , Andreas Moshovos"], "references": [2963341956, 2963748441, 2964299589, 2965373594, 2606722458, 2285660444, 2923014074, 2625745261, 2978017171, 3011411500]}, {"id": 3117450517, "title": "A Survey on Visual Transformer", "abstract": "Transformer is a type of deep neural network mainly based on self-attention mechanism which is originally applied in natural language processing field. Inspired by the strong representation ability of transformer, researchers propose to extend transformer for computer vision tasks. Transformer-based models show competitive and even better performance on various visual benchmarks compared to other network types such as convolutional networks and recurrent networks. In this paper we provide a literature review of these visual transformer models by categorizing them in different tasks and analyze the advantages and disadvantages of these methods. In particular, the main categories include the basic image classification, high-level vision, low-level vision and video processing. Self-attention in computer vision is also briefly revisited as self-attention is the base component in transformer. Efficient transformer methods are included for pushing transformer into real applications. Finally, we give a discussion about the further research directions for visual transformer.", "date": "2020", "authors": ["Kai Han 1, Yunhe Wang 1, Hanting Chen 1, Xinghao Chen 1, Jianyuan Guo 1, Zhenhua Liu 1, Yehui Tang 1, An Xiao 1, Chunjing Xu 2, Yixing Xu 2, Zhaohui Yang 2, Yiman Zhang 2, Dacheng Tao 3"], "references": [2194775991, 2618530766, 639708223, 2963403868, 2963341956, 2964308564, 1861492603, 2310919327, 2884561390, 2064675550]}, {"id": 3015609966, "title": "DynaBERT: Dynamic BERT with Adaptive Width and Depth.", "abstract": "The pre-trained language models like BERT and RoBERTa, though powerful in many natural language processing tasks, are both computational and memory expensive. To alleviate this problem, one approach is to compress them for specific tasks before deployment. However, recent works on BERT compression usually reduce the large BERT model to a fixed smaller size, and can not fully satisfy the requirements of different edge devices with various hardware performances. In this paper, we propose a novel dynamic BERT model (abbreviated as DynaBERT), which can run at adaptive width and depth. The training process of DynaBERT includes first training a width-adaptive BERT and then allows both adaptive width and depth, by distilling knowledge from the full-sized model to small sub-networks. Network rewiring is also used to keep the more important attention heads and neurons shared by more sub-networks. Comprehensive experiments under various efficiency constraints demonstrate that our proposed dynamic BERT (or RoBERTa) at its largest size has comparable performance as BERT (or RoBERTa), while at smaller widths and depths consistently outperforms existing BERT compression methods.", "date": "2020", "authors": ["Lu Hou 1, Lifeng Shang 1, Xin Jiang 1, Qun Liu 2"], "references": [2963403868, 2963341956, 2965373594, 2923014074, 2996428491, 2259472270, 2978017171, 2963287528, 2996035354, 2963748792]}, {"id": 3103473439, "title": "Quantifying the Evaluation of Heuristic Methods for Textual Data Augmentation.", "abstract": "Data augmentation has been shown to be effective in providing more training data for machine learning and resulting in more robust classifiers. However, for some problems, there may be multiple augmentation heuristics, and the choices of which one to use may significantly impact the success of the training. In this work, we propose a metric for evaluating augmentation heuristics; specifically, we quantify the extent to which an example is \u201chard to distinguish\u201d by considering the difference between the distribution of the augmented samples of different classes. Experimenting with multiple heuristics in two prediction tasks (positive/negative sentiment and verbosity/conciseness) validates our claims by revealing the connection between the distribution difference of different classes and the classification accuracy.", "date": "2020", "authors": ["Omid Kashefi , Rebecca Hwa"], "references": [2963341956, 2157331557, 1832693441, 2131744502, 2251939518, 2160660844, 2963012544, 2081580037, 2963216553, 2963399829]}, {"id": 3115789797, "title": "Towards Non-task-specific Distillation of BERT via Sentence Representation Approximation", "abstract": "Recently, BERT has become an essential ingredient of various NLP deep models due to its effectiveness and universal-usability. However, the online deployment of BERT is often blocked by its large-scale parameters and high computational cost. There are plenty of studies showing that the knowledge distillation is efficient in transferring the knowledge from BERT into the model with a smaller size of parameters. Nevertheless, current BERT distillation approaches mainly focus on task-specified distillation, such methodologies lead to the loss of the general semantic knowledge of BERT for universal-usability. In this paper, we propose a sentence representation approximating oriented distillation framework that can distill the pre-trained BERT into a simple LSTM based model without specifying tasks. Consistent with BERT, our distilled model is able to perform transfer learning via fine-tuning to adapt to any sentence-level downstream task. Besides, our model can further cooperate with task-specific distillation procedures. The experimental results on multiple NLP tasks from the GLUE benchmark show that our approach outperforms other task-specific distillation methods or even much larger models, i.e., ELMO, with efficiency well-improved.", "date": "2020", "authors": ["Bowen Wu 1, Huan Zhang 1, Mengyuan Li 1, Zongsheng Wang 1, Qihang Feng 1, Junhong Huang 2, Baoxun Wang 2"], "references": [2964121744, 2963341956, 2250539671, 2962739339, 2251939518, 1821462560, 2970597249, 2963026768, 2923014074, 2963846996]}, {"id": 3110662498, "title": "MiniVLM: A Smaller and Faster Vision-Language Model.", "abstract": "Recent vision-language (VL) studies have shown remarkable progress by learning generic representations from massive image-text pairs with transformer models and then fine-tuning on downstream VL tasks. While existing research has been focused on achieving high accuracy with large pre-trained models, building a lightweight model is of great value in practice but is less explored. In this paper, we propose a smaller and faster VL model, MiniVLM, which can be finetuned with good performance on various downstream tasks like its larger counterpart. MiniVLM consists of two modules, a vision feature extractor and a transformer-based vision-language fusion module. We design a Two-stage Efficient feature Extractor (TEE), inspired by the one-stage EfficientDet network, to significantly reduce the time cost of visual feature extraction by $95\\%$, compared to a baseline model. We adopt the MiniLM structure to reduce the computation cost of the transformer module after comparing different compact BERT models. In addition, we improve the MiniVLM pre-training by adding $7M$ Open Images data, which are pseudo-labeled by a state-of-the-art captioning model. We also pre-train with high-quality image tags obtained from a strong tagging model to enhance cross-modality alignment. The large models are used offline without adding any overhead in fine-tuning and inference. With the above design choices, our MiniVLM reduces the model size by $73\\%$ and the inference time cost by $94\\%$ while being able to retain $94-97\\%$ of the accuracy on multiple VL tasks. We hope that MiniVLM helps ease the use of the state-of-the-art VL research for on-the-edge applications.", "date": "2020", "authors": ["Jianfeng Wang , Xiaowei Hu , Pengchuan Zhang , Xiujun Li , Lijuan Wang , Lei Zhang , Jianfeng Gao , Zicheng Liu"], "references": [2194775991, 639708223, 2963403868, 2963341956, 2117539524, 3106250896, 2963037989, 2806070179, 2884561390, 2565639579]}, {"id": 3106429660, "title": "DiPair: Fast and Accurate Distillation for Trillion-Scale Text Matching and Pair Modeling", "abstract": "Pre-trained models like BERT ((Devlin et al., 2018) have dominated NLP / IR applications such as single sentence classification, text pair classification, and question answering. However, deploying these models in real systems is highly non-trivial due to their exorbitant computational costs. A common remedy to this is knowledge distillation (Hinton et al., 2015), leading to faster inference. However \u2013 as we show here \u2013 existing works are not optimized for dealing with pairs (or tuples) of texts. Consequently, they are either not scalable or demonstrate subpar performance. In this work, we propose DiPair \u2014 a novel framework for distilling fast and accurate models on text pair tasks. Coupled with an end-to-end training strategy, DiPair is both highly scalable and offers improved quality-speed tradeoffs. Empirical studies conducted on both academic and real-world e-commerce benchmarks demonstrate the efficacy of the proposed approach with speedups of over 350x and minimal quality drop relative to the cross-attention teacher BERT model.", "date": "2020", "authors": ["Jiecao Chen , Liu Yang , Karthik Raman , Michael Bendersky , Jung-Jung Yeh , Yun Zhou , Marc Najork , Danyang Cai , Ehsan Emadzadeh"], "references": [2963403868, 2963341956, 2612445135, 2964299589, 1821462560, 1840435438, 2965373594, 2279098554, 2136189984, 2996428491]}, {"id": 2525778437, "title": "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation", "abstract": "Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units (\"wordpieces\") for both input and output. This method provides a good balance between the flexibility of \"character\"-delimited models and the efficiency of \"word\"-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60% compared to Google's phrase-based production system.", "date": "2016", "authors": ["Yonghui Wu , Mike Schuster , Zhifeng Chen , Quoc V. Le , Mohammad Norouzi , Wolfgang Macherey , Maxim Krikun , Yuan Cao , Qin Gao , Klaus Macherey , Jeff Klingner , Apurva Shah , Melvin Johnson , Xiaobing Liu , \u0141ukasz Kaiser , Stephan Gouws , Yoshikiyo Kato , Taku Kudo , Hideto Kazawa , Keith Stevens , George Kurian , Nishant Patil , Wei Wang , Cliff Young , Jason Smith , Jason Riesa , Alex Rudnick , Oriol Vinyals , Greg Corrado , Macduff Hughes , Jeffrey Dean"], "references": [2964121744, 2121879602, 2251743902, 1968594024]}, {"id": 2963626623, "title": "Bag of Tricks for Efficient Text Classification", "abstract": "This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore CPU, and classify half a million sentences among 312K classes in less than a minute.", "date": "2017", "authors": ["Armand Joulin 1, Edouard Grave 2, Piotr Bojanowski 1, Tomas Mikolov 1"], "references": [1614298861, 1832693441, 2097726431, 2118585731, 2149684865, 2963012544, 2147152072, 2250966211, 1550206324, 1615991656]}, {"id": 2164777277, "title": "The measurement of observer agreement for categorical data", "abstract": "This paper presents a general statistical methodology for the analysis of multivariate categorical data arising from observer reliability studies. The procedure essentially involves the construction of functions of the observed proportions which are directed at the extent to which the observers agree among themselves and the construction of test statistics for hypotheses involving these functions. Tests for interobserver bias are presented in terms of first-order marginal homogeneity and measures of interobserver agreement are developed as generalized kappa-type statistics. These procedures are illustrated with a clinical diagnosis example from the epidemiological literature.", "date": "1977", "authors": ["J. R. Landis , Gary G Koch"], "references": [2053154970, 2798510847, 2037789405, 1975879668, 1989774929, 2021142183, 2018385240, 2133012565, 2063803293, 2088041869]}, {"id": 3024622987, "title": "COVID-Twitter-BERT: A Natural Language Processing Model to Analyse COVID-19 Content on Twitter", "abstract": "In this work, we release COVID-Twitter-BERT (CT-BERT), a transformer-based model, pretrained on a large corpus of Twitter messages on the topic of COVID-19. Our model shows a 10-30% marginal improvement compared to its base model, BERT-Large, on five different classification datasets. The largest improvements are on the target domain. Pretrained transformer models, such as CT-BERT, are trained on a specific target domain and can be used for a wide variety of natural language processing tasks, including classification, question-answering and chatbots. CT-BERT is optimised to be used on COVID-19 content, in particular social media posts from Twitter.", "date": "2020", "authors": ["Martin M\u00fcller , Marcel Salath\u00e9 , Per Egil Kummervold"], "references": [2963403868, 2963341956, 2251939518, 2965373594, 2996428491, 2911489562, 2163455955, 2970771982, 2752201871, 2922551710]}, {"id": 1975879668, "title": "Measuring nominal scale agreement among many raters.", "abstract": "", "date": "1970", "authors": ["Joseph L. Fleiss"], "references": [2053154970, 2037789405, 1979773093, 2015793039, 2026715452, 2051470741, 2020954436]}, {"id": 2125980212, "title": "Fast-join: An efficient method for fuzzy token matching based string similarity join", "abstract": "String similarity join that finds similar string pairs between two string sets is an essential operation in many applications, and has attracted significant attention recently in the database community. A significant challenge in similarity join is to implement an effective fuzzy match operation to find all similar string pairs which may not match exactly. In this paper, we propose a new similarity metrics, called \u201cfuzzy token matching based similarity\u201d, which extends token-based similarity functions (e.g., Jaccard similarity and Cosine similarity) by allowing fuzzy match between two tokens. We study the problem of similarity join using this new similarity metrics and present a signature-based method to address this problem. We propose new signature schemes and develop effective pruning techniques to improve the performance. Experimental results show that our approach achieves high efficiency and result quality, and significantly outperforms state-of-the-art methods.", "date": "2011", "authors": ["Jiannan Wang , Guoliang Li , Jianhua Fe"], "references": [2097776316, 2121516976, 2151930506, 2097184821, 2096598900, 2105423800, 2161936973, 2127675794, 2167847032, 2150916025]}, {"id": 3104186312, "title": "BERTweet: A pre-trained language model for English Tweets", "abstract": "We present BERTweet, the first public large-scale pre-trained language model for English Tweets. Our BERTweet, having the same architecture as BERT-base (Devlin et al., 2019), is trained using the RoBERTa pre-training procedure (Liu et al., 2019). Experiments show that BERTweet outperforms strong baselines RoBERTa-base and XLM-R-base (Conneau et al., 2020), producing better performance results than the previous state-of-the-art models on three Tweet NLP tasks: Part-of-speech tagging, Named-entity recognition and text classification. We release BERTweet under the MIT License to facilitate future research and applications on Tweet data. Our BERTweet is available at https://github.com/VinAIResearch/BERTweet", "date": "2020", "authors": ["Dat Quoc Nguyen 1, Thanh Vu 2, Anh Tuan Nguyen 3"], "references": [2964121744, 2963403868, 2963341956, 2962784628, 2965373594, 2963626623, 1632114991, 2153848201, 2911489562, 2157765050]}, {"id": 3105987139, "title": "NLP North at WNUT-2020 Task 2: Pre-training versus Ensembling for Detection of Informative COVID-19 English Tweets.", "abstract": "With the COVID-19 pandemic raging world-wide since the beginning of the 2020 decade, the need for monitoring systems to track relevant information on social media is vitally important. This paper describes our submission to the WNUT-2020 Task 2: Identification of informative COVID-19 English Tweets. We investigate the effectiveness for a variety of classification models, and found that domain-specific pre-trained BERT models lead to the best performance. On top of this, we attempt a variety of ensembling strategies, but these attempts did not lead to further improvements. Our final best model, the standalone CT-BERT model, proved to be highly competitive, leading to a shared first place in the shared task. Our results emphasize the importance of domain and task-related pre-training.", "date": "2020", "authors": ["Anders Giovanni M\u00f8ller , Rob van der Goot 1, Barbara Plank 2"], "references": [2963341956, 2250539671, 2911964244, 2962739339, 2965373594, 1566289585, 2121879602, 3034238904, 3099342932, 2620806258]}, {"id": 2972324944, "title": "What Does BERT Look at? An Analysis of BERT\u2019s Attention", "abstract": "Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data. Most recent analysis has focused on model outputs (e.g., language model surprisal) or internal vector representations (e.g., probing classifiers). Complementary to these works, we propose methods for analyzing the attention mechanisms of pre-trained models and apply them to BERT. BERT\u2019s attention heads exhibit patterns such as attending to delimiter tokens, specific positional offsets, or broadly attending over the whole sentence, with heads in the same layer often exhibiting similar behaviors. We further show that certain attention heads correspond well to linguistic notions of syntax and coreference. For example, we find heads that attend to the direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions with remarkably high accuracy. Lastly, we propose an attention-based probing classifier and use it to further demonstrate that substantial syntactic information is captured in BERT\u2019s attention.", "date": "2019", "authors": ["Kevin Clark 1, Urvashi Khandelwal 1, Omer Levy 1, Christopher D. Manning 2"], "references": [2963403868, 2963341956, 2964308564, 2250539671, 2962739339, 2962784628, 1632114991, 2170973209, 2594633041, 2963751529]}, {"id": 3011411500, "title": "SpanBERT: Improving Pre-training by Representing and Predicting Spans", "abstract": "We present SpanBERT, a pre-training method that is designed to better represent and predict spans of text. Our approach extends BERT by (1) masking contiguous random spans, rather than random token...", "date": "2020", "authors": ["Mandar Joshi 1, Danqi Chen 2, Yinhan Liu 3, Daniel S. Weld 1, Luke Zettlemoyer 1, Omer Levy 3"], "references": [2964121744, 2963403868, 2963341956, 2962739339, 2963748441, 2965373594, 2963026768, 1486649854, 3037932933, 2963846996]}, {"id": 2914120296, "title": "Cross-lingual Language Model Pretraining.", "abstract": "Recent studies have demonstrated the efficiency of generative pretraining for English natural language understanding. In this work, we extend this approach to multiple languages and show the effectiveness of cross-lingual pretraining. We propose two methods to learn cross-lingual language models (XLMs): one unsupervised that only relies on monolingual data, and one supervised that leverages parallel data with a new cross-lingual language model objective. We obtain state-of-the-art results on cross-lingual classification, unsupervised and supervised machine translation. On XNLI, our approach pushes the state of the art by an absolute gain of 4.9% accuracy. On unsupervised machine translation, we obtain 34.3 BLEU on WMT'16 German-English, improving the previous state of the art by more than 9 BLEU. On supervised machine translation, we obtain a new state of the art of 38.5 BLEU on WMT'16 Romanian-English, outperforming the previous best approach by more than 4 BLEU. Our code and pretrained models will be made publicly available.", "date": "2019", "authors": ["Guillaume Lample , Alexis Conneau"], "references": [2964121744, 2963403868, 2963341956, 2153579005, 2064675550, 2251939518, 2493916176, 2899771611, 2962784628, 1840435438]}, {"id": 2891555348, "title": "XNLI: Evaluating Cross-lingual Sentence Representations", "abstract": "State-of-the-art natural language processing systems rely on supervision in the form of annotated data to learn competent models. These models are generally trained on data in a single language (usually English), and cannot be directly used beyond that language. Since collecting data in every language is not realistic, there has been a growing interest in cross-lingual language understanding (XLU) and low-resource cross-language transfer. In this work, we construct an evaluation set for XLU by extending the development and test sets of the Multi-Genre Natural Language Inference Corpus (MultiNLI) to 14 languages, including low-resource languages such as Swahili and Urdu. We hope that our dataset, dubbed XNLI, will catalyze research in cross-lingual sentence understanding by providing an informative standard evaluation task. In addition, we provide several baselines for multilingual sentence understanding, including two based on machine translation systems, and two that use parallel data to train aligned multilingual bag-of-words and LSTM encoders. We find that XNLI represents a practical and challenging evaluation suite, and that directly translating the test data yields the best performance among available baselines.", "date": "2018", "authors": ["Alexis Conneau 1, Ruty Rinott 1, Guillaume Lample 1, Adina Williams 2, Samuel R. Bowman 1, Holger Schwenk 1, Veselin Stoyanov 1"], "references": [2964121744, 2153579005, 2095705004, 2130942839, 2962739339, 2064675550, 2131744502, 2117130368, 1840435438, 1486649854]}, {"id": 2952638691, "title": "How multilingual is Multilingual BERT", "abstract": "In this paper, we show that Multilingual BERT (M-BERT), released by Devlin et al. (2018) as a single language model pre-trained from monolingual corpora in 104 languages, is surprisingly good at zero-shot cross-lingual model transfer, in which task-specific annotations in one language are used to fine-tune the model for evaluation in another language. To understand why, we present a large number of probing experiments, showing that transfer is possible even to languages in different scripts, that transfer works best between typologically similar languages, that monolingual corpora can train models for code-switching, and that the model can find translation pairs. From these results, we can conclude that M-BERT does create multilingual representations, but that these representations exhibit systematic deficiencies affecting certain language pairs.", "date": "2019", "authors": ["Telmo Pires , Eva Schlinger , Dan Garrette"], "references": [2963341956, 2962739339, 2144578941, 2888329843, 2946417913, 2740840489, 2915429162, 2508316494, 2115774663, 2963285772]}, {"id": 2962795068, "title": "Cross-lingual Models of Word Embeddings: An Empirical Comparison", "abstract": "", "date": "2015", "authors": ["Shyam Upadhyay 1, Manaal Faruqui 2, Chris Dyer 2, Dan Roth 3"], "references": [1614298861, 22168010, 1978394996, 2100235303, 2126725946, 2148708890, 2118090838, 1854884267, 2053921957, 2027979924]}, {"id": 2154474435, "title": "Recognizing Textual Entailment: Models and Applications", "abstract": "* List of Figures* List of Tables* Preface* Acknowledgments* Textual Entailment* Architectures and Approaches* Alignment, Classification, and Learning* Case Studies* Knowledge Acquisition for Textual Entailment* Research Directions in RTE* Bibliography* Authors' Biographies", "date": "2013", "authors": ["Ido Dagan 1, Dan Roth 2, Mark Sammons 2, Fabio Massimo Zanzotto 3"], "references": [1880262756, 2119821739, 2135029798, 2081580037, 1970381522, 2097606805, 2008652694, 1512387364, 4508078, 2144578941]}, {"id": 2573062194, "title": "LORELEI Language Packs: Data, Tools, and Resources for Technology Development in Low Resource Languages", "abstract": "In this paper, we describe the textual linguistic resources in nearly 3 dozen languages being produced by Linguistic Data Consortium for DARPA\u2019s LORELEI (Low Resource Languages for Emergent Incidents) Program. The goal of LORELEI is to improve the performance of human language technologies for low-resource languages and enable rapid re-training of such technologies for new languages, with a focus on the use case of deployment of resources in sudden emergencies such as natural disasters. Representative languages have been selected to provide broad typological coverage for training, and surprise incident languages for testing will be selected over the course of the program. Our approach treats the full set of language packs as a coherent whole, maintaining LORELEI-wide specifications, tagsets, and guidelines, while allowing for adaptation to the specific needs created by each language. Each representative language corpus, therefore, both stands on its own as a resource for the specific language and forms part of a large multilingual resource for broader cross-language technology development.", "date": "2016", "authors": ["Stephanie M. Strassel , Jennifer Tracey"], "references": [2740132093, 2995015695, 2971120622, 2760424551, 2963431393, 3098466758]}, {"id": 1832693441, "title": "Convolutional Neural Networks for Sentence Classification", "abstract": "We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks. We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-specific vectors through fine-tuning offers further gains in performance. We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification.", "date": "2014", "authors": ["Yoon Kim"], "references": [2618530766, 2153579005, 2146502635, 2310919327, 1904365287, 2158899491, 2143612262, 2251939518, 2062118960, 2160660844]}, {"id": 2145287260, "title": "DeepFace: Closing the Gap to Human-Level Performance in Face Verification", "abstract": "In modern face recognition, the conventional pipeline consists of four stages: detect => align => represent => classify. We revisit both the alignment step and the representation step by employing explicit 3D face modeling in order to apply a piecewise affine transformation, and derive a face representation from a nine-layer deep neural network. This deep network involves more than 120 million parameters using several locally connected layers without weight sharing, rather than the standard convolutional layers. Thus we trained it on the largest facial dataset to-date, an identity labeled dataset of four million facial images belonging to more than 4, 000 identities. The learned representations coupling the accurate model-based alignment with the large facial database generalize remarkably well to faces in unconstrained environments, even with a simple classifier. Our method reaches an accuracy of 97.35% on the Labeled Faces in the Wild (LFW) dataset, reducing the error of the current state of the art by more than 27%, closely approaching human-level performance.", "date": "2014", "authors": ["Yaniv Taigman 1, Ming Yang 1, Marc'Aurelio Ranzato 1, Lior Wolf 2"], "references": [2618530766, 2310919327, 2168231600, 2072128103, 1782590233, 2163808566, 2047508432, 1498436455, 2536626143, 1976948919]}, {"id": 1970381522, "title": "Cheap and Fast -- But is it Good? Evaluating Non-Expert Annotations for Natural Language Tasks", "abstract": "Human linguistic annotation is crucial for many natural language processing tasks but can be expensive and time-consuming. We explore the use of Amazon's Mechanical Turk system, a significantly cheaper and faster method for collecting annotations from a broad base of paid non-expert contributors over the Web. We investigate five tasks: affect recognition, word similarity, recognizing textual entailment, event temporal ordering, and word sense disambiguation. For all five, we show high agreement between Mechanical Turk non-expert annotations and existing gold standard labels provided by expert labelers. For the task of affect recognition, we also show that using non-expert labels for training machine learning algorithms can be as effective as using gold standard annotations from experts. We propose a technique for bias correction that significantly improves annotation quality on two tasks. We conclude that many large labeling tasks can be effectively designed and carried out in this method at a fraction of the usual expense.", "date": "2008", "authors": ["Rion Snow 1, Brendan O'Connor 2, Daniel Jurafsky 1, Andrew Ng 1"], "references": [2141282920, 1632114991, 2151401338, 2158847908, 2525127255, 2125943921, 2149489787, 2115792525, 1659833910, 2130158090]}, {"id": 1933349210, "title": "VQA: Visual Question Answering", "abstract": "We propose the task of free-form and open-ended Visual Question Answering (VQA). Given an image and a natural language question about the image, the task is to provide an accurate natural language answer. Mirroring real-world scenarios, such as helping the visually impaired, both the questions and answers are open-ended. Visual questions selectively target different areas of an image, including background details and underlying context. As a result, a system that succeeds at VQA typically needs a more detailed understanding of the image and complex reasoning than a system producing generic image captions. Moreover, VQA is amenable to automatic evaluation, since many open-ended answers contain only a few words or a closed set of answers that can be provided in a multiple-choice format. We provide a dataset containing ~0.25M images, ~0.76M questions, and ~10M answers (www.visualqa.org), and discuss the information it provides. Numerous baselines for VQA are provided and compared with human performance.", "date": "2015", "authors": ["Stanislaw Antol 1, Aishwarya Agrawal 1, Jiasen Lu 1, Margaret Mitchell 2, Dhruv Batra 3, C. Lawrence Zitnick 4, Devi Parikh 3"], "references": [2618530766, 2962835968, 2153579005, 2155893237, 1861492603, 1895577753, 2481240925, 1947481528, 2952122856, 1486649854]}, {"id": 2413794162, "title": "A Decomposable Attention Model for Natural Language Inference", "abstract": "", "date": "2016", "authors": ["Ankur P. Parikh 1, Oscar Tackstrom 2, Dipanjan Das 2, Jakob Uszkoreit 2"], "references": [2964308564, 2095705004, 2250539671, 2146502635, 2064675550, 1840435438, 2156387975, 1916559533, 2170738476, 2211192759]}, {"id": 2963969878, "title": "Adversarial Examples for Evaluating Reading Comprehension Systems", "abstract": "Standard accuracy metrics indicate that reading comprehension systems are making rapid progress, but the extent to which these systems truly understand language remains unclear. To reward systems with real language understanding abilities, we propose an adversarial evaluation scheme for the Stanford Question Answering Dataset (SQuAD). Our method tests whether systems can answer questions about paragraphs that contain adversarially inserted sentences, which are automatically generated to distract computer systems without changing the correct answer or misleading humans. In this adversarial setting, the accuracy of sixteen published models drops from an average of 75% F1 score to 36%; when the adversary is allowed to add ungrammatical sequences of words, average accuracy on four models decreases further to 7%. We hope our insights will motivate the development of new models that understand language more precisely.", "date": "2017", "authors": ["Robin Jia , Percy Liang"], "references": [2099471712, 2250539671, 2963207607, 2964153729, 2038721957, 2963748441, 2123442489, 2551396370, 2963223306, 2543927648]}, {"id": 2123442489, "title": "The Stanford CoreNLP Natural Language Processing Toolkit", "abstract": "We describe the design and use of the Stanford CoreNLP toolkit, an extensible pipeline that provides core natural language analysis. This toolkit is quite widely used, both in the research NLP community and also among commercial and government users of open source NLP technology. We suggest that this follows from a simple, approachable design, straightforward interfaces, the inclusion of robust and good quality analysis components, and not requiring use of a large amount of associated baggage.", "date": "2014", "authors": ["Christopher Manning 1, Mihai Surdeanu 2, John Bauer 1, Jenny Finkel 1, Steven Bethard 3, David McClosky 4"], "references": [2251939518, 2096765155, 1521626219, 1508977358, 1996430422, 2096797897, 2147218300, 2250789336, 2251758222, 2167072947]}, {"id": 2912565176, "title": "Fuzzy sets", "abstract": "", "date": "1996", "authors": ["Lotfi A. Zadeh"], "references": [2140190241, 1992419399, 2153233077, 2186428165, 2038420319, 2143451122, 2342408547, 2151498684, 2061240006]}, {"id": 2115792525, "title": "The Berkeley FrameNet Project", "abstract": "FrameNet is a three-year NSF-supported project in corpus-based computational lexicography, now in its second year (NSF IRI-9618838, \"Tools for Lexicon Building\"). The project's key features are (a) a commitment to corpus evidence for semantic and syntactic generalizations, and (b) the representation of the valences of its target words (mostly nouns, adjectives, and verbs) in which the semantic portion makes use of frame semantics. The resulting database will contain (a) descriptions of the semantic frames underlying the meanings of the words described, and (b) the valence representation (semantic and syntactic) of several thousand words and phrases, each accompanied by (c) a representative collection of annotated corpus attestations, which jointly exemplify the observed linkings between \"frame elements\" and their syntactic realizations (e.g. grammatical function, phrase type, and other syntactic traits). This report will present the project's goals and workflow, and information about the computational tools that have been adapted or created in-house for this work.", "date": "1998", "authors": ["Collin F. Baker , Charles J. Fillmore , John B. Lowe"], "references": [3088277579, 2154890447]}, {"id": 2102065370, "title": "Evaluating Content Selection in Summarization: The Pyramid Method", "abstract": "", "date": "2003", "authors": ["Ani Nenkova , Rebecca J. Passonneau"], "references": [2101105183, 2150824314, 3021916629, 2138180048, 1977747299, 2160204597, 2137591918, 1985463960, 2169401582, 2795696357]}, {"id": 2396767181, "title": "The Seventh PASCAL Recognizing Textual Entailment Challenge.", "abstract": "", "date": "2007", "authors": ["Luisa Bentivogli , Peter Clark , Ido Dagan , Danilo Giampiccolo"], "references": [2525127255, 2130158090, 2102065370, 2182572585, 1971714853]}, {"id": 2002664886, "title": "Nonparametric statistics for the behavioral sciences", "abstract": "This is the revision of the classic text in the field, adding two new chapters and thoroughly updating all others. The original structure is retained, and the book continues to serve as a combined text/reference.", "date": "1955", "authors": ["Sidney Siegel"], "references": [2624431344, 2158847908, 2155243985, 2112422413, 1531237901, 2061504941, 2151170651, 2148540129, 2157289187]}, {"id": 1990524510, "title": "Measuring the Semantic Similarity of Texts", "abstract": "This paper presents a knowledge-based method for measuring the semantic-similarity of texts. While there is a large body of previous work focused on finding the semantic similarity of concepts and words, the application of these word-oriented methods to text similarity has not been yet explored. In this paper, we introduce a method that combines word-to-word similarity metrics into a text-to-text metric, and we show that this method outperforms the traditional text similarity metrics based on lexical matching.", "date": "2005", "authors": ["Courtney Corley , Rada Mihalcea"], "references": [2101105183, 2158997610, 1978394996, 1647729745, 2525127255, 2150824314, 2100935296, 2117805756, 1569415500, 2136480620]}, {"id": 137514618, "title": "PROBABILISTIC TEXTUAL ENTAILMENT: GENERIC APPLIED MODELING OF LANGUAGE VARIABILITY", "abstract": "", "date": "2003", "authors": ["Ido Dagan , Oren Glickman"], "references": [2048679005, 2103931177, 2167435923, 2785349534, 2126034021, 2107130271, 2129468719, 2118021410, 2883783597, 1520917717]}, {"id": 2182572585, "title": "Overview of the TAC 2008 Update Summarization Task.", "abstract": "The summarization track at the Text Analysis Conference (TAC) is a direct continuation of the Document Understanding Conference (DUC) series of workshops, focused on providing common data and evaluation framework for research in automatic summarization. In the TAC 2008 summarization track, the main task was to produce two 100-word summaries from two related sets of 10 documents, where the second summary was an update summary. While all of the 71 submitted runs were automatically scored with the ROUGE and BE metrics, NIST assessors manually evaluated only 57 of the submitted runs for readability, content, and overall responsiveness.", "date": "2007", "authors": ["Hoa Trang Dang , Karolina Owczarzak"], "references": [2154652894, 2134061542, 2140424253]}, {"id": 2134061542, "title": "Applying the Pyramid Method in DUC 2005", "abstract": "In DUC 2005, the pyramid method for content evaluation was used for the first time in a crosssite evaluation. We discuss the method used in creating pyramid models and performing peer annotation. Analysis of score averages for the peers indicates that the best systems score half as well as humans, and that systems can be grouped into better and worse performers. There were few significant differences among systems. High score correlations between sets from different annotators, and good interannotator agreement, indicate that participants can perform annotation reliably. We found that a modified pyramid score gave good results and would simplify peer annotation in the future.", "date": "2004", "authors": ["Kathleen McKeown , Rebecca Passonneau , Ani Nenkova , Sergey Sigelman"], "references": [3021916629, 2102065370, 2036002592, 2002664886, 1940278502, 1977747299, 1985463960, 1501617060, 2907147533, 2128530885]}, {"id": 2604319603, "title": "Efficient Processing of Deep Neural Networks: A Tutorial and Survey", "abstract": "Deep neural networks (DNNs) are currently widely used for many artificial intelligence (AI) applications including computer vision, speech recognition, and robotics. While DNNs deliver state-of-the-art accuracy on many AI tasks, it comes at the cost of high computational complexity. Accordingly, techniques that enable efficient processing of DNNs to improve energy efficiency and throughput without sacrificing application accuracy or increasing hardware cost are critical to the wide deployment of DNNs in AI systems. This article aims to provide a comprehensive tutorial and survey about the recent advances toward the goal of enabling efficient processing of DNNs. Specifically, it will provide an overview of DNNs, discuss various hardware platforms and architectures that support DNNs, and highlight key trends in reducing the computation cost of DNNs either solely via hardware design changes or via joint hardware design and DNN algorithm changes. It will also summarize various development resources that enable researchers and practitioners to quickly get started in this field, and highlight important benchmarking metrics and design considerations that should be used for evaluating the rapidly growing number of DNN hardware designs, optionally including algorithmic codesigns, being proposed in academia and industry. The reader will take away the following concepts from this article: understand the key design considerations for DNNs; be able to evaluate different DNN hardware implementations with benchmarks and comparison metrics; understand the tradeoffs between various hardware architectures and platforms; be able to evaluate the utility of various DNN design techniques for efficient processing; and understand recent implementation trends and opportunities.", "date": "2017", "authors": ["Vivienne Sze , Yu-Hsin Chen , Tien-Ju Yang , Joel S. Emer"], "references": [2194775991, 2618530766, 2962835968, 2097117768, 2919115771, 1836465849, 2102605133, 2117539524, 1903029394, 2155893237]}, {"id": 2955425717, "title": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks", "abstract": "", "date": "2019", "authors": ["Mingxing Tan , Quoc V. Le"], "references": [2955425717, 3018757597, 3034971973, 3035160371, 2994749257, 3035743198, 3034429256]}, {"id": 2965658867, "title": "Regularized Evolution for Image Classifier Architecture Search", "abstract": "The effort devoted to hand-crafting neural network image classifiers has motivated the use of architecture search to discover them automatically. Although evolutionary algorithms have been repeatedly applied to neural network topologies, the image classifiers thus discovered have remained inferior to human-crafted ones. Here, we evolve an image classifier\u2014 AmoebaNet-A\u2014that surpasses hand-designs for the first time. To do this, we modify the tournament selection evolutionary algorithm by introducing an age property to favor the younger genotypes. Matching size, AmoebaNet-A has comparable accuracy to current state-of-the-art ImageNet models discovered with more complex architecture-search methods. Scaled to larger size, AmoebaNet-A sets a new state-of-theart 83.9% top-1 / 96.6% top-5 ImageNet accuracy. In a controlled comparison against a well known reinforcement learning algorithm, we give evidence that evolution can obtain results faster with the same hardware, especially at the earlier stages of the search. This is relevant when fewer compute resources are available. Evolution is, thus, a simple method to effectively discover high-quality architectures.", "date": "2019", "authors": ["Esteban Real , Alok Aggarwal , Yanping Huang , Quoc V. Le"], "references": [2883780447, 2963918968, 3111681398, 2949736877, 2963136578, 3034971973, 3035160371]}, {"id": 2886335102, "title": "CornerNet: Detecting Objects as Paired Keypoints", "abstract": "We propose CornerNet, a new approach to object detection where we detect an object bounding box as a pair of keypoints, the top-left corner and the bottom-right corner, using a single convolution neural network. By detecting objects as paired keypoints, we eliminate the need for designing a set of anchor boxes commonly used in prior single-stage detectors. In addition to our novel formulation, we introduce corner pooling, a new type of pooling layer that helps the network better localize corners. Experiments show that CornerNet achieves a 42.2% AP on MS COCO, outperforming all existing one-stage detectors.", "date": "2020", "authors": ["Hei Law , Jia Deng"], "references": [2194775991, 2618530766, 2962835968, 639708223, 1836465849, 2102605133, 2108598243, 3106250896, 1677182931, 2963037989]}, {"id": 2942841021, "title": "Squeeze-and-Excitation Networks", "abstract": "The central building block of convolutional neural networks (CNNs) is the convolution operator, which enables networks to construct informative features by fusing both spatial and channel-wise information within local receptive fields at each layer. A broad range of prior research has investigated the spatial component of this relationship, seeking to strengthen the representational power of a CNN by enhancing the quality of spatial encodings throughout its feature hierarchy. In this work, we focus instead on the channel relationship and propose a novel architectural unit, which we term the \u201cSqueeze-and-Excitation\u201d (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We show that these blocks can be stacked together to form SENet architectures that generalise extremely effectively across different datasets. We further demonstrate that SE blocks bring significant improvements in performance for existing state-of-the-art CNNs at slight additional computational cost. Squeeze-and-Excitation Networks formed the foundation of our ILSVRC 2017 classification submission which won first place and reduced the top-5 error to 2.251 percent, surpassing the winning entry of 2016 by a relative improvement of ${\\sim }$ \u223c 25 percent. Models and code are available at https://github.com/hujie-frank/SENet .", "date": "2020", "authors": ["Jie Hu 1, Li Shen 2, Samuel Albanie 2, Gang Sun 1, Enhua Wu 1"], "references": [2194775991, 2618530766, 2962835968, 2097117768, 639708223, 2963403868, 1836465849, 2117539524, 1903029394, 1677182931]}, {"id": 2963918968, "title": "MnasNet: Platform-Aware Neural Architecture Search for Mobile", "abstract": "Designing convolutional neural networks (CNN) for mobile devices is challenging because mobile models need to be small and fast, yet still accurate. Although significant efforts have been dedicated to design and improve mobile CNNs on all dimensions, it is very difficult to manually balance these trade-offs when there are so many architectural possibilities to consider. In this paper, we propose an automated mobile neural architecture search (MNAS) approach, which explicitly incorporate model latency into the main objective so that the search can identify a model that achieves a good trade-off between accuracy and latency. Unlike previous work, where latency is considered via another, often inaccurate proxy (e.g., FLOPS), our approach directly measures real-world inference latency by executing the model on mobile phones. To further strike the right balance between flexibility and search space size, we propose a novel factorized hierarchical search space that encourages layer diversity throughout the network. Experimental results show that our approach consistently outperforms state-of-the-art mobile CNN models across multiple vision tasks. On the ImageNet classification task, our MnasNet achieves 75.2% top-1 accuracy with 78ms latency on a Pixel phone, which is 1.8\u00d7 faster than MobileNetV2 with 0.5% higher accuracy and 2.3\u00d7 faster than NASNet with 1.2% higher accuracy. Our MnasNet also achieves better mAP quality than MobileNets for COCO object detection. Code is at https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet.", "date": "2019", "authors": ["Mingxing Tan , Bo Chen , Ruoming Pang , Vijay Vasudevan , Mark Sandler , Andrew Howard , Quoc V. Le"], "references": [2194775991, 1861492603, 2963163009, 2963420686, 2964081807, 2736601468, 2963374479, 2963125010, 2810075754, 2965658867]}, {"id": 2774644650, "title": "Boosting Adversarial Attacks with Momentum", "abstract": "Deep neural networks are vulnerable to adversarial examples, which poses security concerns on these algorithms due to the potentially severe consequences. Adversarial attacks serve as an important surrogate to evaluate the robustness of deep learning models before they are deployed. However, most of existing adversarial attacks can only fool a black-box model with a low success rate. To address this issue, we propose a broad class of momentum-based iterative algorithms to boost adversarial attacks. By integrating the momentum term into the iterative process for attacks, our methods can stabilize update directions and escape from poor local maxima during the iterations, resulting in more transferable adversarial examples. To further improve the success rates for black-box attacks, we apply momentum iterative algorithms to an ensemble of models, and show that the adversarially trained models with a strong defense ability are also vulnerable to our black-box attacks. We hope that the proposed methods will serve as a benchmark for evaluating the robustness of various deep models and defense methods. With this method, we won the first places in NIPS 2017 Non-targeted Adversarial Attack and Targeted Adversarial Attack competitions.", "date": "2018", "authors": ["Yinpeng Dong 1, Fangzhou Liao 1, Tianyu Pang 1, Hang Su 1, Jun Zhu 1, Xiaolin Hu 1, Jianguo Li 2"], "references": [2117539524, 2183341477, 2963207607, 2964153729, 2302255633, 2964350391, 2963857521, 104184427, 2963542245, 2964082701]}, {"id": 2963402313, "title": "Simple Baselines for Human Pose Estimation and Tracking", "abstract": "There has been significant progress on pose estimation and increasing interests on pose tracking in recent years. At the same time, the overall algorithm and system complexity increases as well, making the algorithm analysis and comparison more difficult. This work provides simple and effective baseline methods. They are helpful for inspiring and evaluating new ideas for the field. State-of-the-art results are achieved on challenging benchmarks. The code will be available at https://github.com/leoxiaobin/pose.pytorch.", "date": "2018", "authors": ["Bin Xiao 1, Haiping Wu 2, Yichen Wei 1"], "references": [2194775991, 2618530766, 2964121744, 639708223, 1836465849, 2117539524, 2806070179, 1861492603, 2964350391, 2559085405]}, {"id": 36903255, "title": "Hierarchical Probabilistic Neural Network Language Model.", "abstract": "In recent years, variants of a neural network architecture for statistical language modeling have been proposed and successfully applied, e.g. in the language modeling component of speech recognizers. The main advantage of these architectures is that they learn an embedding for words (or other symbols) in a continuous space that helps to smooth the language model and provide good generalization even when the number of training examples is insufficient. However, these models are extremely slow in comparison to the more commonly used n-gram models, both for training and recognition. As an alternative to an importance sampling method proposed to speed-up training, we introduce a hierarchical decomposition of the conditional probabilities that yields a speed-up of about 200 both during training and recognition. The hierarchical decomposition is a binary hierarchical clustering constrained by the prior knowledge extracted from the WordNet semantic hierarchy.", "date": "2004", "authors": ["Frederic Morin , Yoshua Bengio"], "references": [2038721957, 2116064496, 2132339004, 2147152072, 1631260214, 2096175520, 2110485445, 1978394996, 2121227244, 2127314673]}, {"id": 2096072088, "title": "A Joint Language Model With Fine-grain Syntactic Tags", "abstract": "We present a scalable joint language model designed to utilize fine-grain syntactic tags. We discuss challenges such a design faces and describe our solutions that scale well to large tagsets and corpora. We advocate the use of relatively simple tags that do not require deep linguistic knowledge of the language but provide more structural information than POS tags and can be derived from automatically generated parse trees - a combination of properties that allows easy adoption of this model for new languages. We propose two fine-grain tagsets and evaluate our model using these tags, as well as POS tags and SuperARV tags in a speech recognition task and discuss future directions.", "date": "2009", "authors": ["Denis Filimonov , Mary Harper"], "references": [2158195707, 2121227244, 2056250865, 2099345940, 2113691817, 1924403233, 2080018251, 2155280192, 2143866356, 2116625254]}, {"id": 2468573742, "title": "The 2005 AMI system for the transcription of speech in meetings", "abstract": "In this paper we describe the 2005 AMI system for the transcription of speech in meetings used in the 2005 NIST RT evaluations. The system was designed for participation in the speech to text part of the evaluations, in particular for transcription of speech recorded with multiple distant microphones and independent headset microphones. System performance was tested on both conference room and lecture style meetings. Although input sources are processed using different front-ends, the recognition process is based on a unified system architecture. The system operates in multiple passes and makes use of state of the art technologies such as discriminative training, vocal tract length normalisation, heteroscedastic linear discriminant analysis, speaker adaptation with maximum likelihood linear regression and minimum word error rate decoding. In this paper we describe the system performance on the official development and test sets for the NIST RT05s evaluations. The system was jointly developed in less than 10 months by a multi-site team and was shown to achieve competitive performance.", "date": "2005", "authors": ["Thomas Hain 1, Lukas Burget 2, John Dines 3, Giulia Garau 4, Martin Karafiat 2, Mike Lincoln 4, Iain McCowan 3, Darren Moore 3, Vincent Wan 1, Roeland Ordelman 5, Steve Renals 4"], "references": [2093225945, 2100969003, 2150907703, 1591607137, 2125336414, 2046317813, 1569447338, 2037740282, 1571931074, 2094971681]}, {"id": 2152808281, "title": "Adaptive Importance Sampling to Accelerate Training of a Neural Probabilistic Language Model", "abstract": "Previous work on statistical language modeling has shown that it is possible to train a feedforward neural network to approximate probabilities over sequences of words, resulting in significant error reduction when compared to standard baseline models based on n-grams. However, training the neural network model with the maximum-likelihood criterion requires computations proportional to the number of words in the vocabulary. In this paper, we introduce adaptive importance sampling as a way to accelerate training of the model. The idea is to use an adaptive n-gram model to track the conditional distributions produced by the neural network. We show that a very significant speedup can be obtained on standard problems.", "date": "2008", "authors": ["Y. Bengio , J.-S. Senecal"], "references": [2116064496, 1574901103, 2132339004, 1985093013, 2096175520, 2069739265, 1802356529, 2134237567, 1934041838, 2140679639]}, {"id": 2027499299, "title": "The AMI System for the Transcription of Speech in Meetings", "abstract": "This paper describes the AMI transcription system for speech in meetings developed in collaboration by five research groups. The system includes generic techniques such as discriminative and speaker adaptive training, vocal tract length normalisation, heteroscedastic linear discriminant analysis, maximum likelihood linear regression, and phone posterior based features, as well as techniques specifically designed for meeting data. These include segmentation and cross-talk suppression, beam-forming, domain adaptation, Web-data collection, and channel adaptive training. The system was improved by more than 20% relative in word error rate compared to our previous system and was used in the NIST RT106 evaluations where it was found to yield competitive performance.", "date": "2007", "authors": ["T. Hain 1, V. Wan 1, L. Burget 2, M. Karafiat 2, J. Dines 3, J. Vepa 4, G. Garau 4, M. Lincoln 4"], "references": [2093225945, 2100969003, 2150907703, 1591607137, 2468573742, 2125336414, 2046317813, 1569447338, 2037740282, 1571931074]}, {"id": 2292896937, "title": "A guide to recurrent neural networks and backpropagation", "abstract": "This paper provides guidance to some of the concepts surrounding recurrent neural networks. Contrary to feedforward networks, recurrent networks can be sensitive, and be adapted to past inputs. Backpropagation learning is described for feedforward networks, adapted to suit our (probabilistic) modeling needs, and extended to cover recurrent networks. The aim of this brief paper is to set the scene for applying and understanding recurrent neural networks.", "date": "2000", "authors": ["Mikael Boden"], "references": [2110485445, 2107878631, 2339378878, 2016589492, 3036751298, 1979684610, 1959983357, 2151834591, 2121553911, 2468203291]}, {"id": 2437096199, "title": "Training Neural Network Language Models on Very Large Corpora", "abstract": "During the last years there has been growing interest in using neural networks for language modeling. In contrast to the well known back-off n-gram language models, the neural network approach attempts to overcome the data sparseness problem by performing the estimation in a continuous space. This type of language model was mostly used for tasks for which only a very limited amount of in-domain training data is available.In this paper we present new algorithms to train a neural network language model on very large text corpora. This makes possible the use of the approach in domains where several hundreds of millions words of texts are available. The neural network language model is evaluated in a state-of-the-art real-time continuous speech recognizer for French Broadcast News. Word error reductions of 0.5% absolute are reported using only a very limited amount of additional processing time.", "date": "2005", "authors": ["Holger Schwenk , Jean-Luc Gauvain"], "references": [2156909104, 2148603752, 2132339004, 1631260214, 2158195707, 2121227244, 2134237567, 2070534370, 2140679639, 2056590938]}, {"id": 2963163009, "title": "MobileNetV2: Inverted Residuals and Linear Bottlenecks", "abstract": "In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3. is based on an inverted residual structure where the shortcut connections are between the thin bottleneck layers. The intermediate expansion layer uses lightweight depthwise convolutions to filter features as a source of non-linearity. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on ImageNet [1] classification, COCO object detection [2], VOC image segmentation [3]. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as actual latency, and the number of parameters.", "date": "2018", "authors": ["Mark Sandler , Andrew Howard , Menglong Zhu , Andrey Zhmoginov , Liang-Chieh Chen"], "references": [2194775991, 2962835968, 2097117768, 639708223, 2117539524, 2155893237, 3106250896, 1861492603, 2963542991, 2412782625]}, {"id": 2963420686, "title": "Squeeze-and-Excitation Networks", "abstract": "Convolutional neural networks are built upon the convolution operation, which extracts informative features by fusing spatial and channel-wise information together within local receptive fields. In order to boost the representational power of a network, several recent approaches have shown the benefit of enhancing spatial encoding. In this work, we focus on the channel relationship and propose a novel architectural unit, which we term the \"Squeeze-and-Excitation\" (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We demonstrate that by stacking these blocks together, we can construct SENet architectures that generalise extremely well across challenging datasets. Crucially, we find that SE blocks produce significant performance improvements for existing state-of-the-art deep architectures at minimal additional computational cost. SENets formed the foundation of our ILSVRC 2017 classification submission which won first place and significantly reduced the top-5 error to 2.251%, achieving a ~25% relative improvement over the winning entry of 2016. Code and models are available at https://github.com/hujie-frank/SENet.", "date": "2018", "authors": ["Jie Hu 1, Li Shen 2, Gang Sun 1"], "references": [2194775991, 2618530766, 2962835968, 2097117768, 639708223, 1836465849, 2117539524, 1903029394, 1677182931, 2963446712]}, {"id": 2964081807, "title": "Learning Transferable Architectures for Scalable Image Recognition", "abstract": "Developing neural network image classification models often requires significant architecture engineering. In this paper, we study a method to learn the model architectures directly on the dataset of interest. As this approach is expensive when the dataset is large, we propose to search for an architectural building block on a small dataset and then transfer the block to a larger dataset. The key contribution of this work is the design of a new search space (which we call the \"NASNet search space\") which enables transferability. In our experiments, we search for the best convolutional layer (or \"cell\") on the CIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking together more copies of this cell, each with their own parameters to design a convolutional architecture, which we name a \"NASNet architecture\". We also introduce a new regularization technique called ScheduledDropPath that significantly improves generalization in the NASNet models. On CIFAR-10 itself, a NASNet found by our method achieves 2.4% error rate, which is state-of-the-art. Although the cell is not searched for directly on ImageNet, a NASNet constructed from the best cell achieves, among the published works, state-of-the-art accuracy of 82.7% top-1 and 96.2% top-5 on ImageNet. Our model is 1.2% better in top-1 accuracy than the best human-invented architectures while having 9 billion fewer FLOPS - a reduction of 28% in computational demand from the previous state-of-the-art model. When evaluated at different levels of computational cost, accuracies of NASNets exceed those of the state-of-the-art human-designed models. For instance, a small version of NASNet also achieves 74% top-1 accuracy, which is 3.1% better than equivalently-sized, state-of-the-art models for mobile platforms. Finally, the image features learned from image classification are generically useful and can be transferred to other computer vision problems. On the task of object detection, the learned features by NASNet used with the Faster-RCNN framework surpass state-of-the-art by 4.0% achieving 43.1% mAP on the COCO dataset.", "date": "2018", "authors": ["Barret Zoph , Vijay Vasudevan , Jonathon Shlens , Quoc V. Le"], "references": [2194775991, 2618530766, 2962835968, 2097117768, 639708223, 1836465849, 2095705004, 2108598243, 2963446712, 2183341477]}, {"id": 2810075754, "title": "DARTS: Differentiable Architecture Search", "abstract": "This paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner. Unlike conventional approaches of applying evolution or reinforcement learning over a discrete and non-differentiable search space, our method is based on the continuous relaxation of the architecture representation, allowing efficient search of the architecture using gradient descent. Extensive experiments on CIFAR-10, ImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling, while being orders of magnitude faster than state-of-the-art non-differentiable techniques. Our implementation has been made publicly available to facilitate further research on efficient architecture search algorithms.", "date": "2018", "authors": ["Hanxiao Liu 1, Karen Simonyan 2, Yiming Yang 1"], "references": [2964110616, 2942841021, 2963918968, 2964212578, 2967733054, 2962847160, 2963136578, 2766839578]}, {"id": 2962851801, "title": "Learning Efficient Convolutional Networks through Network Slimming", "abstract": "The deployment of deep convolutional neural networks (CNNs) in many real world applications is largely hindered by their high computational cost. In this paper, we propose a novel learning scheme for CNNs to simultaneously 1) reduce the model size; 2) decrease the run-time memory footprint; and 3) lower the number of computing operations, without compromising accuracy. This is achieved by enforcing channel-level sparsity in the network in a simple but effective way. Different from many existing approaches, the proposed method directly applies to modern CNN architectures, introduces minimum overhead to the training process, and requires no special software/hardware accelerators for the resulting models. We call our approach network slimming, which takes wide and large networks as input models, but during training insignificant channels are automatically identified and pruned afterwards, yielding thin and compact models with comparable accuracy. We empirically demonstrate the effectiveness of our approach with several state-of-the-art CNN models, including VGGNet, ResNet and DenseNet, on various image classification datasets. For VGGNet, a multi-pass version of network slimming gives a 20\u00d7 reduction in model size and a 5\u00d7 reduction in computing operations.", "date": "2017", "authors": ["Zhuang Liu 1, Jianguo Li 2, Zhiqiang Shen 3, Gao Huang 1, Shoumeng Yan 2, Changshui Zhang 1"], "references": [2194775991, 2618530766, 2962835968, 2097117768, 1836465849, 2102605133, 1903029394, 1677182931, 2963446712, 3118608800]}, {"id": 1793121960, "title": "End-to-end memory networks", "abstract": "We introduce a neural network with a recurrent attention model over a possibly large external memory. The architecture is a form of Memory Network [23] but unlike the model in that work, it is trained end-to-end, and hence requires significantly less supervision during training, making it more generally applicable in realistic settings. It can also be seen as an extension of RNNsearch [2] to the case where multiple computational steps (hops) are performed per output symbol. The flexibility of the model allows us to apply it to tasks as diverse as (synthetic) question answering [22] and to language modeling. For the former our approach is competitive with Memory Networks, but with less supervision. For the latter, on the Penn TreeBank and Text8 datasets our approach demonstrates comparable performance to RNNs and LSTMs. In both cases we show that the key concept of multiple computational hops yields improved results.", "date": "2015", "authors": ["Sainbayar Sukhbaatar 1, Arthur Szlam 2, Jason Weston 2, Rob Fergus 2"], "references": [2964308564, 2064675550, 1514535095, 1924770834, 2132339004, 1810943226, 2962741254, 1632114991, 1591801644, 2584341106]}, {"id": 2551396370, "title": "Bidirectional Attention Flow for Machine Comprehension", "abstract": "", "date": "2016", "authors": ["Min Joon Seo 1, Aniruddha Kembhavi 2, Ali Farhadi 1, Hannaneh Hajishirzi 1"], "references": [2963341956, 2962739339, 2963756346, 2923014074, 2963339397, 2889787757, 2963961878, 2740747242, 2962985038]}, {"id": 2507756961, "title": "Pointer networks", "abstract": "We introduce a new neural architecture to learn the conditional probability of an output sequence with elements that are discrete tokens corresponding to positions in an input sequence. Such problems cannot be trivially addressed by existent approaches such as sequence-to-sequence [1] and Neural Turing Machines [2], because the number of target classes in each step of the output depends on the length of the input, which is variable. Problems such as sorting variable sized sequences, and various combinatorial optimization problems belong to this class. Our model solves the problem of variable size output dictionaries using a recently proposed mechanism of neural attention. It differs from the previous attention attempts in that, instead of using attention to blend hidden units of an encoder to a context vector at each decoder step, it uses attention as a pointer to select a member of the input sequence as the output. We call this architecture a Pointer Net (Ptr-Net). We show Ptr-Nets can be used to learn approximate solutions to three challenging geometric problems - finding planar convex hulls, computing Delaunay triangulations, and the planar Travelling Salesman Problem - using training examples alone. Ptr-Nets not only improve over sequence-to-sequence with input attention, but also allow us to generalize to variable size output dictionaries. We show that the learnt models generalize beyond the maximum lengths they were trained on. We hope our results on these tasks will encourage a broader exploration of neural learning for discrete problems.", "date": "2015", "authors": ["Oriol Vinyals 1, Meire Fortunato 2, Navdeep Jaitly 1"], "references": [2964308564, 2130942839, 2064675550, 1895577753, 1947481528, 2154642048, 2963069010, 2584341106, 2167839676, 1581407678]}, {"id": 2963907629, "title": "A simple neural network module for relational reasoning", "abstract": "Relational reasoning is a central component of generally intelligent behavior, but has proven difficult for neural networks to learn. In this paper we describe how to use Relation Networks (RNs) as a simple plug-and-play module to solve problems that fundamentally hinge on relational reasoning. We tested RN-augmented networks on three tasks: visual question answering using a challenging dataset called CLEVR, on which we achieve state-of-the-art, super-human performance; text-based question answering using the bAbI suite of tasks; and complex reasoning about dynamical physical systems. Then, using a curated dataset called Sort-of-CLEVR we show that powerful convolutional networks do not have a general capacity to solve relational questions, but can gain this capacity when augmented with RNs. Thus, by simply augmenting convolutions, LSTMs, and MLPs with RNs, we can remove computational burden from network components that are not well-suited to handle relational reasoning, reduce overall network complexity, and gain a general ability to reason about the relations between entities and their properties.", "date": "2017", "authors": ["Adam Santoro 1, David Raposo 2, David G. T. Barrett 1, Mateusz Malinowski 3, Razvan Pascanu 1, Peter W. Battaglia 4, Timothy P. Lillicrap 1"], "references": [2963091558, 2964105864, 2962711740, 2892094955, 2997428643, 2905224888, 2738724892, 2963738360, 2966926453, 2963563276]}, {"id": 2964210218, "title": "Learning End-to-End Goal-Oriented Dialog", "abstract": "", "date": "2016", "authors": ["Antoine Bordes 1, Y-Lan Boureau 2, Jason Weston 1"], "references": [2768661419, 2963825865, 2586847566, 2594726847, 2964006684, 2963475460, 2963491014, 2580175322]}, {"id": 2331432542, "title": "An Introduction to the Bootstrap.", "abstract": "Introduction The Accuracy of a Sample Mean Random Samples and Probabilities The Empirical Distribution Function and the Plug-In Principle Standard Errors and Estimated Standard Errors The Bootstrap Estimate of Standard Error Bootstrap Standard Errors: Some Examples More Complicated Data Structures Regression Models Estimates of Bias The Jackknife Confidence Intervals Based on Bootstrap \"Tables\" Confidence Intervals Based on Bootstrap Percentiles Better Bootstrap Confidence Intervals Permutation Tests Hypothesis Testing with the Bootstrap Cross-Validation and Other Estimates of Prediction Error Adaptive Estimation and Calibration Assessing the Error in Bootstrap Estimates A Geometrical Representation for the Bootstrap and Jackknife An Overview of Nonparametric and Parametric Inference Further Topics in Bootstrap Confidence Intervals Efficient Bootstrap Computations Approximate Likelihoods Bootstrap Bioequivalence Discussion and Further Topics Appendix: Software for Bootstrap Computations References", "date": "1995", "authors": ["Bradley Efron 1, Robert J. Tibshirani 2"], "references": [2056279562, 2119444539, 1970142344, 2100805904, 2151226564, 222053410, 2157452732, 2125837480, 2165786903, 2147105902]}, {"id": 3085162807, "title": "Classification and Regression Trees.", "abstract": "", "date": "1986", "authors": ["John Van Ryzin , Leo Breiman , Jerome H. Friedman , Richard A. Olshen , Charles J. Stone"], "references": [2133990480, 2140190241, 1570448133, 2072128103, 2135046866, 1484413656, 2158698691, 2912934387, 2166559705, 2063978378]}, {"id": 2158940042, "title": "Ideal spatial adaptation by wavelet shrinkage", "abstract": "SUMMARY With ideal spatial adaptation, an oracle furnishes information about how best to adapt a spatially variable estimator, whether piecewise constant, piecewise polynomial, variable knot spline, or variable bandwidth kernel, to the unknown function. Estimation with the aid of an oracle offers dramatic advantages over traditional linear estimation by nonadaptive kernels; however, it is a priori unclear whether such performance can be obtained by a procedure relying on the data alone. We describe a new principle for spatially-adaptive estimation: selective wavelet reconstruction. We show that variable-knot spline fits and piecewise-polynomial fits, when equipped with an oracle to select the knots, are not dramatically more powerful than selective wavelet reconstruction with an oracle. We develop a practical spatially adaptive method, RiskShrink, which works by shrinkage of empirical wavelet coefficients. RiskShrink mimics the performance of an oracle for selective wavelet reconstruction as well as it is possible to do so. A new inequality in multivariate normal decision theory which we call the oracle inequality shows that attained performance differs from ideal performance by at most a factor of approximately 2 log n, where n is the sample size. Moreover no estimator can give a better guarantee than this. Within the class of spatially adaptive procedures, RiskShrink is essentially optimal. Relying only on the data, it comes within a factor log 2 n of the performance of piecewise polynomial and variableknot spline methods equipped with an oracle. In contrast, it is unknown how or if piecewise polynomial methods could be made to function this well when denied access to an oracle and forced to rely on data alone.", "date": "1994", "authors": ["David L Donoho , Iain M Johnstone"], "references": [2062024414, 2098914003, 1489213177, 1969423031, 139959648, 654435104, 2797502950, 2024599390, 2140832824, 2036144352]}, {"id": 1594031697, "title": "Classification and regression trees", "abstract": "Background. Introduction to Tree Classification. Right Sized Trees and Honest Estimates. Splitting Rules. Strengthening and Interpreting. Medical Diagnosis and Prognosis. Mass Spectra Classification. Regression Trees. Bayes Rules and Partitions. Optimal Pruning. Construction of Trees from a Learning Sample. Consistency. Bibliography. Notation Index. Subject Index.", "date": "1982", "authors": ["Leo Breiman"], "references": [2011039300, 3017143921, 2127218421, 2022554507, 2583466288, 2123838014, 2164240509, 1976123439, 1970074386, 2150418026]}, {"id": 2797583072, "title": "Generalized Additive Models.", "abstract": "", "date": "1991", "authors": ["R. A. Brown , T. J. Hastie , R. J. Tibshirani"], "references": []}, {"id": 2106706098, "title": "Reversible jump Markov chain Monte Carlo computation and Bayesian model determination", "abstract": "Markov chain Monte Carlo methods for Bayesian computation have until recently been restricted to problems where the joint distribution of all variables has a density with respect to some fixed standard underlying measure. They have therefore not been available for application to Bayesian model determination, where the dimensionality of the parameter vector is typically not fixed. This paper proposes a new framework for the construction of reversible Markov chain samplers that jump between parameter subspaces of differing dimensionality, which is flexible and entirely constructive. It should therefore have wide applicability in model determination problems. The methodology is illustrated with applications to multiple change-point analysis in one and two dimensions, and to a Bayesian comparison of binomial experiments.", "date": "1995", "authors": ["Peter J. Green"], "references": [1997063559, 2116044718, 2136796925, 1988684120, 2111051773, 1582801283, 1555683961, 2138309709, 2171166366, 1989457171]}, {"id": 2102201073, "title": "Multivariate Adaptive Regression Splines", "abstract": "", "date": "1991", "authors": ["Jerome H. Friedman"], "references": [2798909945, 3085162807, 2146766088, 2162870748, 2017977879, 133977063, 2091886411, 2040615655, 3000332379, 2082102453]}, {"id": 2117897510, "title": "Bootstrap Methods: Another Look at the Jackknife", "abstract": "We discuss the following problem given a random sample X = (X 1, X 2,\u2026, X n) from an unknown probability distribution F, estimate the sampling distribution of some prespecified random variable R(X, F), on the basis of the observed data x. (Standard jackknife theory gives an approximate mean and variance in the case R(X, F) = \\(\\theta \\left( {\\hat F} \\right) - \\theta \\left( F \\right)\\), \u03b8 some parameter of interest.) A general method, called the \u201cbootstrap\u201d, is introduced, and shown to work satisfactorily on a variety of estimation problems. The jackknife is shown to be a linear approximation method for the bootstrap. The exposition proceeds by a series of examples: variance of the sample median, error rates in a linear discriminant analysis, ratio estimation, estimating regression parameters, etc.", "date": "1978", "authors": ["Bradley Efron"], "references": [2063698478, 1981492171, 2325429543, 2079100340, 1989584148, 2061905897, 2033260623, 1971382239, 1528761061, 2069645522]}, {"id": 2075665712, "title": "Solving Least Squares Problems", "abstract": "", "date": "1986", "authors": ["Charles L. Lawson , Richard J. Hanson"], "references": [1570448133, 2135046866, 2024165284, 2063978378, 2047920195, 2752849906, 2172249709, 2110096996, 2017288758, 2099857446]}, {"id": 2954064014, "title": "Practical Optimization", "abstract": "", "date": "1981", "authors": ["Philip E. Gill"], "references": [2135046866, 2030723843, 2035379092, 1970101292, 1521785144, 2110575115, 2124313187, 2090267299, 1755563775, 2141870784]}, {"id": 3110653090, "title": "An information-maximization approach to blind separation and blind deconvolution", "abstract": "We derive a new self-organizing learning algorithm that maximizes the information transferred in a network of nonlinear units. The algorithm does not assume any knowledge of the input distributions, and is defined here for the zero-noise limit. Under these conditions, information maximization has extra properties not found in the linear case (Linsker 1989). The nonlinearities in the transfer function are able to pick up higher-order moments of the input distributions and perform something akin to true redundancy reduction between units in the output representation. This enables the network to separate statistically independent components in the inputs: a higher-order generalization of principal components analysis. We apply the network to the source separation (or cocktail party) problem, successfully separating unknown mixtures of up to 10 speakers. We also show that a variant on the network architecture is able to perform blind deconvolution (cancellation of unknown echoes and reverberation in a speech ...", "date": "1999", "authors": ["Anthony J. Bell , Terrence J. Sejnowski"], "references": [2124776405, 1548802052, 2099741732, 1996355918, 2038085771, 2180838288, 2912889105, 1667165204, 2006544565, 2056211671]}, {"id": 1479807131, "title": "Semi-Supervised Learning", "abstract": "In the field of machine learning, semi-supervised learning (SSL) occupies the middle ground, between supervised learning (in which all training examples are labeled) and unsupervised learning (in which no label data are given). Interest in SSL has increased in recent years, particularly because of application domains in which unlabeled data are plentiful, such as images, text, and bioinformatics. This first comprehensive overview of SSL presents state-of-the-art algorithms, a taxonomy of the field, selected applications, benchmark experiments, and perspectives on ongoing and future research. Semi-Supervised Learning first presents the key assumptions and ideas underlying the field: smoothness, cluster or low-density separation, manifold structure, and transduction. The core of the book is the presentation of SSL methods, organized according to algorithmic strategies. After an examination of generative models, the book describes algorithms that implement the low-density separation assumption, graph-based methods, and algorithms that perform two-step learning. The book then discusses SSL applications and offers guidelines for SSL practitioners by analyzing the results of extensive benchmark experiments. Finally, the book looks at interesting directions for SSL research. The book closes with a discussion of the relationship between semi-supervised learning and transduction. Adaptive Computation and Machine Learning series", "date": "2010", "authors": ["Olivier Chapelle , Bernhard Schlkopf , Alexander Zien"], "references": [2296319761, 2156909104, 2158714788, 2148694408, 2055043387, 1480376833, 2119821739, 2053186076, 2121947440, 1988790447]}, {"id": 1746819321, "title": "Gaussian Processes for Machine Learning", "abstract": "Gaussian processes (GPs) provide a principled, practical, probabilistic approach to learning in kernel machines. GPs have received increased attention in the machine-learning community over the past decade, and this book provides a long-needed systematic and unified treatment of theoretical and practical aspects of GPs in machine learning. The treatment is comprehensive and self-contained, targeted at researchers and students in machine learning and applied statistics.The book deals with the supervised-learning problem for both regression and classification, and includes detailed algorithms. A wide variety of covariance (kernel) functions are presented and their properties discussed. Model selection is discussed both from a Bayesian and a classical perspective. Many connections to other well-known techniques from machine learning and statistics are discussed, including support-vector machines, neural networks, splines, regularization networks, relevance vector machines and others. Theoretical issues including learning curves and the PAC-Bayesian framework are treated, and several approximation methods for learning with large datasets are discussed. The book contains illustrative examples and exercises, and code and datasets are available on the Web. Appendixes provide mathematical background and a discussion of Gaussian Markov processes.", "date": "2005", "authors": ["Carl Edward Rasmussen 1, Christopher K I Williams 2"], "references": [2296319761, 2156909104, 2148603752, 2170120409, 1554663460, 3023786531, 2165363188, 2798909945, 2117812871, 2078206416]}, {"id": 2097998348, "title": "Random search for hyper-parameter optimization", "abstract": "Grid search and manual search are the most widely used strategies for hyper-parameter optimization. This paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a comparison with a large previous study that used grid search and manual search to configure neural networks and deep belief networks. Compared with neural networks configured by a pure grid search, we find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search finds better models by effectively searching a larger, less promising configuration space. Compared with deep belief networks configured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional configuration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for configuring algorithms for new data sets. Our analysis casts some light on why recent \"High Throughput\" methods achieve surprising success--they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural baseline against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms.", "date": "2012", "authors": ["James Bergstra , Yoshua Bengio"], "references": [2153635508, 2136922672, 2310919327, 1746819321, 1554663460, 2072128103, 1533861849, 2025768430, 2581275558, 44815768]}, {"id": 2106411961, "title": "Algorithms for Hyper-Parameter Optimization", "abstract": "Several recent advances to the state of the art in image classification benchmarks have come from better configurations of existing techniques rather than novel approaches to feature learning. Traditionally, hyper-parameter optimization has been the job of humans because they can be very efficient in regimes where only a few trials are possible. Presently, computer clusters and GPU processors make it possible to run more trials and we show that algorithmic approaches can find better results. We present hyper-parameter optimization results on tasks of training neural networks and deep belief networks (DBNs). We optimize hyper-parameters using random search and two new greedy sequential methods based on the expected improvement criterion. Random search has been shown to be sufficiently efficient for learning neural networks for several datasets, but we show it is unreliable for training DBNs. The sequential algorithms are applied to the most difficult DBN learning problems from [1] and find significantly better results than the best previously reported. This work contributes novel techniques for making response surface models P(y|x) in which many elements of hyper-parameter assignment (x) are known to be irrelevant given particular values of other elements.", "date": "2011", "authors": ["James S. Bergstra 1, R\u00e9mi Bardenet 2, Yoshua Bengio 3, Bal\u00e1zs K\u00e9gl 2"], "references": [2136922672, 2310919327, 1746819321, 1554663460, 2145094598, 2097998348, 2123649031, 2118858186, 1994197834, 2144161366]}, {"id": 2951665052, "title": "Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design", "abstract": "Many applications require optimizing an unknown, noisy function that is expensive to evaluate. We formalize this task as a multi-armed bandit problem, where the payoff function is either sampled from a Gaussian process (GP) or has low RKHS norm. We resolve the important open problem of deriving regret bounds for this setting, which imply novel convergence rates for GP optimization. We analyze GP-UCB, an intuitive upper-confidence based algorithm, and bound its cumulative regret in terms of maximal information gain, establishing a novel connection between GP optimization and experimental design. Moreover, by bounding the latter in terms of operator spectra, we obtain explicit sublinear regret bounds for many commonly used covariance functions. In some important cases, our bounds have surprisingly weak dependence on the dimensionality. In our experiments on real sensor data, GP-UCB compares favorably with other heuristical GP optimization approaches.", "date": "2010", "authors": ["Niranjan Srinivas 1, Andreas Krause 1, Matthias Seeger 2, Sham M. Kakade 3"], "references": [1746819321, 2099111195, 2168405694, 1625390266, 1510052597, 2146766088, 2099201756, 50486269, 2168464387, 2108114251]}, {"id": 60686164, "title": "Sequential model-based optimization for general algorithm configuration", "abstract": "State-of-the-art algorithms for hard computational problems often expose many parameters that can be modified to improve empirical performance. However, manually exploring the resulting combinatorial space of parameter settings is tedious and tends to lead to unsatisfactory outcomes. Recently, automated approaches for solving this algorithm configuration problem have led to substantial improvements in the state of the art for solving various problems. One promising approach constructs explicit regression models to describe the dependence of target algorithm performance on parameter settings; however, this approach has so far been limited to the optimization of few numerical algorithm parameters on single instances. In this paper, we extend this paradigm for the first time to general algorithm configuration problems, allowing many categorical parameters and optimization for sets of instances. We experimentally validate our new algorithm configuration procedure by optimizing a local search and a tree search solver for the propositional satisfiability problem (SAT), as well as the commercial mixed integer programming (MIP) solver CPLEX. In these experiments, our procedure yielded state-of-the-art performance, and in many cases outperformed the previous best configuration approach.", "date": "2011", "authors": ["Frank Hutter , Holger H. Hoos , Kevin Leyton-Brown"], "references": [2911964244, 1480376833, 1510052597, 2147148915, 2005059149, 1533856049, 3016663334, 2435432491, 1499307573, 2031585418]}, {"id": 2165599843, "title": "Online Learning for Latent Dirichlet Allocation", "abstract": "We develop an online variational Bayes (VB) algorithm for Latent Dirichlet Allocation (LDA). Online LDA is based on online stochastic optimization with a natural gradient step, which we show converges to a local optimum of the VB objective function. It can handily analyze massive document collections, including those arriving in a stream. We study the performance of online LDA in several ways, including by fitting a 100-topic topic model to 3.3M articles from Wikipedia in a single pass. We demonstrate that online LDA finds topic models as good or better than those found with batch VB, and in a fraction of the time.", "date": "2010", "authors": ["Matthew Hoffman 1, Francis R. Bach 2, David M. Blei 1"], "references": [1880262756, 2001082470, 2049633694, 2112447569, 1516111018, 1536929369, 2159426623, 2567948266, 2113651538, 2172085063]}, {"id": 2099201756, "title": "A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning", "abstract": "We present a tutorial on Bayesian optimization, a method of finding the maximum of expensive cost functions. Bayesian optimization employs the Bayesian technique of setting a prior over the objective function and combining it with evidence to get a posterior function. This permits a utility-based selection of the next observation to make on the objective function, which must take into account both exploration (sampling from areas of high uncertainty) and exploitation (sampling areas likely to offer improvement over the current best observation). We also present two detailed extensions of Bayesian optimization, with experiments---active user modelling with preferences, and hierarchical reinforcement learning---and a discussion of the pros and cons of Bayesian optimization based on our experiences.", "date": "2010", "authors": ["Eric Brochu , Vlad M. Cora , Nando de Freitas"], "references": [2100495367, 2121863487, 2903158431, 2018044188, 1510052597, 1576452626, 2951665052, 2041946752, 2130761473, 2131824593]}, {"id": 1973333099, "title": "Bayesian Calibration of computer models", "abstract": "We consider prediction and uncertainty analysis for systems which are approximated using complex mathematical models. Such models, implemented as computer codes, are often generic in the sense that by a suitable choice of some of the model's input parameters the code can be used to predict the behaviour of the system in a variety of specific applications. However, in any specific application the values of necessary parameters may be unknown. In this case, physical observations of the system in the specific context are used to learn about the unknown parameters. The process of fitting the model to the observed data by adjusting the parameters is known as calibration. Calibration is typically effected by ad hoc fitting, and after calibration the model is used, with the fitted input values, to predict the future behaviour of the system. We present a Bayesian calibration technique which improves on this traditional approach in two respects. First, the predictions allow for all sources of uncertainty, including the remaining uncertainty over the fitted parameters. Second, they attempt to correct for any inadequacy of the model which is revealed by a discrepancy between the observed data and the model predictions from even the best\u2010fitting parameter values. The method is illustrated by using data from a nuclear radiation release at Tomsk, and from a more complex simulated nuclear accident exercise.", "date": "2000", "authors": ["Marc C. Kennedy , Anthony O'Hagan"], "references": [2018044188, 2038669746, 2143022286, 1567512734, 2027792629, 2033900415, 999207820, 1977046327, 1585773866, 2056145269]}, {"id": 2030723843, "title": "The interior-point revolution in optimization: History, recent developments, and lasting consequences", "abstract": "Interior methods are a pervasive feature of the optimization landscape today, but it was not always so. Although interior-point techniques, primarily in the form of barrier methods, were widely used during the 1960s for problems with nonlinear constraints, their use for the fundamental problem of linear programming was unthinkable because of the total dominance of the simplex method. During the 1970s, barrier methods were superseded, nearly to the point of oblivion, by newly emerging and seemingly more efficient alternatives such as augmented Lagrangian and sequential quadratic programming methods. By the early 1980s, barrier methods were almost universally regarded as a closed chapter in the history of optimization. This picture changed dramatically in 1984, when Narendra Karmarkar announced a fast polynomial-time interior method for linear programming; in 1985, a formal connection was established between his method and classical barrier methods. Since then, interior methods have continued to transform both the theory and practice of constrained optimization. We present a condensed, unavoidably incomplete look at classical material and recent research about interior methods.", "date": "2004", "authors": ["Margaret H. Wright"], "references": [2798766386, 2077658674, 2099839128, 2912522929, 2611147814, 1985123706, 2954064014, 1963547452, 2041684917, 2008772536]}, {"id": 2000296233, "title": "Primal-dual algorithms and infinite-dimensional Jordan algebras of finite rank", "abstract": "We consider primal-dual algorithms for certain types of infinite-dimensional optimization problems. Our approach is based on the generalization of the technique of finite-dimensional Euclidean Jordan algebras to the case of infinite-dimensional JB-algebras of finite rank. This generalization enables us to develop polynomial-time primal-dual algorithms for ``infinite-dimensional second-order cone programs.'' We consider as an example a long-step primal-dual algorithm based on the Nesterov-Todd direction. It is shown that this algorithm can be generalized along with complexity estimates to the infinite-dimensional situation under consideration. An application is given to an important problem of control theory: multi-criteria analytic design of the linear regulator. The calculation of the Nesterov-Todd direction requires in this case solving one matrix differential Riccati equation plus solving a finite-dimensional system of linear algebraic equations on each iteration. The number of equations and unknown variables of this algebraic system is m+1, where m is a number of quadratic performance criteria.", "date": "2003", "authors": ["Leonid Faybusovich 1, Takashi Tsuchiya 2"], "references": [2020830442, 2006980285, 82689443, 10800830, 2117065890, 1981027849, 2016700492, 2042592476, 2004747456, 1543242366]}, {"id": 2020830442, "title": "A long-step primal\u2013dual algorithm for the symmetric programming problem", "abstract": "Abstract Based on the techniques of Euclidean Jordan algebras, we prove complexity estimates for a long-step primal\u2013dual interior-point algorithm for the optimization problem of the minimization of a linear function on a feasible set obtained as the intersection of an affine subspace and a symmetric cone. This result provides a meaningful illustration of a power of the technique of Euclidean Jordan algebras applied to problems under consideration.", "date": "2001", "authors": ["Leonid Faybusovich , R. Arana"], "references": [2006980285, 82689443, 10800830, 2117065890, 1981027849, 1518039036, 2016700492, 2056549531, 2150768816]}, {"id": 2006980285, "title": "Linear systems in Jordan algebras and primal-dual interior-point algorithms", "abstract": "We discuss a possibility of the extension of a primal-dual interior-point algorithm suggested recently by Alizadeh et al. (1994). We consider optimization problems defined on the intersection of a symmetric cone and an affine subspace. The question of solvability of a linear system arising in the implementation of the primal-dual algorithm is analyzed. A nondegeneracy theory for the considered class of problems is developed. The Jordan algebra technique suggested by Faybusovich (1995) plays major role in the present paper.", "date": "1997", "authors": ["Leonid Faybusovich"], "references": [10800830, 2012110988, 1980610004, 576051538, 1484725137, 2040773875, 1506692979]}, {"id": 82689443, "title": "Euclidean Jordan Algebras and Interior-point Algorithms", "abstract": "We provide an introduction to the theory of interior-point algorithms of optimization based on the theory of Euclidean Jordan algebras. A short-step path-following algorithm for the convex quadratic problem on the domain, obtained as the intersection of a symmetric cone with an affine subspace, is considered. Connections with the Linear monotone complementarity problem are discussed. Complexity estimates in terms of the rank of the corresponding Jordan algebra are obtained. Necessary results from the theory of Euclidean Jordan algebras are presented.", "date": "1997", "authors": ["Leonid Faybusovich"], "references": [2006980285, 10800830, 1518039036, 2075379600, 576051538, 1484725137, 1538534850, 2021530792, 1524254951, 1971195964]}, {"id": 2611147814, "title": "A new polynomial-time algorithm for linear programming", "abstract": "We present a new polynomial-time algorithm for linear programming. In the worst case, the algorithm requiresO(n 3.5 L) arithmetic operations onO(L) bit numbers, wheren is the number of variables andL is the number of bits in the input. The running-time of this algorithm is better than the ellipsoid algorithm by a factor ofO(n 2.5). We prove that given a polytopeP and a strictly interior point a eP, there is a projective transformation of the space that mapsP, a toP\u2032, a\u2032 having the following property. The ratio of the radius of the smallest sphere with center a\u2032, containingP\u2032 to the radius of the largest sphere with center a\u2032 contained inP\u2032 isO(n). The algorithm consists of repeated application of such projective transformations each followed by optimization over an inscribed sphere to create a sequence of points which converges to the optimal solution in polynomial time.", "date": "1984", "authors": ["N. Karmarkar"], "references": [2012329067, 2033040247, 2127470768, 2026745357, 1571051474, 3026407818]}, {"id": 2164278908, "title": "Distributed Optimization and Statistical Learning Via the Alternating Direction Method of Multipliers", "abstract": "Many problems of recent interest in statistics and machine learning can be posed in the framework of convex optimization. Due to the explosion in size and complexity of modern datasets, it is increasingly important to be able to solve problems with a very large number of features or training examples. As a result, both the decentralized collection or storage of these datasets as well as accompanying distributed solution methods are either necessary or at least highly desirable. In this review, we argue that the alternating direction method of multipliers is well suited to distributed convex optimization, and in particular to large-scale problems arising in statistics, machine learning, and related areas. The method was developed in the 1970s, with roots in the 1950s, and is equivalent or closely related to many other algorithms, such as dual decomposition, the method of multipliers, Douglas\u2013Rachford splitting, Spingarn's method of partial inverses, Dykstra's alternating projections, Bregman iterative algorithms for l1 problems, proximal methods, and others. After briefly surveying the theory and history of the algorithm, we discuss applications to a wide variety of statistical and machine learning problems of recent interest, including the lasso, sparse logistic regression, basis pursuit, covariance selection, support vector machines, and many others. We also discuss general distributed optimization, extensions to the nonconvex setting, and efficient implementation, including some details on distributed MPI and Hadoop MapReduce implementations.", "date": "2011", "authors": ["Stephen Boyd 1, Neal Parikh 1, Eric Chu 1, Borja Peleato 1, Jonathan Eckstein 2"], "references": [2296319761, 2173213060, 2156909104, 2296616510, 2145096794, 1554944419, 3029645440, 2129638195, 2100556411, 1981420413]}, {"id": 3029645440, "title": "Numerical Optimization", "abstract": "Numerical Optimization presents a comprehensive and up-to-date description of the most effective methods in continuous optimization. It responds to the growing interest in optimization in engineering, science, and business by focusing on the methods that are best suited to practical problems. For this new edition the book has been thoroughly updated throughout. There are new chapters on nonlinear interior methods and derivative-free methods for optimization, both of which are used widely in practice and the focus of much current research. Because of the emphasis on practical methods, as well as the extensive illustrations and exercises, the book is accessible to a wide audience. It can be used as a graduate text in engineering, operations research, mathematics, computer science, and business. It also serves as a handbook for researchers and practitioners in the field. The authors have strived to produce a text that is pleasant to read, informative, and rigorous - one that reveals both the beautiful nature of the discipline and its practical side.", "date": "2005", "authors": ["Jorge Nocedal 1, Stephen J. Wright 2"], "references": [2152195021, 2798766386, 2109364787, 2610857016, 2151554678, 2124541940, 2077658674, 391578156, 1576347883, 2162737890]}, {"id": 2100556411, "title": "A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Problems", "abstract": "We consider the class of iterative shrinkage-thresholding algorithms (ISTA) for solving linear inverse problems arising in signal/image processing. This class of methods, which can be viewed as an extension of the classical gradient algorithm, is attractive due to its simplicity and thus is adequate for solving large-scale problems even with dense matrix data. However, such methods are also known to converge quite slowly. In this paper we present a new fast iterative shrinkage-thresholding algorithm (FISTA) which preserves the computational simplicity of ISTA but with a global rate of convergence which is proven to be significantly better, both theoretically and practically. Initial promising numerical results for wavelet-based image deblurring demonstrate the capabilities of FISTA which is shown to be faster than ISTA by several orders of magnitude.", "date": "2008", "authors": ["Amir Beck 1, Marc Teboulle 2"], "references": [2078204800, 2798766386, 2109449402, 2115706991, 2028349405, 2079724595, 1543439990, 2006262045, 2126607811, 1568307856]}, {"id": 1964357740, "title": "A tutorial on support vector regression", "abstract": "In this tutorial we give an overview of the basic ideas underlying Support Vector (SV) machines for function estimation. Furthermore, we include a summary of currently used algorithms for training SV machines, covering both the quadratic (or convex) programming part and advanced methods for dealing with large datasets. Finally, we mention some modifications and extensions that have been applied to the standard SV algorithm, and discuss the aspect of regularization from a SV perspective.", "date": "2004", "authors": ["Alex J. Smola 1, Bernhard Sch\u00f6lkopf 2"], "references": [2153635508, 2156909104, 2148603752, 2124776405, 2331432542, 1554663460, 2119821739, 2139212933, 1563088657, 3023786531]}, {"id": 2202343345, "title": "Guaranteed Minimum-Rank Solutions of Linear Matrix Equations via Nuclear Norm Minimization", "abstract": "", "date": "2010", "authors": ["Benjamin Recht , Maryam Fazel , Pablo A. Parrilo"], "references": [2011359124, 131378802, 2156739854, 3104684837, 2041101642, 2146616964, 2906621894, 648260396, 1982059935, 2100875869]}, {"id": 2109449402, "title": "Gradient Projection for Sparse Reconstruction: Application to Compressed Sensing and Other Inverse Problems", "abstract": "Many problems in signal processing and statistical inference involve finding sparse solutions to under-determined, or ill-conditioned, linear systems of equations. A standard approach consists in minimizing an objective function which includes a quadratic (squared ) error term combined with a sparseness-inducing regularization term. Basis pursuit, the least absolute shrinkage and selection operator (LASSO), wavelet-based deconvolution, and compressed sensing are a few well-known examples of this approach. This paper proposes gradient projection (GP) algorithms for the bound-constrained quadratic programming (BCQP) formulation of these problems. We test variants of this approach that select the line search parameters in different ways, including techniques based on the Barzilai-Borwein method. Computational experiments show that these GP approaches perform well in a wide range of applications, often being significantly faster (in terms of computation time) than competing methods. Although the performance of GP methods tends to degrade as the regularization term is de-emphasized, we show how they can be embedded in a continuation scheme to recover their efficient practical performance.", "date": "2007", "authors": ["M.A.T. Figueiredo , R.D. Nowak , S.J. Wright"], "references": [2296616510, 2145096794, 3029645440, 2115755118, 2129638195, 2135046866, 2164452299, 2063978378, 2078204800, 2101675075]}, {"id": 1601740268, "title": "An introduction to Support Vector Machines", "abstract": "This book is the first comprehensive introduction to Support Vector Machines (SVMs), a new generation learning system based on recent advances in statistical learning theory. The book also introduces Bayesian analysis of learning and relates SVMs to Gaussian Processes and other kernel based learning methods. SVMs deliver state-of-the-art performance in real-world applications such as text categorisation, hand-written character recognition, image classification, biosequences analysis, etc. Their first introduction in the early 1990s lead to a recent explosion of applications and deepening theoretical analysis, that has now established Support Vector Machines along with neural networks as one of the standard tools for machine learning and data mining. Students will find the book both stimulating and accessible, while practitioners will be guided smoothly through the material required for a good grasp of the theory and application of these techniques. The concepts are introduced gradually in accessible and self-contained stages, though in each stage the presentation is rigorous and thorough. Pointers to relevant literature and web sites containing software ensure that it forms an ideal starting point for further study. Equally the book will equip the practitioner to apply the techniques and an associated web site will provide pointers to updated literature, new applications, and on-line software.", "date": "2000", "authors": ["Nello Cristianini , J Shawe-Taylor"], "references": [2153635508, 2798766386, 1560724230, 1485732691, 100063776]}, {"id": 2132870739, "title": "Estimating the Support of a High-Dimensional Distribution", "abstract": "Suppose you are given some data set drawn from an underlying probability distribution P and you want to estimate a \"simple\" subset S of input space such that the probability that a test point drawn from P lies outside of S equals some a priori specified value between 0 and 1. We propose a method to approach this problem by trying to estimate a function f that is positive on S and negative on the complement. The functional form of f is given by a kernel expansion in terms of a potentially small subset of the training data; it is regularized by controlling the length of the weight vector in an associated feature space. The expansion coefficients are found by solving a quadratic programming problem, which we do by carrying out sequential optimization over pairs of input patterns. We also provide a theoretical analysis of the statistical performance of our algorithm. The algorithm is a natural extension of the support vector algorithm to the case of unlabeled data.", "date": "2001", "authors": ["Bernhard Sch\u00f6lkopf 1, John C. Platt 1, John C. Shawe-Taylor 2, Alex J. Smola 3, Robert C. Williamson 3"], "references": [2156909104, 2148603752, 2099111195, 2798766386, 3021469268, 1512098439, 2087347434, 1604938182, 2108995755, 1576520375]}, {"id": 2107396783, "title": "Consensus problems in networks of agents with switching topology and time-delays", "abstract": "In this paper, we discuss consensus problems for networks of dynamic agents with fixed and switching topologies. We analyze three cases: 1) directed networks with fixed topology; 2) directed networks with switching topology; and 3) undirected networks with communication time-delays and fixed topology. We introduce two consensus protocols for networks with and without time-delays and provide a convergence analysis in all three cases. We establish a direct connection between the algebraic connectivity (or Fiedler eigenvalue) of the network and the performance (or negotiation speed) of a linear consensus protocol. This required the generalization of the notion of algebraic connectivity of undirected graphs to digraphs. It turns out that balanced digraphs play a key role in addressing average-consensus problems. We introduce disagreement functions for convergence analysis of consensus protocols. A disagreement function is a Lyapunov function for the disagreement network dynamics. We proposed a simple disagreement function that is a common Lyapunov function for the disagreement dynamics of a directed network with switching topology. A distinctive feature of this work is to address consensus problems for networks with directed information flow. We provide analytical tools that rely on algebraic graph theory, matrix theory, and control theory. Simulations are provided that demonstrate the effectiveness of our theoretical results.", "date": "2004", "authors": ["R. Olfati-Saber , R.M. Murray"], "references": [2165744313, 2146890818, 2610857016, 2164727176, 247697463, 2798588639, 1888172398, 2074796812, 2611515161, 1864806140]}, {"id": 2118040894, "title": "Space-time codes for high data rate wireless communication: performance criterion and code construction", "abstract": "We consider the design of channel codes for improving the data rate and/or the reliability of communications over fading channels using multiple transmit antennas. Data is encoded by a channel code and the encoded data is split into n streams that are simultaneously transmitted using n transmit antennas. The received signal at each receive antenna is a linear superposition of the n transmitted signals perturbed by noise. We derive performance criteria for designing such codes under the assumption that the fading is slow and frequency nonselective. Performance is shown to be determined by matrices constructed from pairs of distinct code sequences. The minimum rank among these matrices quantifies the diversity gain, while the minimum determinant of these matrices quantifies the coding gain. The results are then extended to fast fading channels. The design criteria are used to design trellis codes for high data rate wireless communication. The encoding/decoding complexity of these codes is comparable to trellis codes employed in practice over Gaussian channels. The codes constructed here provide the best tradeoff between data rate, diversity advantage, and trellis complexity. Simulation results are provided for 4 and 8 PSK signal sets with data rates of 2 and 3 bits/symbol, demonstrating excellent performance that is within 2-3 dB of the outage capacity for these channels using only 64 state encoders.", "date": "1998", "authors": ["V. Tarokh 1, N. Seshadri 2, A.R. Calderbank 2"], "references": [2130509920, 1667950888, 2610857016, 2330078975, 2085099144, 1596939795, 2021573106, 2117507580, 1606480398, 2165205968]}, {"id": 2139212933, "title": "A Tutorial on Support Vector Machines for Pattern Recognition", "abstract": "The tutorial starts with an overview of the concepts of VC dimension and structural risk minimization. We then describe linear Support Vector Machines (SVMs) for separable and non-separable data, working through a non-trivial example in detail. We describe a mechanical analogy, and discuss when SVM solutions are unique and when they are global. We describe how support vector training can be practically implemented, and discuss in detail the kernel mapping technique which is used to construct SVM solutions which are nonlinear in the data. We show how Support Vector machines can have very large (even infinite) VC dimension by computing the VC dimension for homogeneous polynomial and Gaussian radial basis function kernels. While very high VC dimension would normally bode ill for generalization performance, and while at present there exists no theory which shows that good generalization performance is guaranteed for SVMs, there are several arguments which support the observed high accuracy of SVMs, which we review. Results of some experiments which were inspired by these arguments are also presented. We give numerous examples and proofs of most of the key theorems. There is new material, and I hope that the reader will find that even old material is cast in a fresh light.", "date": "1998", "authors": ["Christopher J. C. Burges"], "references": [2156909104, 2148603752, 1554663460, 2119821739, 2313307644, 2610857016, 2140095548, 2797532987, 2087347434, 740415]}, {"id": 2160643434, "title": "Consensus and Cooperation in Networked Multi-Agent Systems", "abstract": "This paper provides a theoretical framework for analysis of consensus algorithms for multi-agent networked systems with an emphasis on the role of directed information flow, robustness to changes in network topology due to link/node failures, time-delays, and performance guarantees. An overview of basic concepts of information consensus in networks and methods of convergence and performance analysis for the algorithms are provided. Our analysis framework is based on tools from matrix theory, algebraic graph theory, and control theory. We discuss the connections between consensus problems in networked dynamic systems and diverse applications including synchronization of coupled oscillators, flocking, formation control, fast consensus in small-world networks, Markov processes and gossip-based algorithms, load balancing in networks, rendezvous in space, distributed sensor fusion in sensor networks, and belief propagation. We establish direct connections between spectral and structural properties of complex networks and the speed of information diffusion of consensus algorithms. A brief introduction is provided on networked systems with nonlocal information flow that are considerably faster than distributed systems with lattice-type nearest neighbor interactions. Simulation results are presented that demonstrate the role of small-world effects on the speed of consensus algorithms and cooperative control of multivehicle formations", "date": "2007", "authors": ["R. Olfati-Saber 1, J.A. Fax 2, R.M. Murray 3"], "references": [2112090702, 2008620264, 2148606196, 2107396783, 2165744313, 2146890818, 2610857016, 2099175737, 2105850748, 247697463]}, {"id": 2165744313, "title": "Coordination of groups of mobile autonomous agents using nearest neighbor rules", "abstract": "In a recent Physical Review Letters article, Vicsek et al. propose a simple but compelling discrete-time model of n autonomous agents (i.e., points or particles) all moving in the plane with the same speed but with different headings. Each agent's heading is updated using a local rule based on the average of its own heading plus the headings of its \"neighbors.\" In their paper, Vicsek et al. provide simulation results which demonstrate that the nearest neighbor rule they are studying can cause all agents to eventually move in the same direction despite the absence of centralized coordination and despite the fact that each agent's set of nearest neighbors change with time as the system evolves. This paper provides a theoretical explanation for this observed behavior. In addition, convergence results are derived for several other similarly inspired models. The Vicsek model proves to be a graphic example of a switched linear system which is stable, but for which there does not exist a common quadratic Lyapunov function.", "date": "2003", "authors": ["A. Jadbabaie , Jie Lin , A.S. Morse"], "references": [2610857016, 247697463, 2176056341, 2015410655, 2130647945, 391578156, 2165469059, 390146837, 2147928602, 2060203271]}, {"id": 2151795416, "title": "On the achievable throughput of a multiantenna Gaussian broadcast channel", "abstract": "A Gaussian broadcast channel (GBC) with r single-antenna receivers and t antennas at the transmitter is considered. Both transmitter and receivers have perfect knowledge of the channel. Despite its apparent simplicity, this model is, in general, a nondegraded broadcast channel (BC), for which the capacity region is not fully known. For the two-user case, we find a special case of Marton's (1979) region that achieves optimal sum-rate (throughput). In brief, the transmitter decomposes the channel into two interference channels, where interference is caused by the other user signal. Users are successively encoded, such that encoding of the second user is based on the noncausal knowledge of the interference caused by the first user. The crosstalk parameters are optimized such that the overall throughput is maximum and, surprisingly, this is shown to be optimal over all possible strategies (not only with respect to Marton's achievable region). For the case of r>2 users, we find a somewhat simpler choice of Marton's region based on ordering and successively encoding the users. For each user i in the given ordering, the interference caused by users j>i is eliminated by zero forcing at the transmitter, while interference caused by users j<i is taken into account by coding for noncausally known interference. Under certain mild conditions, this scheme is found to be throughput-wise asymptotically optimal for both high and low signal-to-noise ratio (SNR). We conclude by providing some numerical results for the ergodic throughput of the simplified zero-forcing scheme in independent Rayleigh fading.", "date": "2003", "authors": ["G. Caire 1, S. Shamai 2"], "references": [2137775453, 2130509920, 2099111195, 2610857016, 2161872841, 1989225545, 2098257210, 2116485279, 391578156, 2111992817]}, {"id": 1568307856, "title": "Lectures on modern convex optimization: analysis, algorithms, and engineering applications", "abstract": "This is a book devoted to well-structured and thus efficiently solvable convex optimization problems, with emphasis on conic quadratic and semidefinite programming. The authors present the basic theory underlying these problems as well as their numerous applications in engineering, including synthesis of filters, Lyapunov stability analysis, and structural design. The authors also discuss the complexity issues and provide an overview of the basic theory of state-of-the-art polynomial time interior point methods for linear, conic quadratic, and semidefinite programming. The book's focus on well-structured convex problems in conic form allows for unified theoretical and algorithmical treatment of a wide spectrum of important optimization problems arising in applications.", "date": "2001", "authors": ["Aharon Ben-Tal , Arkadiaei Semenovich Nemirovskiaei"], "references": [2100556411, 2167732364, 2074796812, 2103519107, 2397723600, 2113642685, 2950077417, 2142949395, 2023722580]}, {"id": 1553702074, "title": "Convex analysis and minimization algorithms", "abstract": "IX. Inner Construction of the Subdifferential.- X. Conjugacy in Convex Analysis.- XI. Approximate Subdifferentials of Convex Functions.- XII. Abstract Duality for Practitioners.- XIII. Methods of ?-Descent.- XIV. Dynamic Construction of Approximate Subdifferentials: Dual Form of Bundle Methods.- XV. Acceleration of the Cutting-Plane Algorithm: Primal Forms of Bundle Methods.- Bibliographical Comments.- References.", "date": "1993", "authors": ["Jean-Baptiste Hiriart-Urruty , Claude Lemar\u00e9chal"], "references": [2146502635, 2120340025, 2167732364, 607505555, 2074796812, 2963277051, 2117905067, 1946620893, 2148087609]}, {"id": 1568288633, "title": "Introduction to optimization", "abstract": "", "date": "1986", "authors": ["B. T. Poli\ufe20a\ufe21k"], "references": [2100556411, 2167732364, 2913535645, 2142224912, 3022380717, 2044535354, 2000769684, 2012445782, 2963794891]}, {"id": 1669104078, "title": "Constrained Optimization and Lagrange Multiplier Methods", "abstract": "", "date": "1982", "authors": ["Dimitri P. Bertsekas"], "references": [2164278908, 2145962650, 2202343345, 2000982976, 1997201895, 2167732364, 1506281249, 2913535645, 1736339626]}, {"id": 2015263936, "title": "Minimization methods for non-differentiable functions", "abstract": "", "date": "1984", "authors": ["N. Z. Shor 1, Krzysztof C. Kiwiel 2, Andrzej Ruszcay\u01f9ski 3"], "references": [2164278908, 2167732364, 2161272050, 2099839128, 2162986857, 2101727634, 2131350052, 2147730843, 2134711723, 2169498096]}, {"id": 2150126561, "title": "Nonlinear rescaling vs. smoothing technique in convex optimization", "abstract": "We introduce an alternative to the smoothing technique approach for constrained optimization. As it turns out for any given smoothing function there exists a modification with particular properties. We use the modification for Nonlinear Rescaling (NR) the constraints of a given constrained optimization problem into an equivalent set of constraints.\u00b6The constraints transformation is scaled by a vector of positive parameters. The Lagrangian for the equivalent problems is to the correspondent Smoothing Penalty functions as Augmented Lagrangian to the Classical Penalty function or MBFs to the Barrier Functions. Moreover the Lagrangians for the equivalent problems combine the best properties of Quadratic and Nonquadratic Augmented Lagrangians and at the same time are free from their main drawbacks.\u00b6Sequential unconstrained minimization of the Lagrangian for the equivalent problem in primal space followed by both Lagrange multipliers and scaling parameters update leads to a new class of NR multipliers methods, which are equivalent to the Interior Quadratic Prox methods for the dual problem.\u00b6We proved convergence and estimate the rate of convergence of the NR multipliers method under very mild assumptions on the input data. We also estimate the rate of convergence under various assumptions on the input data.\u00b6In particular, under the standard second order optimality conditions the NR method converges with Q-linear rate without unbounded increase of the scaling parameters, which correspond to the active constraints.\u00b6We also established global quadratic convergence of the NR methods for Linear Programming with unique dual solution.\u00b6We provide numerical results, which strongly support the theory.", "date": "2002", "authors": ["Roman A. Polyak"], "references": [2798766386, 1568288633, 2038497950, 2014746566, 2099679613, 2036368324, 2016939833, 2042360648, 2135779729, 2032690622]}, {"id": 2969945254, "title": "A method for unconstrained convex minimization problem with the rate of convergence o(1/k^2)", "abstract": "", "date": "1982", "authors": ["Y. Nesterov"], "references": [2963114950, 2167732364, 2523246573, 2095984592, 1974042113, 2023722580, 2963156201, 2108563286]}, {"id": 2008164266, "title": "On convergence rates of subgradient optimization methods", "abstract": "Rates of convergence of subgradient optimization are studied. If the step size is chosen to be a geometric progression with ratio\u03c1 the convergence, if it occurs, is geometric with rate\u03c1. For convergence to occur, it is necessary that the initial step size be large enough, and that the ratio\u03c1 be greater than a sustainable ratez(\u03bc), which depends upon a condition number\u03bc, defined for both differentiable and nondifferentiable functions. The sustainable ratez(\u03bc) is closely related to the rate of convergence of the steepest ascent method for differentiable functions: in fact it is identical if the function is not too well conditioned.", "date": "1977", "authors": ["Jean-Louis Goffin"], "references": [2071609006, 2017697298, 2058748455, 1948965050, 2008571296, 2013243898, 2130469840, 2329579984, 44132750, 11112849]}, {"id": 2149684865, "title": "Text Categorization with Suport Vector Machines: Learning with Many Relevant Features", "abstract": "This paper explores the use of Support Vector Machines (SVMs) for learning text classifiers from examples. It analyzes the particular properties of learning with text data and identifies why SVMs are appropriate for this task. Empirical results support the theoretical findings. SVMs achieve substantial improvements over the currently best performing methods and behave robustly over a variety of different learning tasks. Furthermore they are fully automatic, eliminating the need for manual parameter tuning.", "date": "1998", "authors": ["Thorsten Joachims"], "references": [2156909104, 2119821739, 2125055259, 2435251607, 740415, 1504694836, 2114535528, 1978394996, 2096152098, 2087614174]}, {"id": 2098162425, "title": "An algorithm for suffix stripping", "abstract": "The automatic removal of suffixes from words in English is of particular interest in the field of information retrieval. An algorithm for suffix stripping is described, which has been implemented as a short, fast program in BCPL. Although simple, it performs slightly better than a much more elaborate system with which it has been compared. It effectively works by treating complex suffixes as compounds made up of simple suffixes, and removing the simple suffixes in a number of steps. In each step the removal of the suffix is made to depend upon the form of the remaining stem, which usually involves a measure of its syllable length.", "date": "1997", "authors": ["M. F. Porter"], "references": [26591655, 1495821370, 1988344511, 2005095359]}, {"id": 2118202495, "title": "Practical statistics for medical research", "abstract": "Most medical researchers, whether clinical or non-clinical, receive some background in statistics as undergraduates. However, it is most often brief, a long time ago, and largely forgotten by the time it is needed. Furthermore, many introductory texts fall short of adequately explaining the underlying concepts of statistics, and often are divorced from the reality of conducting and assessing medical research. Practical Statistics for Medical Research is a problem-based text for medical researchers, medical students, and others in the medical arena who need to use statistics but have no specialized mathematics background. The author draws on twenty years of experience as a consulting medical statistician to provide clear explanations to key statistical concepts, with a firm emphasis on practical aspects of designing and analyzing medical research. The text gives special attention to the presentation and interpretation of results and the many real problems that arise in medical research", "date": "1990", "authors": ["Douglas G. Altman"], "references": [2115988342, 2150102617, 1987138256, 2171811563, 3021329446, 2170021941, 2146272590, 1986622667, 1531237901, 1981501248]}, {"id": 2435251607, "title": "A Comparative Study on Feature Selection in Text Categorization", "abstract": "", "date": "1997", "authors": ["Yiming Yang , Jan O. Pedersen"], "references": [2118020653, 2149684865, 2150102617, 2097089247, 2167101736, 2103333826, 2114535528, 2005422315, 2052684427, 1530276735]}, {"id": 2114535528, "title": "An Evaluation of Statistical Approaches to Text Categorization", "abstract": "This paper focuses on a comparative evaluation of a wide-range of text categorization methods, including previously published results on the Reuters corpus and new results of additional experiments. A controlled study using three classifiers, kNN, LLSF and WORD, was conducted to examine the impact of configuration variations in five versions of Reuters on the observed performance of classifiers. Analysis and empirical evidence suggest that the evaluation results on some versions of Reuters were significantly affected by the inclusion of a large portion of unlabelled documents, mading those results difficult to interpret and leading to considerable confusions in the literature. Using the results evaluated on the other versions of Reuters which exclude the unlabelled documents, the performance of twelve methods are compared directly or indirectly. For indirect compararions, kNN, LLSF and WORD were used as baselines, since they were evaluated on all versions of Reuters that exclude the unlabelled documents. As a global observation, kNN, LLSF and a neural network method had the best performances except for a Naive Bayes approach, the other learning algorithms also performed relatively well.", "date": "1999", "authors": ["Yiming Yang"], "references": [2149706766, 2435251607, 1956559956, 1833785989, 2060216474, 1969572066, 2063198646, 1983078185, 1986913017, 2071664212]}, {"id": 2005422315, "title": "A re-examination of text categorization methods", "abstract": "", "date": "1999", "authors": ["Yiming Yang , Xin Liu"], "references": [2156909104, 2119821739, 2149684865, 2435251607, 2114535528, 1550206324, 2164641162, 2137346077, 1620204465, 2064580901]}, {"id": 2768149277, "title": "Support Vector Machine", "abstract": "Support Vector Machine is one of the classical machine learning techniques that can still help solve big data classification problems. Especially, it can help the multidomain applications in a big data environment. However, the support vector machine is mathematically complex and computationally expensive. The main objective of this chapter is to simplify this approach using process diagrams and data flow diagrams to help readers understand theory and implement it successfully. To achieve this objective, the chapter is divided into three parts: (1) modeling of a linear support vector machine; (2) modeling of a nonlinear support vector machine; and (3) Lagrangian support vector machine algorithm and its implementations. The Lagrangian support vector machine with simple examples is also implemented using the R programming platform on Hadoop and non-Hadoop systems.", "date": "2016", "authors": ["Shan Suthaharan"], "references": [2156909104, 2148603752, 2162915993, 1480376833, 2135046866, 1563088657, 2340020088, 2138019504, 2020925091, 2074682976]}, {"id": 2000672666, "title": "Improving Retrieval Performance by Relevance Feedback", "abstract": "Relevance feedback is an automatic process, introduced over 20 years ago, designed to produce query formulations following an initial retrieval operation. The principal relevance feedback methods described over the years are examined briefly, and evaluation data are included to demonstrate the effectiveness of the various methods. Prescriptions are given for conducting text retrieval operations iteratively using relevance feedback.", "date": "1997", "authors": ["Gerard Salton , Chris Buckley"], "references": [1978394996, 2043909051, 2019087979, 2164547069, 2026937586, 2053039612, 120261275, 2030350774, 2174678383, 1555286071]}, {"id": 2092663520, "title": "A First-Order Primal-Dual Algorithm for Convex Problems with Applications to Imaging", "abstract": "In this paper we study a first-order primal-dual algorithm for non-smooth convex optimization problems with known saddle-point structure. We prove convergence to a saddle-point with rate O(1/N) in finite dimensions for the complete class of problems. We further show accelerations of the proposed algorithm to yield improved rates on problems with some degree of smoothness. In particular we show that we can achieve O(1/N 2) convergence on problems, where the primal or the dual objective is uniformly convex, and we can show linear convergence, i.e. O(? N ) for some ??(0,1), on smooth problems. The wide applicability of the proposed algorithm is demonstrated on several imaging problems such as image denoising, image deconvolution, image inpainting, motion estimation and multi-label image segmentation.", "date": "2011", "authors": ["Antonin Chambolle 1, Thomas Pock 2"], "references": [2100556411, 2103559027, 2167732364, 2115528090, 2124541940, 2005089986, 2146482778, 1578985305, 2114487471, 2089559088]}, {"id": 2473418344, "title": "Deep Learning with Differential Privacy", "abstract": "Machine learning techniques based on neural networks are achieving remarkable results in a wide variety of domains. Often, the training of models requires large, representative datasets, which may be crowdsourced and contain sensitive information. The models should not expose private information in these datasets. Addressing this goal, we develop new algorithmic techniques for learning and a refined analysis of privacy costs within the framework of differential privacy. Our implementation and experiments demonstrate that we can train deep neural networks with non-convex objectives, under a modest privacy budget, and at a manageable cost in software complexity, training efficiency, and model quality.", "date": "2016", "authors": ["Martin Abadi 1, Andy Chu 1, Ian Goodfellow 2, H. Brendan McMahan 1, Ilya Mironov 1, Kunal Talwar 1, Li Zhang 1"], "references": [2618530766, 2097117768, 1614298861, 2250539671, 1677182931, 2146502635, 2257979135, 2310919327, 2271840356, 2546302380]}, {"id": 2107438106, "title": "Accelerating Stochastic Gradient Descent using Predictive Variance Reduction", "abstract": "Stochastic gradient descent is popular for large scale optimization but has slow convergence asymptotically due to the inherent variance. To remedy this problem, we introduce an explicit variance reduction method for stochastic gradient descent which we call stochastic variance reduced gradient (SVRG). For smooth and strongly convex functions, we prove that this method enjoys the same fast convergence rate as those of stochastic dual coordinate ascent (SDCA) and Stochastic Average Gradient (SAG). However, our analysis is significantly simpler and more intuitive. Moreover, unlike SDCA or SAG, our method does not require the storage of gradients, and thus is more easily applicable to complex problems such as some structured prediction problems and neural network learning.", "date": "2013", "authors": ["Rie Johnson 1, Tong Zhang 2"], "references": [2165966284, 2124541940, 2142623206, 2125993116, 1939652453, 2963156201, 2091825929, 2120350480]}, {"id": 607505555, "title": "Understanding Machine Learning: From Theory to Algorithms", "abstract": "Machine learning is one of the fastest growing areas of computer science, with far-reaching applications. The aim of this textbook is to introduce machine learning, and the algorithmic paradigms it offers, in a principled way. The book provides an extensive theoretical account of the fundamental ideas underlying machine learning and the mathematical derivations that transform these principles into practical algorithms. Following a presentation of the basics of the field, the book covers a wide array of central topics that have not been addressed by previous textbooks. These include a discussion of the computational complexity of learning and the concepts of convexity and stability; important algorithmic paradigms including stochastic gradient descent, neural networks, and structured output learning; and emerging theoretical concepts such as the PAC-Bayes approach and compression-based bounds. Designed for an advanced undergraduate or beginning graduate course, the text makes the fundamentals and algorithms of machine learning accessible to students and non-expert readers in statistics, computer science, mathematics, and engineering.", "date": "2014", "authors": ["Shai Shalev-Shwartz 1, Shai Ben-David 2"], "references": [2296319761, 2156909104, 2911964244, 1663973292, 2296616510, 2136922672, 2148603752, 2129131372, 1480376833, 2147880316]}, {"id": 2913535645, "title": "Proximal Algorithms", "abstract": "This monograph is about a class of optimization algorithms called proximal algorithms. Much like Newton's method is a standard tool for solving unconstrained smooth optimization problems of modest size, proximal algorithms can be viewed as an analogous tool for nonsmooth, constrained, large-scale, or distributed versions of these problems. They are very generally applicable, but are especially well-suited to problems of substantial recent interest involving large or high-dimensional datasets. Proximal methods sit at a higher level of abstraction than classical algorithms like Newton's method: the base operation is evaluating the proximal operator of a function, which itself involves solving a small convex optimization problem. These subproblems, which generalize the problem of projecting a point onto a convex set, often admit closed-form solutions or can be solved very quickly with standard or simple specialized methods. Here, we discuss the many different interpretations of proximal operators and algorithms, describe their connections to many other topics in optimization and applied mathematics, survey some popular algorithms, and provide a large number of examples of proximal operators that commonly arise in practice.", "date": "2013", "authors": ["Neal Parikh , Stephen Boyd"], "references": [2296319761, 2164278908, 3029645440, 2100556411, 2135046866, 2122825543, 2078204800, 2103972604, 2146842127, 2138019504]}, {"id": 2038669746, "title": "A comparison of three methods for selecting values of input variables in the analysis of output from a computer code", "abstract": "Two types of sampling plans are examined as alternatives to simple random sampling in Monte Carlo studies. These plans are shown to be improvements over simple random sampling with respect to variance for a class of estimators which includes the sample mean and the empirical distribution function.", "date": "2000", "authors": ["M. D. McKay 1, R. J. Beckman 1, W. J. Conover 2"], "references": [3041461815, 2096992152, 2116414161, 1638594382, 2059347608, 2330498442, 2022498557, 2090895976, 1977573324, 2010374567]}, {"id": 2169713291, "title": "Primal-dual subgradient methods for convex problems", "abstract": "In this paper we present a new approach for constructing subgradient schemes for different types of nonsmooth problems with convex structure. Our methods are primal-dual since they are always able to generate a feasible approximation to the optimum of an appropriately formulated dual problem. Besides other advantages, this useful feature provides the methods with a reliable stopping criterion. The proposed schemes differ from the classical approaches (divergent series methods, mirror descent methods) by presence of two control sequences. The first sequence is responsible for aggregating the support functions in the dual space, and the second one establishes a dynamically updated scale between the primal and dual spaces. This additional flexibility allows to guarantee a boundedness of the sequence of primal test points even in the case of unbounded feasible set (however, we always assume the uniform boundedness of subgradients). We present the variants of subgradient schemes for nonsmooth convex minimization, minimax problems, saddle point problems, variational inequalities, and stochastic optimization. In all situations our methods are proved to be optimal from the view point of worst-case black-box lower complexity bounds.", "date": "2009", "authors": ["Yurii Nesterov"], "references": []}, {"id": 203276351, "title": "Introduction to Stochastic Search and Optimization", "abstract": "From the Publisher: * Unique in its survey of the range of topics. * Contains a strong, interdisciplinary format that will appeal to both students and researchers. * Features exercises and web links to software and data sets.", "date": "2003", "authors": ["James C. Spall"], "references": [3029645440, 2166851633, 2225156818, 1992208280, 2142623206, 1977655452, 2125993116, 2077723394, 2129160848, 2024484010]}, {"id": 2064076655, "title": "Introduction to Stochastic Search and Optimization. Estimation, Simulation, and Control (Spall, J.C.; 2003) [book review]", "abstract": "This comprehensive book offers 504 main pages divided into 17 chapters. In addition, five very useful and clearly written appendices are provided, covering multivariate analysis, basic tests in statistics, probability theory and convergence, random number generators and Markov processes. Some of the topics covered in the book include: stochastic approximation in nonlinear search and optimization; evolutionary computations; reinforcement learning via temporal differences; mathematical model selection; and computer-simulation-based optimizations. Over 250 exercises are provided in the book, though only a small number of them have solutions included in the volume. A separate solution manual is available, as is a very informative webpage. The book may serve as either a reference for researchers and practitioners in many fields or as an excellent graduate level textbook.", "date": "2007", "authors": ["W. Nowak"], "references": [2166851633, 2225156818, 1992208280, 2001979953, 2077723394, 2024484010, 2585392941, 2963470657, 2148080122]}, {"id": 1983916623, "title": "The Sample Average Approximation Method for Stochastic Discrete Optimization", "abstract": "In this paper we study a Monte Carlo simulation--based approach to stochastic discrete optimization problems. The basic idea of such methods is that a random sample is generated and the expected value function is approximated by the corresponding sample average function. The obtained sample average optimization problem is solved, and the procedure is repeated several times until a stopping criterion is satisfied. We discuss convergence rates, stopping rules, and computational complexity of this procedure and present a numerical example for the stochastic knapsack problem.", "date": "2002", "authors": ["Anton J. Kleywegt , Alexander Shapiro , Tito Homem-de-Mello"], "references": [1569990960, 2059120410, 2796586415, 1997917263, 2000953623, 2085300404, 2017411673, 1965288387, 1967331190, 2115535801]}, {"id": 2090359754, "title": "A new approach to the maximum-flow problem", "abstract": "All previously known efficient maximum-flow algorithms work by finding augmenting paths, either one path at a time (as in the original Ford and Fulkerson algorithm) or all shortest-length augmenting paths at once (using the layered network approach of Dinic). An alternative method based on the preflow concept of Karzanov is introduced. A preflow is like a flow, except that the total amount flowing into a vertex is allowed to exceed the total amount flowing out. The method maintains a preflow in the original network and pushes local flow excess toward the sink along what are estimated to be shortest paths. The algorithm and its analysis are simple and intuitive, yet the algorithm runs as fast as any other known method on dense graphs, achieving an O ( n 3 ) time bound on an n -vertex graph. By incorporating the dynamic tree data structure of Sleator and Tarjan, we obtain a version of the algorithm running in O ( nm log( n 2 / m )) time on an n -vertex, m -edge graph. This is as fast as any known method for any graph density and faster on graphs of moderate density. The algorithm also admits efficient distributed and parallel implementations. A parallel implementation running in O ( n 2 log n ) time using n processors and O ( m ) space is obtained. This time bound matches that of the Shiloach-Vishkin algorithm, which also uses n processors but requires O ( n 2 ) space.", "date": "1988", "authors": ["Andrew V. Goldberg 1, Robert E. Tarjan 2"], "references": [2053913299, 1513400187, 2090359754, 2125690626, 2130055503, 2340365916, 2077371116, 2092534058, 1964119857, 2018804864]}, {"id": 2000257769, "title": "The empirical behavior of sampling methods for stochastic programming", "abstract": "We investigate the quality of solutions obtained from sample-average approximations to two-stage stochastic linear programs with recourse. We use a recently developed software tool executing on a computational grid to solve many large instances of these problems, allowing us to obtain high-quality solutions and to verify optimality and near-optimality of the computed solutions in various ways.", "date": "2006", "authors": ["Jeff T. Linderoth 1, Alexander Shapiro 2, Stephen J. Wright 3"], "references": [2091257550, 2038669746, 2786741601, 1983916623, 2000953623, 2161292693, 2127470768, 1518930036, 2063790063, 2015616421]}, {"id": 1490324987, "title": "Monte Carlo Sampling Methods", "abstract": "Abstract In this chapter we discuss Monte Carlo sampling methods for solving large scale stochastic programming problems. We concentrate on the \u201cexterior\u201d approach where a random sample is generated outside of an optimization procedure, and then the constructed, so-called sample average approximation (SAA), problem is solved by an appropriate deterministic algorithm. We study statistical properties of the obtained SAA estimators. The developed statistical inference is incorporated into validation analysis and error estimation. We describe some variance reduction techniques which may enhance convergence of sampling based estimates. We also discuss difficulties in extending this methodology to multistage stochastic programming. Finally, we briefly discuss the SAA method applied to stochastic generalized equations and variational inequalities.", "date": "2002", "authors": ["Alexander Shapiro"], "references": [2059120410, 2010353172, 1601741115, 1969481231, 1983916623, 2000257769, 2000953623, 1752958389, 107938046, 2017411673]}, {"id": 2086161653, "title": "Acceleration of stochastic approximation by averaging", "abstract": "A new recursive algorithm of stochastic approximation type with the averaging of trajectories is investigated. Convergence with probability one is proved for a variety of classical optimization and identification problems. It is also demonstrated for these problems that the proposed algorithm achieves the highest possible rate of convergence.", "date": "1992", "authors": ["B. T. Polyak , A. B. Juditsky"], "references": [1569320505, 1540723801, 1498711961, 2116501508, 1994616650, 1485035304, 2070709745, 2060471940, 2056698052, 2336878578]}, {"id": 2000953623, "title": "Monte Carlo bounding techniques for determining solution quality in stochastic programs", "abstract": "A stochastic program SP with solution value z^* can be approximately solved by sampling n realizations of the program's stochastic parameters, and by solving the resulting ''approximating problem'' for (x^*\"n,z^*\"n). We show that, in expectation, z^*\"n is a lower bound on z^* and that this bound monotonically improves as n increases. The first result is used to construct confidence intervals on the optimality gap for any candidate solution x@^ to SP, e.g., x@^=x^*\"n. A sampling procedure based on common random numbers ensures nonnegative gap estimates and provides significant variance reduction over naive sampling on four test problems.", "date": "1999", "authors": ["Wai-Kei Mak 1, David P. Morton 1, R.Kevin Wood 2"], "references": [1495951130, 2140481308, 1983916623, 2143863355, 1963748947, 2053877403, 2017411673, 2055635846, 2311126677, 1994377487]}, {"id": 2148603752, "title": "Statistical learning theory", "abstract": "A comprehensive look at learning and generalization theory. The statistical theory of learning and generalization concerns the problem of choosing desired functions on the basis of empirical data. Highly applicable to a variety of computer science and robotics fields, this book offers lucid coverage of the theory as a whole. Presenting a method for determining the necessary and sufficient conditions for consistency of learning process, the author covers function estimates from small data pools, applying these estimations to real-life problems, and much more.", "date": "1997", "authors": ["Vladimir Naumovich Vapnik"], "references": [2156909104]}, {"id": 1563088657, "title": "An Introduction to Support Vector Machines and Other Kernel-based Learning Methods", "abstract": "From the publisher: This is the first comprehensive introduction to Support Vector Machines (SVMs), a new generation learning system based on recent advances in statistical learning theory. SVMs deliver state-of-the-art performance in real-world applications such as text categorisation, hand-written character recognition, image classification, biosequences analysis, etc., and are now established as one of the standard tools for machine learning and data mining. Students will find the book both stimulating and accessible, while practitioners will be guided smoothly through the material required for a good grasp of the theory and its applications. The concepts are introduced gradually in accessible and self-contained stages, while the presentation is rigorous and thorough. Pointers to relevant literature and web sites containing software ensure that it forms an ideal starting point for further study. Equally, the book and its associated web site will guide practitioners to updated literature, new applications, and on-line software.", "date": "1999", "authors": ["Nello Cristianini 1, John Shawe-Taylor 2"], "references": [2140190241, 1570448133, 1964357740, 2143426320, 2104978738, 2034328688, 1736726159, 2145295623, 1873332500]}, {"id": 1560724230, "title": "Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond", "abstract": "From the Publisher: In the 1990s, a new type of learning algorithm was developed, based on results from statistical learning theory: the Support Vector Machine (SVM). This gave rise to a new class of theoretically elegant learning machines that use a central concept of SVMs\u0097-kernels--for a number of learning tasks. Kernel machines provide a modular framework that can be adapted to different tasks and domains by the choice of the kernel function and the base algorithm. They are replacing neural networks in a variety of fields, including engineering, information retrieval, and bioinformatics. Learning with Kernels provides an introduction to SVMs and related kernel methods. Although the book begins with the basics, it also includes the latest research. It provides all of the concepts necessary to enable a reader equipped with some basic mathematical knowledge to enter the world of machine learning using theoretically well-founded yet easy-to-use kernel algorithms and to understand and apply the powerful algorithms that have been developed over the last few years.", "date": "2001", "authors": ["Bernhard Scholkopf , Alexander J. Smola"], "references": [2164278908, 2154889144, 2153233077, 1601740268, 161114242, 607505555, 2112020727, 2130556178, 2160218441, 2104657103]}, {"id": 2053463056, "title": "BoosTexter: A Boosting-based Systemfor Text Categorization", "abstract": "This work focuses on algorithms which learn from examples to perform multiclass text and speech categorization tasks. Our approach is based on a new and improved family of boosting algorithms. We describe in detail an implementation, called BoosTexter, of the new boosting algorithms for text categorization tasks. We present results comparing the performance of BoosTexter and a number of other text-categorization algorithms on a variety of tasks. We conclude by describing the application of our system to automatic call-type identification from unconstrained spoken customer responses.", "date": "2000", "authors": ["Robert E. Schapire 1, Yoram Singer 2"], "references": [1988790447, 2112076978, 1975846642, 1956559956, 2114535528, 1670263352, 2096152098, 2032210760, 1966280301, 2067885219]}, {"id": 1978394996, "title": "Term Weighting Approaches in Automatic Text Retrieval", "abstract": "The experimental evidence accumulated over the past 20 years indicates that textindexing systems based on the assignment of appropriately weighted single terms produce retrieval results that are superior to those obtainable with other more elaborate text representations. These results depend crucially on the choice of effective term weighting systems. This paper summarizes the insights gained in automatic term weighting, and provides baseline single term indexing models with which other more elaborate content analysis procedures can be compared.", "date": "1988", "authors": ["Gerard Salton , Christopher Buckley"], "references": [1956559956, 2043909051, 2083605078, 2068632118, 3090556797, 2095396650, 2075006521, 11171803, 3091372544, 1557757161]}, {"id": 2032210760, "title": "Improved boosting algorithms using confidence-rated predictions", "abstract": "We describe several improvements to Freund and Schapire\u2018s AdaBoost boosting algorithm, particularly in a setting in which hypotheses may assign confidences to each of their predictions. We give a simplified analysis of AdaBoost in this setting, and we show how this analysis can be used to find improved parameter settings as well as a refined criterion for training weak hypotheses. We give a specific method for assigning confidences to the predictions of decision trees, a method closely related to one used by Quinlan. This method also suggests a technique for growing decision trees which turns out to be identical to one proposed by Kearns and Mansour. We focus next on how to apply the new boosting algorithms to multiclass classification problems, particularly to the multi-label case in which each example may belong to more than one class. We give two boosting methods for this problem, plus a third method based on output coding. One of these leads to a new method for handling the single-label case which is simpler but as effective as techniques suggested by Freund and Schapire. Finally, we give some experimental results comparing a few of the algorithms discussed in this paper.", "date": "1998", "authors": ["Robert E. Schapire , Yoram Singer"], "references": [1988790447, 2112076978, 1975846642, 2152761983, 1605688901, 2982720039, 1966280301, 2107890099, 1676820704, 2067885219]}, {"id": 2157791002, "title": "On the algorithmic implementation of multiclass kernel-based vector machines", "abstract": "In this paper we describe the algorithmic implementation of multiclass kernel-based vector machines. Our starting point is a generalized notion of the margin to multiclass problems. Using this notion we cast multiclass categorization problems as a constrained optimization problem with a quadratic objective function. Unlike most of previous approaches which typically decompose a multiclass problem into multiple independent binary classification tasks, our notion of margin yields a direct method for training multiclass predictors. By using the dual of the optimization problem we are able to incorporate kernels with a compact set of constraints and decompose the dual problem into multiple optimization problems of reduced size. We describe an efficient fixed-point algorithm for solving the reduced optimization problems and prove its convergence. We then discuss technical details that yield significant running time improvements for large datasets. Finally, we describe various experiments with our approach comparing it to previously studied kernel-based methods. Our experiments indicate that for multiclass problems we attain state-of-the-art accuracy.", "date": "2002", "authors": ["Koby Crammer , Yoram Singer"], "references": [2148603752, 2119821739, 2139212933, 1988790447, 1563088657, 3085162807, 1512098439, 1601740268, 2087347434, 1604938182]}, {"id": 1997063559, "title": "Stochastic relaxation, Gibbs distributions and the Bayesian restoration of images*", "abstract": "We make an analogy between images and statistical mechanics systems. Pixel gray levels and the presence and orientation of edges are viewed as states of atoms or molecules in a lattice-like physical system. The assignment of an energy function in the physical system determines its Gibbs distribution. Because of the Gibbs distribution, Markov random field (MRF) equivalence, this assignment also determines an MRF image model. The energy function is a more convenient and natural mechanism for embodying picture attributes than are the local characteristics of the MRF. For a range of degradation mechanisms, including blurring, non-linear deformations, and multiplicative or additive noise, the posterior distribution is an MRF with a structure akin to the image model. By the analogy, the posterior distribution defines another (imaginary) physical system. Gradual temperature reduction in the physical system isolates low-energy states (\u2018annealing\u2019), or what is the same thing, the most probable states under the Gib...", "date": "1992", "authors": ["Stuart Geman 1, Donald Geman 2"], "references": [2581275558, 2150060382, 1622620102, 2154061444, 2114220616, 2065301447, 1979622972, 2107792892, 2014208555, 1567885833]}, {"id": 1746680969, "title": "Learning in graphical models", "abstract": "Part 1 Inference: introduction to inference for Bayesian networks, Robert Cowell advanced inference in Bayesian networks, Robert Cowell inference in Bayesian networks using nested junction trees, Uffe Kjoerulff bucket elimination - a unifying framework for probabilistic inference, R. Dechter an introduction to variational methods for graphical models, Michael I. Jordan et al improving the mean field approximation via the use of mixture distributions, Tommi S. Jaakkola and Michael I. Jordan introduction to Monte Carlo methods, D.J.C. MacKay suppressing random walls in Markov chain Monte Carlo using ordered overrelaxation, Radford M. Neal. Part 2 Independence: chain graphs and symmetric associations, Thomas S. Richardson the multiinformation function as a tool for measuring stochastic dependence, M. Studeny and J. Vejnarova. Part 3 Foundations for learning: a tutorial on learning with Bayesian networks, David Heckerman a view of the EM algorithm that justifies incremental, sparse and other variants, Radford M. Neal and Geoffrey E. Hinton. Part 4 Learning from data: latent variable models, Christopher M. Bishop stochastic algorithms for exploratory data analysis - data clustering and data visualization, Joachim M. Buhmann learning Bayesian networks with local structure, Nir Friedman and Moises Goldszmidt asymptotic model selection for directed networks with hidden variables, Dan Geiger et al a hierarchical community of experts, Geoffrey E. Hinton et al an information-theoretic analysis of hard and soft assignment methods for clustering, Michael J. Kearns et al learning hybrid Bayesian networks from data, Stefano Monti and Gregory F. Cooper a mean field learning algorithm for unsupervised neural networks, Lawrence Saul and Michael Jordan edge exclusion tests for graphical Gaussian models, Peter W.F. Smith and Joe Whittaker hepatitis B - a case study in MCMC, D.J. Spiegelhalter et al prediction with Gaussian processes - from linear regression to linear prediction and beyond, C.K.I. Williams.", "date": "1999", "authors": ["Michael I. Jordan"], "references": [1880262756, 2072128103, 2116064496, 2166049352, 2166851633, 2097089247, 1755360231, 1873332500]}, {"id": 2083380015, "title": "Connectionist learning of belief networks", "abstract": "Abstract Connectionist learning procedures are presented for \u201csigmoid\u201d and \u201cnoisy-OR\u201d varieties of probabilistic belief networks. These networks have previously been seen primarily as a means of representing knowledge derived from experts. Here it is shown that the \u201cGibbs sampling\u201d simulation procedure for such networks can support maximum-likelihood learning from empirical data through local gradient ascent. This learning procedure resembles that used for \u201cBoltzmann machines\u201d, and like it, allows the use of \u201chidden\u201d variables to model correlations between visible variables. Due to the directed nature of the connections in a belief network, however, the \u201cnegative phase\u201d of Boltzmann machine learning is unnecessary. Experimental results show that, as a result, learning in a sigmoid belief network can be faster than in a Boltzmann machine. These networks have other advantages over Boltzmann machines in pattern classification and decision making applications, are naturally applicable to unsupervised learning problems, and provide a link between work on connectionist learning and work on the representation of expert knowledge.", "date": "1992", "authors": ["Radford M. Neal"], "references": [1652505363, 2159080219, 1498436455, 2049633694, 2083875149, 1593793857, 1507849272, 2166698530, 1992880122, 1547224907]}, {"id": 1547224907, "title": "Learning and relearning in Boltzmann machines", "abstract": "This chapter contains sections titled: Relaxation Searches, Easy and Hard Learning, The Boltzmann Machine Learning Algorithm, An Example of Hard Learning, Achieving Reliable Computation with Unreliable Hardware, An Example of the Effects of Damage, Conclusion, Acknowledgments, Appendix: Derivation of the Learning Algorithm, References", "date": "1986", "authors": ["G. E. Hinton , T. J. Sejnowski"], "references": [2310919327, 2076063813, 2072128103, 2116064496, 2137813581, 2133671888, 1562911371, 1516111018, 2321533354]}, {"id": 2114153178, "title": "Rate-coded Restricted Boltzmann Machines for Face Recognition", "abstract": "We describe a neurally-inspired, unsupervised learning algorithm that builds a non-linear generative model for pairs of face images from the same individual. Individuals are then recognized by finding the highest relative probability pair among all pairs that consist of a test image and an image whose identity is known. Our method compares favorably with other methods in the literature. The generative model consists of a single layer of rate-coded, non-linear feature detectors and it has the property that, given a data vector, the true posterior probability distribution over the feature detector activities can be inferred rapidly without iteration or approximation. The weights of the feature detectors are learned by comparing the correlations of pixel intensities and feature activations in two phases: When the network is observing real data and when it is observing reconstructions of real data generated from the feature activations.", "date": "1999", "authors": ["Yee Whye Teh 1, Geoffrey E. Hinton 2"], "references": [2116064496, 2138451337, 1902027874, 2121647436, 2159080219, 2113341759, 2125027820, 2128716185, 1547224907, 2143784448]}, {"id": 2101706260, "title": "Recognizing handwritten digits using hierarchical products of experts", "abstract": "The product of experts learning procedure can discover a set of stochastic binary features that constitute a nonlinear generative model of handwritten images of digits. The quality of generative models learned in this way can be assessed by learning a separate model for each class of digit and then comparing the unnormalized probabilities of test images under the 10 different class-specific models. To improve discriminative performance, a hierarchy of separate models can be learned, for each digit class. Each model in the hierarchy learns a layer of binary feature detectors that model the probability distribution of vectors of activity of feature detectors in the layer below. The models in the hierarchy are trained sequentially and each model uses a layer of binary feature detectors to learn a generative model of the patterns of feature activities in the preceding layer. After training, each layer of feature detectors produces a separate, unnormalized log probability score. With three layers of feature detectors for each of the 10 digit classes, a test image produces 30 scores which can be used as inputs to a supervised, logistic classification network that is trained on separate data.", "date": "2002", "authors": ["G. Mayraz , G.E. Hinton"], "references": [2912934387, 2116064496, 2159080219, 2150884987, 28412257, 1547224907, 2104867159, 1667072054, 1813659000, 2100559472]}, {"id": 2123977795, "title": "Visual learning and recognition of 3-D objects from appearance", "abstract": "The problem of automatically learning object models for recognition and pose estimation is addressed. In contrast to the traditional approach, the recognition problem is formulated as one of matching appearance rather than shape. The appearance of an object in a two-dimensional image depends on its shape, reflectance properties, pose in the scene, and the illumination conditions. While shape and reflectance are intrinsic properties and constant for a rigid object, pose and illumination vary from scene to scene. A compact representation of object appearance is proposed that is parametrized by pose and illumination. For each object of interest, a large set of images is obtained by automatically varying pose and illumination. This image set is compressed to obtain a low-dimensional subspace, called the eigenspace, in which the object is represented as a manifold. Given an unknown input image, the recognition system projects the image to eigenspace. The object is recognized based on the manifold it lies on. The exact position of the projection on the manifold determines the object's pose in the image. A variety of experiments are conducted using objects with complex appearance characteristics. The performance of the recognition and pose estimation algorithms is studied using over a thousand input images of sample objects. Sensitivity of recognition to the number of eigenspace dimensions and the number of learning samples is analyzed. For the objects used, appearance representation in eigenspaces with less than 20 dimensions produces accurate recognition results with an average pose estimation error of about 1.0 degree. A near real-time recognition system with 20 complex objects in the database has been developed. The paper is concluded with a discussion on various issues related to the proposed learning and recognition methodology.", "date": "1994", "authors": ["Hiroshi Murase 1, Shree K. Nayar 2"], "references": [2170120409, 2098693229, 2143956139, 2130259898, 2135346934, 3110825305, 1996773532, 2086479969, 2026311529, 2033554200]}, {"id": 2160225842, "title": "Learning a Sparse Representation for Object Detection", "abstract": "We present an approach for learning to detect objects in still gray images, that is based on a sparse, part-based representation of objects. A vocabulary of information-rich object parts is automatically constructed from a set of sample images of the object class of interest. Images are then represented using parts from this vocabulary, along with spatial relations observed among them. Based on this representation, a feature-efficient learning algorithm is used to learn to detect instances of the object class. The framework developed can be applied to any object with distinguishable parts in a relatively fixed spatial configuration. We report experiments on images of side views of cars. Our experiments show that the method achieves high detection accuracy on a difficult test set of real-world images, and is highly robust to partial occlusion and background variation.In addition, we discuss and offer solutions to several methodological issues that are significant for the research community to be able to evaluate object detection approaches.", "date": "2002", "authors": ["Shivani Agarwal , Dan Roth"], "references": [2164598857, 2217896605, 2152473410, 2124087378, 2124351082, 2155511848, 1949116567, 1564419782, 2156406284, 2124722975]}, {"id": 2141376824, "title": "Contour and Texture Analysis for Image Segmentation", "abstract": "This paper provides an algorithm for partitioning grayscale images into disjoint regions of coherent brightness and texture. Natural images contain both textured and untextured regions, so the cues of contour and texture differences are exploited simultaneously. Contours are treated in the intervening contour framework, while texture is analyzed using textons. Each of these cues has a domain of applicability, so to facilitate cue combination we introduce a gating operator based on the texturedness of the neighborhood at a pixel. Having obtained a local measure of how likely two nearby pixels are to belong to the same region, we use the spectral graph theoretic framework of normalized cuts to find partitions of the image into regions of coherent texture and brightness. Experimental results on a wide range of images are shown.", "date": "2001", "authors": ["Jitendra Malik , Serge Belongie , Thomas Leung , Jianbo Shi"], "references": [2121947440, 2145023731, 1578099820, 1997063559, 2121927366, 1634005169, 3017143921, 2114487471, 2160167256, 1490632837]}, {"id": 1612003148, "title": "Probabilistic latent semantic analysis", "abstract": "Probabilistic Latent Semantic Analysis is a novel statistical technique for the analysis of two-mode and co-occurrence data, which has applications in information retrieval and filtering, natural language processing, machine learning from text, and in related areas. Compared to standard Latent Semantic Analysis which stems from linear algebra and performs a Singular Value Decomposition of co-occurrence tables, the proposed method is based on a mixture decomposition derived from a latent class model. This results in a more principled approach which has a solid foundation in statistics. In order to avoid overfitting, we propose a widely applicable generalization of maximum likelihood model fitting by tempered EM. Our approach yields substantial and consistent improvements over Latent Semantic Analysis in a number of experiments.", "date": "1999", "authors": ["Thomas Hofmann"], "references": [2147152072, 2049633694, 2107743791, 1956559956, 1983578042, 2134731454, 2127314673, 2056029990, 2140842551, 2143144851]}, {"id": 2122090912, "title": "Maximum-Margin Matrix Factorization", "abstract": "We present a novel approach to collaborative prediction, using low-norm instead of low-rank factorizations. The approach is inspired by, and has strong connections to, large-margin linear discrimination. We show how to learn low-norm factorizations by solving a semi-definite program, and discuss generalization error bounds for them.", "date": "2004", "authors": ["Nathan Srebro 1, Jason Rennie 2, Tommi S. Jaakkola 2"], "references": [1902027874, 2145295623, 2134731454, 2049455633, 2165395308, 1966096622, 2151052953, 2118079529, 2135001774, 1999613943]}, {"id": 2158164339, "title": "Modeling Human Motion Using Binary Latent Variables", "abstract": "We propose a non-linear generative model for human motion data that uses an undirected model with binary latent variables and real-valued \"visible\" variables that represent joint angles. The latent and visible variables at each time step receive directed connections from the visible variables at the last few time-steps. Such an architecture makes on-line inference efficient and allows us to use a simple approximate learning procedure. After training, the model finds a single set of parameters that simultaneously capture several different kinds of motion. We demonstrate the power of our approach by synthesizing various motion sequences and by performing on-line filling in of data lost during motion capture.", "date": "2006", "authors": ["Graham W. Taylor , Geoffrey E. Hinton , Sam T. Roweis"], "references": [2136922672, 2310919327, 2116064496, 2124914669, 2293741035, 2114153178, 2147010501, 2248685949, 1991942383, 2123236823]}, {"id": 205159212, "title": "Learning a Nonlinear Embedding by Preserving Class Neighbourhood Structure", "abstract": "", "date": "2007", "authors": ["Ruslan Salakhutdinov , Geoffrey E. Hinton"], "references": [2136922672, 2100495367, 2116064496, 2117154949, 2130556178, 2157364932, 2144935315, 2159737176, 2124914669, 2157444450]}, {"id": 2165395308, "title": "Weighted low-rank approximations", "abstract": "We study the common problem of approximating a target matrix with a matrix of lower rank. We provide a simple and efficient (EM) algorithm for solving weighted low-rank approximation problems, which, unlike their unweighted version, do not admit a closed-form solution in general. We analyze, in addition, the nature of locally optimal solutions that arise in this context, demonstrate the utility of accommodating the weights in reconstructing the underlying low-rank representation, and extend the formulation to non-Gaussian noise models such as logistic models. Finally, we apply the methods developed to a collaborative filtering task.", "date": "2003", "authors": ["Nathan Srebro , Tommi Jaakkola"], "references": [2117354486, 1673941785, 2170653751, 2021680564, 2135001774, 1496451467, 1516172206, 1568698519, 2139451327, 2252194958]}, {"id": 1989702938, "title": "Face recognition: A literature survey", "abstract": "As one of the most successful applications of image analysis and understanding, face recognition has recently received significant attention, especially during the past several years. At least two reasons account for this trend: the first is the wide range of commercial and law enforcement applications, and the second is the availability of feasible technologies after 30 years of research. Even though current machine recognition systems have reached a certain level of maturity, their success is limited by the conditions imposed by many real applications. For example, recognition of face images acquired in an outdoor environment with changes in illumination and/or pose remains a largely unsolved problem. In other words, current systems are still far away from the capability of the human perception system.This paper provides an up-to-date critical survey of still- and video-based face recognition research. There are two underlying motivations for us to write this survey paper: the first is to provide an up-to-date review of the existing literature, and the second is to offer some insights into the studies of machine recognition of faces. To provide a comprehensive survey, we not only categorize existing recognition techniques but also present detailed descriptions of representative methods within each category. In addition, relevant topics such as psychophysical studies, system evaluation, and issues of illumination and pose variation are covered.", "date": "2003", "authors": ["W. Zhao 1, R. Chellappa 2, P. J. Phillips 3, A. Rosenfeld 2"], "references": [2156909104, 2164598857, 2138451337, 2217896605, 2121647436, 3111950349, 2033419168, 2038952578, 3110653090, 3111038685]}, {"id": 2098947662, "title": "View-based and modular eigenspaces for face recognition", "abstract": "We describe experiments with eigenfaces for recognition and interactive search in a large-scale face database. Accurate visual recognition is demonstrated using a database of O(10/sup 3/) faces. The problem of recognition under general viewing orientation is also examined. A view-based multiple-observer eigenspace technique is proposed for use in face recognition under variable pose. In addition, a modular eigenspace description technique is used which incorporates salient features such as the eyes, nose and mouth, in an eigenfeature layer. This modular representation yields higher recognition rates as well as a more robust framework for face recognition. An automatic feature extraction technique using feature eigentemplates is also demonstrated. >", "date": "1994", "authors": ["Pentland , Moghaddam , Starner"], "references": [2138451337, 2113341759, 2135463994, 2138313032, 2130506643, 2157418942, 2112684592, 1993867646, 2121863133, 2030234875]}, {"id": 2905573712, "title": "Face recognition: A Literature Survey", "abstract": "", "date": "2007", "authors": ["W. Zhao , R. Rosenfeld , R. Chellappa"], "references": [2129812935, 2111993661, 2536626143, 2131081720, 2149382413, 2027805700, 2078088780, 2097777575, 2106488920, 2096027770]}, {"id": 2155759509, "title": "The CMU Pose, Illumination, and Expression (PIE) database", "abstract": "Between October 2000 and December 2000, we collected a database of over 40,000 facial images of 68 people. Using the CMU (Carnegie Mellon University) 3D Room, we imaged each person across 13 different poses, under 43 different illumination conditions, and with four different expressions. We call this database the CMU Pose, Illumination and Expression (PIE) database. In this paper, we describe the imaging hardware, the collection procedure, the organization of the database, several potential uses of the database, and how to obtain the database.", "date": "2002", "authors": ["T. Sim , S. Baker , M. Bsat"], "references": [2125127226, 2118774738, 2102760078, 2120420721, 2110822444, 2121114545, 2106143125, 2141503314, 2144855601]}, {"id": 2119565742, "title": "The Google file system", "abstract": "We have designed and implemented the Google File System, a scalable distributed file system for large distributed data-intensive applications. It provides fault tolerance while running on inexpensive commodity hardware, and it delivers high aggregate performance to a large number of clients. While sharing many of the same goals as previous distributed file systems, our design has been driven by observations of our application workloads and technological environment, both current and anticipated, that reflect a marked departure from some earlier file system assumptions. This has led us to reexamine traditional choices and explore radically different design points. The file system has successfully met our storage needs. It is widely deployed within Google as the storage platform for the generation and processing of data used by our service as well as research and development efforts that require large data sets. The largest cluster to date provides hundreds of terabytes of storage across thousands of disks on over a thousand machines, and it is concurrently accessed by hundreds of clients. In this paper, we present file system interface extensions designed to support distributed applications, discuss many aspects of our design, and report measurements from both micro-benchmarks and real world use.", "date": "2003", "authors": ["Sanjay Ghemawat , Howard Gobioff , Shun-Tak Leung"], "references": [2131645490, 2147504831, 2005373714, 2133338501, 2025413686, 2137808089, 2115457697, 2088221489, 2149609142]}, {"id": 2148317584, "title": "Distributed computing in practice: the Condor experience", "abstract": "SUMMARY Since 1984, the Condor project has enabled ordinary users to do extraordinary computing. Today, the project continues to explore the social and technical problems of cooperative computing on scales ranging from the desktop to the world-wide computational Grid. In this paper, we provide the history and philosophy of the Condor project and describe how it has interacted with other projects and evolved along with the field of distributed computing. We outline the core components of the Condor system and describe how the technology of computing must correspond to social structures. Throughout, we reflect on the lessons of experience and chart the course travelled by research ideas as they grow into production systems. Copyright c \ufffd 2005 John Wiley & Sons, Ltd.", "date": "2005", "authors": ["Douglas Thain , Todd Tannenbaum , Miron Livny"], "references": [2091257550, 2439240014, 2152526229, 2985178474, 1973501242, 2146749986, 2120510885, 2083469471, 1809943808, 2131053137]}, {"id": 2073965851, "title": "Web search for a planet: The Google cluster architecture", "abstract": "Amenable to extensive parallelization, Google's web search application lets different queries run on different processors and, by partitioning the overall index, also lets a single query use multiple processors. to handle this workload, Googless architecture features clusters of more than 15,000 commodity-class PCs with fault tolerant software. This architecture achieves superior performance at a fraction of the cost of a system built from fewer, but more expensive, high-end servers.", "date": "2003", "authors": ["L.A. Barroso , J. Dean , U. Holzle"], "references": [3013264884, 2114421447, 2155350341, 1971851724, 2338343669]}, {"id": 2109722477, "title": "Map-Reduce for Machine Learning on Multicore", "abstract": "We are at the beginning of the multicore era. Computers will have increasingly many cores (processors), but there is still no good programming framework for these architectures, and thus no simple and unified way for machine learning to take advantage of the potential speed up. In this paper, we develop a broadly applicable parallel programming method, one that is easily applied to many different learning algorithms. Our work is in distinct contrast to the tradition in machine learning of designing (often ingenious) ways to speed up a single algorithm at a time. Specifically, we show that algorithms that fit the Statistical Query model [15] can be written in a certain \"summation form,\" which allows them to be easily parallelized on multicore computers. We adapt Google's map-reduce [7] paradigm to demonstrate this parallel speed up technique on a variety of learning algorithms including locally weighted linear regression (LWLR), k-means, logistic regression (LR), naive Bayes (NB), SVM, ICA, PCA, gaussian discriminant analysis (GDA), EM, and backpropagation (NN). Our experimental results show basically linear speedup with an increasing number of processors.", "date": "2006", "authors": ["Cheng-tao Chu , Sang K. Kim , Yi-an Lin , Yuanyuan Yu , Gary Bradski , Kunle Olukotun , Andrew Y. Ng"], "references": [2173213060, 1512098439, 3110653090, 1530699444, 1924689489, 2019363670, 2147898188, 1985690171, 2017977879, 2089468765]}, {"id": 2104644701, "title": "Evaluating MapReduce for Multi-core and Multiprocessor Systems", "abstract": "This paper evaluates the suitability of the MapReduce model for multi-core and multi-processor systems. MapReduce was created by Google for application development on data-centers with thousands of servers. It allows programmers to write functional-style code that is automatically parallelized and scheduled in a distributed system. We describe Phoenix, an implementation of MapReduce for shared-memory systems that includes a programming API and an efficient runtime system. The Phoenix runtime automatically manages thread creation, dynamic task scheduling, data partitioning, and fault tolerance across processor nodes. We study Phoenix with multi-core and symmetric multiprocessor systems and evaluate its performance potential and error recovery features. We also compare MapReduce code to code written in lower-level APIs such as P-threads. Overall, we establish that, given a careful implementation, MapReduce is a promising model for scalable performance on shared-memory systems with simple parallel code", "date": "2007", "authors": ["C. Ranger , R. Raghuraman , A. Penmetsa , G. Bradski , C. Kozyrakis"], "references": [2173213060, 2119565742, 2073965851, 2109065830, 2022740893, 2108204150, 2072725684, 2162465831, 2116059696, 2134408405]}, {"id": 1510543252, "title": "Using MPI: Portable Parallel Programming with the Message-Passing Interface", "abstract": "This book offers a thoroughly updated guide to the MPI (Message-Passing Interface) standard library for writing programs for parallel computers. Since the publication of the previous edition of Using MPI, parallel computing has become mainstream. Today, applications run on computers with millions of processors; multiple processors sharing memory and multicore processors with multiple hardware threads per core are common. The MPI-3 Forum recently brought the MPI standard up to date with respect to developments in hardware capabilities, core language evolution, the needs of applications, and experience gained over the years by vendors, implementers, and users. This third edition of Using MPI reflects these changes in both text and example code. The book takes an informal, tutorial approach, introducing each concept through easy-to-understand examples, including actual code in C and Fortran. Topics include using MPI in simple programs, virtual topologies, MPI datatypes, parallel libraries, and a comparison of MPI with sockets. For the third edition, example code has been brought up to date; applications have been updated; and references reflect the recent attention MPI has received in the literature. A companion volume, Using Advanced MPI, covers more advanced topics, including hybrid programming and coping with large data.", "date": "1993", "authors": ["William Gropp 1, Ewing Lusk 1, Anthony Skjellum 2"], "references": []}, {"id": 2044534358, "title": "Cluster-based scalable network services", "abstract": "We identify three fundamental requirements for scalable network services: incremental scalability and overflow growth provisioning, 24x7 availability through fault masking, and cost-effectiveness. We argue that clusters of commodity workstations interconnected by a high-speed SAN are exceptionally well-suited to meeting these challenges for Internet-server workloads, provided the software infrastructure for managing partial failures and administering a large cluster does not have to be reinvented for each new service. To this end, we propose a general, layered architecture for building cluster-based scalable network services that encapsulates the above requirements for reuse, and a service-programming model based on composable workers that perform transformation, aggregation, caching, and customization (TACC) of Internet content. For both performance and implementation simplicity, the architecture and TACC programming model exploit BASE, a weaker-than-ACID data semantics that results from trading consistency for availability and relying on soft state for robustness in failure management. Our architecture can be used as an off the shelf infrastructural platform for creating new network services, allowing authors to focus on the content of the service (by composing TACC building blocks) rather than its implementation. We discuss two real implementations of services based on this architecture: TranSend, a Web distillation proxy deployed to the UC Berkeley dialup IP population, and HotBot, the commercial implementation of the Inktomi search engine. We present detailed measurements of TranSend's performance based on substantial client traces, as well as anecdotal evidence from the TranSend and HotBot experience, to support the claims made for the architecture.", "date": "1997", "authors": ["Armando Fox 1, Steven D. Gribble 1, Yatin Chawathe 1, Eric A. Brewer 1, Paul Gauthier 2"], "references": [2155106456, 2105818147, 2134342348, 2114728910, 1527961683, 2096322909, 2135131646, 2008793926, 2150048640, 2043550167]}, {"id": 1988243929, "title": "Explicit control a batch-aware distributed file system", "abstract": "We present the design, implementation, and evaluation of the Batch-Aware Distributed File System (BAD-FS), a system designed to orchestrate large, I/O-intensive batch workloads on remote computing clusters distributed across the wide area. BAD-FS consists of two novel components: a storage layer that exposes control of traditionally fixed policies such as caching, consistency, and replication; and a scheduler that exploits this control as necessary for different workloads. By extracting control from the storage layer and placing it within an external scheduler, BAD-FS manages both storage and computation in a coordinated way while gracefully dealing with cache consistency, fault-tolerance, and space management issues in a workload-specific manner. Using both microbenchmarks and real workloads, we demonstrate the performance benefits of explicit control, delivering excellent end-to-end performance across the wide-area.", "date": "2004", "authors": ["John Bent , Douglas Thain , Andrea C. Arpaci-Dusseau , Remzi H. Arpaci-Dusseau , Miron Livny"], "references": [2158714788, 2154010459, 2119565742, 2104210894, 2199552016, 2123820820, 2150676586, 2439240014, 2100406636, 2152526229]}, {"id": 2045271686, "title": "A bridging model for parallel computation", "abstract": "The success of the von Neumann model of sequential computation is attributable to the fact that it is an efficient bridge between software and hardware: high-level languages can be efficiently compiled on to this model; yet it can be effeciently implemented in hardware. The author argues that an analogous bridge between software and hardware in required for parallel computation if that is to become as widely used. This article introduces the bulk-synchronous parallel (BSP) model as a candidate for this role, and gives results quantifying its efficiency both in implementing high-level language features and algorithms, as well as in being implemented in hardware.", "date": "1990", "authors": ["Leslie G. Valiant"], "references": [2052207834, 2143462372, 1555673550, 1989582918, 1969008575, 2107997203, 2137239103, 2069489095, 2103012681, 1544480906]}, {"id": 2138857742, "title": "Why Does Unsupervised Pre-training Help Deep Learning?", "abstract": "Much recent research has been devoted to learning algorithms for deep architectures such as Deep Belief Networks and stacks of auto-encoder variants, with impressive results obtained in several areas, mostly on vision and language data sets. The best results obtained on supervised learning tasks involve an unsupervised learning component, usually in an unsupervised pre-training phase. Even though these new algorithms have enabled training deep models, many questions remain as to the nature of this difficult learning problem. The main question investigated here is the following: how does unsupervised pre-training work? Answering this questions is important if learning in deep architectures is to be further improved. We propose several explanatory hypotheses and test them through extensive simulations. We empirically show the influence of pre-training with respect to architecture depth, model capacity, and number of training examples. The experiments confirm and clarify the advantage of unsupervised pre-training. The results suggest that unsupervised pre-training guides the learning towards basins of attraction of minima that support better generalization from the training data set; the evidence from these results supports a regularization explanation for the effect of pre-training.", "date": "2010", "authors": ["Dumitru Erhan 1, Yoshua Bengio 1, Aaron Courville 1, Pierre-Antoine Manzagol 1, Pascal Vincent 1, Samy Bengio 2"], "references": [2136922672, 2100495367, 2310919327, 2187089797, 2072128103, 2001141328, 2116064496, 2117130368, 2025768430, 2110798204]}, {"id": 2148461049, "title": "Flexible, high performance convolutional neural networks for image classification", "abstract": "We present a fast, fully parameterizable GPU implementation of Convolutional Neural Network variants. Our feature extractors are neither carefully designed nor pre-wired, but rather learned in a supervised way. Our deep hierarchical architectures achieve the best published results on benchmarks for object classification (NORB, CIFAR10) and handwritten digit recognition (MNIST), with error rates of 2.53%, 19.51%, 0.35%, respectively. Deep nets trained by simple back-propagation perform better than more shallow ones. Learning is surprisingly rapid. NORB is completely trained within five epochs. Test error rates on MNIST drop to 2.42%, 0.97% and 0.48% after 1, 3 and 17 epochs, respectively.", "date": "2011", "authors": ["Dan C. Cire\u015fan , Ueli Meier , Jonathan Masci , Luca M. Gambardella , J\u00fcrgen Schmidhuber"], "references": [3118608800, 2310919327, 2546302380, 2118858186, 2134557905, 2156163116, 2144161366, 1624854622, 2132424367, 2105464873]}, {"id": 2144982973, "title": "Robust Object Recognition with Cortex-Like Mechanisms", "abstract": "We introduce a new general framework for the recognition of complex visual scenes, which is motivated by biology: We describe a hierarchical system that closely follows the organization of visual cortex and builds an increasingly complex and invariant feature representation by alternating between a template matching and a maximum pooling operation. We demonstrate the strength of the approach on a range of recognition tasks: From invariant single object recognition in clutter to multiclass categorization problems and complex scene understanding tasks that rely on the recognition of both shape-based as well as texture-based objects. Given the biological constraints that the system had to satisfy, the approach performs surprisingly well: It has the capability of learning from only a few training examples and competes with state-of-the-art systems. We also discuss the existence of a universal, redundant dictionary of features that could handle the recognition of most object categories. In addition to its relevance for computer vision, the success of this approach suggests a plausibility proof for a class of feedforward models of object recognition in cortex", "date": "2007", "authors": ["T. Serre 1, L. Wolf 1, S. Bileschi 2, M. Riesenhuber 3, T. Poggio 4"], "references": [2161969291, 2310919327, 2124386111, 2177274842, 2057175746, 2154422044, 2166049352, 2134557905, 2152473410, 2145889472]}, {"id": 1625255723, "title": "Visual categorization with bags of keypoints", "abstract": "We present a novel method for generic visual categorization: the problem of identifying the object content of natural images while generalizing across variations inherent to the object class. This bag of keypoints method is based on vector quantization of affine invariant descriptors of image patches. We propose and compare two alternative implementations using different classifiers: Naive Bayes and SVM. The main advantages of the method are that it is simple, computationally efficient and intrinsically invariant. We present results for simultaneously classifying seven semantic visual categories. These results clearly demonstrate that the method is robust to background clutter and produces good categorization accuracy even without exploiting geometric information.", "date": "2003", "authors": ["G. Csurka"], "references": [2148603752, 2164598857, 2124386111, 2177274842, 2154422044, 2149684865, 1676552347, 2124351082, 2155511848, 1484228140]}, {"id": 2606321545, "title": "Why Does Unsupervised Pre-training Help Deep Learning?", "abstract": "", "date": "2010", "authors": ["Dumitru Erhan , Aaron C. Courville , Yoshua Bengio , Pascal Vincent"], "references": [2076063813, 2163922914, 3104097132, 2147768505, 2145094598, 2141125852, 2097998348, 2605350416, 196761320, 2964082701]}, {"id": 2006903949, "title": "Fast exact multiplication by the Hessian", "abstract": "Just storing the Hessian H (the matrix of second derivatives \u03b42E/\u03b4wi\u03b4 wj of the error E with respect to each pair of weights) of a large neural network is difficult. Since a common use of a large matrix like H is to compute its product with various vectors, we derive a technique that directly calculates Hv, where v is an arbitrary vector. To calculate Hv, we first define a differential operator Rv{f(w)} = (\u03b4/\u03b4r)f(w + rv)|r=0, note that Rv{\u2207w} = Hv and Rv{w} = v, and then apply Rv{\u00b7} to the equations used to compute \u2207w. The result is an exact and numerically stable procedure for computing Hv, which takes about as much computation, and is about as local, as a gradient evaluation. We then apply the technique to a one pass gradient calculation algorithm (backpropagation), a relaxation gradient calculation algorithm (recurrent backpropagation), and two stochastic gradient calculation algorithms (Boltzmann machines and weight perturbation). Finally, we show that this technique can be used at the heart of many iterative techniques for computing various properties of H, obviating any need to calculate the full Hessian.", "date": "1993", "authors": ["Barak A. Pearlmutter"], "references": [2170120409, 2051812123, 1507849272, 2114766824, 2111051539, 2176028050, 19621276, 2007431958, 2111406701, 2125389748]}, {"id": 2130984546, "title": "Fast curvature matrix-vector products for second-order gradient descent", "abstract": "We propose a generic method for iteratively approximating various second-order gradient steps--Newton, Gauss-Newton, Levenberg-Marquardt, and natural gradient--in linear time per iteration, using special curvature matrix-vector products that can be computed in O(n). Two recent acceleration techniques for on-line learning, matrix momentum and stochastic meta-descent (SMD), implement this approach. Since both were originally derived by very different routes, this offers fresh insight into their operation, resulting in further improvements to SMD.", "date": "2002", "authors": ["Nicol N. Schraudolph"], "references": [1554663460, 1970789124, 1708197474, 2986444355, 2006903949, 1520168181, 2011395874, 2112462566, 2153795254, 2137269967]}, {"id": 2166347285, "title": "Adaptive Method of Realizing Natural Gradient Learning for Multilayer Perceptrons", "abstract": "The natural gradient learning method is known to have ideal performances for on-line training of multilayer perceptrons. It avoids plateaus, which give rise to slow convergence of the backpropagation method. It is Fisher efficient, whereas the conventional method is not. However, for implementing the method, it is necessary to calculate the Fisher information matrix and its inverse, which is practically very difficult. This article proposes an adaptive method of directly obtaining the inverse of the Fisher information matrix. It generalizes the adaptive Gauss-Newton algorithms and provides a solid theoretical justification of them. Simulations show that the proposed adaptive method works very well for realizing natural gradient learning.", "date": "2000", "authors": ["Shun-Ichi Amari , Hyeyoung Park , Kenji Fukumizu"], "references": [2914484425, 1970789124, 2045512849, 1568229137, 2104760318, 2020107577, 1995842804, 2047962774, 2011055444]}, {"id": 3023533631, "title": "Second-order stagewise backpropagation for Hessian-matrix analyses and investigation of negative curvature", "abstract": "Multi-stage feed-forward neural network (NN) learning with sigmoidal-shaped hidden-node functions is implicitly constrained optimization featuring negative curvature. Our analyses on the Hessian matrix H of the sum-squared-error measure highlight the following intriguing findings: At an early stage of learning, H tends to be indefinite and much better-conditioned than the Gauss-Newton Hessian J T J. The NN structure influences the indefiniteness and rank of H. Exploiting negative curvature leads to effective learning. All these can be numerically confirmed owing to our stagewise second-order backpropagation; the systematic procedure exploits NN's \"layered symmetry\" to compute H efficiently, making exact Hessian evaluation feasible for fairly large practical problems.", "date": "2007", "authors": ["Eiji Mizutani , Stuart E. Dreyfus"], "references": [196761320, 2995638726, 3037521947, 2155147726, 2768859775, 2727634334, 2159852605, 3087978365]}, {"id": 139959648, "title": "Multivariate adaptive regression splines (with discussion)", "abstract": "", "date": "1990", "authors": ["J. H. Friedman"], "references": [2135046866, 2912934387, 1678356000, 2024046085, 2158940042, 2079724595, 2033686454, 2144404214, 2289748525, 1580948147]}, {"id": 2111814036, "title": "Submodel selection and evaluation in regression. The X-random case", "abstract": "Summary Often, in a regression situation with many variables, a sequence of submodels is generated containing fewer variables by using such methods as stepwise addition or deletion of variables, or 'best subsets'. The question is which of this sequence of submodels is 'best', and how can submodel performance be evaluated. This was explored in Breiman (1988) for a fixed X-design. This is a sequel exploring the case of random X-designs. Analytical results are difficult, if not impossible. This study involved an extensive simulation. The basis of the study is the theoretical definition of prediction error (PE) as the expected squared error produced by applying a prediction equation to the distributional universe of (y, x) values. This definition is used throughout to compare various submodels. There can be startling differences between the x-fixed and x-random situations and different PE estimates are appropriate. Non-resampling estimates such as C,, adjusted RZ,etc. turn out to be highly biased methods for submodel selection. The two best methods are cross-validation and bootstrap. One surprise is that 5 fold cross-validation (leave out 20% of the data) is better at submodel selection and evaluation than leave-one-out cross-validation. There are a number of other surprises.", "date": "1992", "authors": ["Leo Breiman , Philip Spector"], "references": [3085162807, 1594031697, 1969423031, 2797502950, 2104499761, 2154415584, 2032514760, 2003202757, 2013580793]}, {"id": 1969557815, "title": "Multisurface Method of Pattern Separation for Medical Diagnosis Applied to Breast Cytology", "abstract": "Abstract Multisurface pattern separation is a mathematical method for distinguishing between elements of two pattern sets. Each element of the pattern sets is comprised of various scalar observations. In this paper, we use the diagnosis of breast cytology to demonstrate the applicability of this method to medical diagnosis and decision making. Each of 11 cytological characteristics of breast fine-needle aspirates reported to differ between benign and malignant samples was graded 1 to 10 at the time of sample collection. Nine characteristics were found to differ significantly between benign and malignant samples. Mathematically, these values for each sample were represented by a point in a nine-dimensional space of real variables. Benign points were separated from malignant ones by planes determined by linear programming. Correct separation was accomplished in 369 of 370 samples (201 benign and 169 malignant). In the one misclassified malignant case, the fine-needle aspirate cytology was so definitely benign and the cytology of the excised cancer so definitely malignant that we believe the tumor was missed on aspiration. Our mathematical method is applicable to other medical diagnostic and decision-making problems.", "date": "1990", "authors": ["William H. Wolberg , Olvi L. Mangasarian"], "references": [2912934387, 2155653793, 2155074104, 2152523366, 2089210920, 2103914106, 2022279610, 1973041621, 2019778169, 2151832869]}, {"id": 2030748132, "title": "Estimating Optimal Transformations for Multiple Regression and Correlation.", "abstract": "Abstract In regression analysis the response variable Y and the predictor variables X 1 \u2026, Xp are often replaced by functions \u03b8(Y) and O1(X 1), \u2026, O p (Xp ). We discuss a procedure for estimating those functions \u03b8 and O1, \u2026, O p that minimize e 2 = E{[\u03b8(Y) \u2014 \u03a3 O j (Xj )]2}/var[\u03b8(Y)], given only a sample {(yk , xk1 , \u2026, xkp ), 1 \u2a7d k \u2a7d N} and making minimal assumptions concerning the data distribution or the form of the solution functions. For the bivariate case, p = 1, \u03b8 and O satisfy \u03c1 = p(\u03b8, O) = max\u03b8,O\u03c1[\u03b8(Y), O(X)], where \u03c1 is the product moment correlation coefficient and \u03c1 is the maximal correlation between X and Y. Our procedure thus also provides a method for estimating the maximal correlation between two variables.", "date": "1985", "authors": ["Leo Breiman 1, Jerome H. Friedman 2"], "references": [2117897510, 2162870748, 2024081693, 2777019853, 3000332379, 2800289289, 129305155, 2059507684, 2029469881, 1973192023]}, {"id": 1482451543, "title": "Multiple decision trees", "abstract": "This paper describes experiments, on two domains, to investigate the effect of averaging over predictions of multiple decision trees, instead of using a single tree. Other authors have pointed out theoretical and commonsense reasons for preferring the multiple tree approach. Ideally, we would like to consider predictions from all trees, weighted by their probability. However, there is a vast number of different trees, and it is difficult to estimate the probability of each tree. We sidestep the estimation problem by using a modified version of the ID3 algorithm to build good trees, and average over only these trees. Our results are encouraging. For each domain, we managed to produce a small number of good trees. We find that it is best to average across sets of trees with different structure; this usually gives better performance than any of the constituent trees, including the ID3 tree. Keywords: machine learning, transduction, empirical evaluation", "date": "1989", "authors": ["Suk Wah Kwok , Chris Carter"], "references": [2149706766, 2128420091, 1567276288, 2096059947, 1985624473, 98436501, 2962939508, 2004101233, 2073241381]}, {"id": 1531648066, "title": "Error-correcting output codes: a general method for improving multiclass inductive learning programs", "abstract": "Multiclass learning problems involve finding a definition for an unknown function f(x) whose range is a discrete set containing k > 2 values (i.e., k \"classes\"). The definition is acquired by studying large collections of training examples of the form \u2329Xi, f(Xi)\u232a. Existing approaches to this problem include (a) direct application of multiclass algorithms such as the decision-tree algorithms ID3 and CART, (b) application of binary concept learning algorithms to learn individual binary functions for each of the k classes, and (c) application of binary concept learning algorithms with distributed output codes such as those employed by Sejnowski and Rosenberg in the NETtalk system. This paper compares these three approaches to a new technique in which BCH error-correcting codes are employed as a distributed output representation. We show that these output representations improve the performance of ID3 on the NETtalk task and of backpropagation on an isolated-letter speech-recognition task. These results demonstrate that error-correcting output codes provide a general-purpose method for improving the performance of inductive learning programs on multiclass problems.", "date": "1991", "authors": ["Thomas G. Dietterich , Ghulum Bakiri"], "references": [3085162807, 2149706766, 2019363670, 3036751298, 2128420091, 2159047538, 2048060899, 2605974740, 1552785817, 105804906]}, {"id": 1541145887, "title": "Using the ADAP Learning Algorithm to Forecast the Onset of Diabetes Mellitus", "abstract": "Abstract Neural networks or connectionist models for parallel processing are not new. However, a resurgence of interest in the past half decade has occurred. In part, this is related to a better understanding of what are now referred to as hidden nodes. These algorithms are considered to be of marked value in pattern recognition problems. Because of that, we tested the ability of an early neural network model, ADAP, to forecast the onset of diabetes mellitus in a high risk population of Pima Indians. The algorithm's performance was analyzed using standard measures for clinical tests: sensitivity, specificity, and a receiver operating characteristic curve. The crossover point for sensitivity and specificity is 0.76. We are currently further examining these methods by comparing the ADAP results with those obtained from logistic regression and linear perceptron models using precisely the same training and forecasting sets. A description of the algorithm is included.", "date": "1988", "authors": ["Jack W. Smith , J.E. Everhart , W.C. Dickson , W.C. Knowler , R.S. Johannes"], "references": [2912934387, 2155653793, 1515620500, 2125847307, 2026841079, 2138906630, 2149086733, 2100110464, 3091670682]}, {"id": 2116825644, "title": "Training restricted Boltzmann machines using approximations to the likelihood gradient", "abstract": "A new algorithm for training Restricted Boltzmann Machines is introduced. The algorithm, named Persistent Contrastive Divergence, is different from the standard Contrastive Divergence algorithms in that it aims to draw samples from almost exactly the model distribution. It is compared to some standard Contrastive Divergence and Pseudo-Likelihood algorithms on the tasks of modeling and classifying various types of data. The Persistent Contrastive Divergence algorithm outperforms the other algorithms, and is equally fast and simple.", "date": "2008", "authors": ["Tijmen Tieleman"], "references": [2136922672, 2100495367, 2116064496, 2110798204, 2099866409, 1994197834, 2096192494, 2120340025, 2124914669, 66838807]}, {"id": 1956559956, "title": "Introduction to Modern Information Retrieval", "abstract": "", "date": "1982", "authors": ["Gerard Salton , Michael J. McGill"], "references": [1880262756, 2140190241, 2031489346, 2037227137, 1902027874, 2031454541, 2110325612, 2147152072, 2158266063, 2107743791]}]